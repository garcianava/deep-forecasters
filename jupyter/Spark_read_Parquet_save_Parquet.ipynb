{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "José Luis García Nava\n",
    "DEPFIE-SCOM\n",
    "Cloud-based Implementation of Distributed Machine Learning Algorithms for Time Series Forecasting\n",
    "Parquet Archive as Time Series Database Management System\n",
    "Updated from MIRD-related research to Spark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PARQUET_PATH = '/home/developer/On_Premises/Data_Lake/complete_20160101000000_to_20180809114000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 'hourly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ['CPE04015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in devices:\n",
    "    path = '{}/{}/{}.parquet'.format(SOURCE_PARQUET_PATH, RESOLUTION, devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DataFrame contains hourly data for multiple dates\n",
    "raw_df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change timestamp from string to datetime\n",
    "# from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Van', 'Vbn', 'Vcn', 'Vav', 'ia', 'ib', 'ic', 'iav', 'kw', 'kvar', 'kwan', 'kwbn', 'kwcn', 'kvaran', 'kvarbn', 'kvarcn', 'f', 'fp', 'thdvan', 'thdvbn', 'thdvcn', 'thdia', 'thdib', 'thdic', 'desbV', 'desbI', 'kwhE', 'kwhR', 'kvarhDel', 'kvarhrec', 'kvarhq3', 'kvarhq4\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a nice trick to get a string with remaining columns, for copy and paste\n",
    "\"', '\".join(raw_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multidate_df is a Spark DataFrame with timestamp as datetime\n",
    "# use a Spark DataFrame operation instead of a Spark SQL query\n",
    "multidate_df = raw_df.select(to_timestamp(raw_df.timestamp, 'yyyy-MM-dd HH:mm:ss').alias('timestamp'),\n",
    "                             'Van', 'Vbn', 'Vcn', 'Vav', \\\n",
    "                             'ia', 'ib', 'ic', 'iav', \\\n",
    "                             'kw', 'kvar', 'kwan', 'kwbn', 'kwcn', 'kvaran', 'kvarbn', 'kvarcn', \\\n",
    "                             'f', 'fp', 'thdvan', 'thdvbn', 'thdvcn', 'thdia', 'thdib', 'thdic', \\\n",
    "                             'desbV', 'desbI', \\\n",
    "                             'kwhE', 'kwhR', 'kvarhDel', 'kvarhrec', 'kvarhq3', 'kvarhq4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- Van: double (nullable = true)\n",
      " |-- Vbn: double (nullable = true)\n",
      " |-- Vcn: double (nullable = true)\n",
      " |-- Vav: double (nullable = true)\n",
      " |-- ia: double (nullable = true)\n",
      " |-- ib: double (nullable = true)\n",
      " |-- ic: double (nullable = true)\n",
      " |-- iav: double (nullable = true)\n",
      " |-- kw: double (nullable = true)\n",
      " |-- kvar: double (nullable = true)\n",
      " |-- kwan: double (nullable = true)\n",
      " |-- kwbn: double (nullable = true)\n",
      " |-- kwcn: double (nullable = true)\n",
      " |-- kvaran: double (nullable = true)\n",
      " |-- kvarbn: double (nullable = true)\n",
      " |-- kvarcn: double (nullable = true)\n",
      " |-- f: double (nullable = true)\n",
      " |-- fp: double (nullable = true)\n",
      " |-- thdvan: double (nullable = true)\n",
      " |-- thdvbn: double (nullable = true)\n",
      " |-- thdvcn: double (nullable = true)\n",
      " |-- thdia: double (nullable = true)\n",
      " |-- thdib: double (nullable = true)\n",
      " |-- thdic: double (nullable = true)\n",
      " |-- desbV: double (nullable = true)\n",
      " |-- desbI: double (nullable = true)\n",
      " |-- kwhE: double (nullable = true)\n",
      " |-- kwhR: double (nullable = true)\n",
      " |-- kvarhDel: double (nullable = true)\n",
      " |-- kvarhrec: double (nullable = true)\n",
      " |-- kvarhq3: double (nullable = true)\n",
      " |-- kvarhq4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now timestamp column in Spark DataFrame is really a timestamp\n",
    "multidate_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the temporary view on the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "multidate_df.createOrReplaceTempView('multidate_df_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22832"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many hour-based lectures in the DataFrame\n",
    "spark.sql('select timestamp, kw \\\n",
    "           from multidate_df_view \\\n",
    "           order by timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a DataFrame with all dates in the hour-based DataFrame\n",
    "date_df = spark.sql('select substring(timestamp, 1, 10) as date \\\n",
    "                     from multidate_df_view \\\n",
    "                     group by substring(timestamp, 1, 10) \\\n",
    "                     order by substring(timestamp, 1, 10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all dates to an ordered list\n",
    "dates_list = date_df.rdd.map(lambda row : row.date).collect()\n",
    "dates_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dates_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-01-01'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build the SQL query string\n",
    "query_string = 'select * from multidate_df_view \\\n",
    "                where substring(timestamp, 1, 10) = \"{}\" \\\n",
    "                order by substring(timestamp, 1, 10)'.format(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the above query string to persist timestamp as string\n",
    "query_string = 'select string(timestamp) as timestamp, \\\n",
    "                       Van, \\\n",
    "                       Vbn, \\\n",
    "                       Vcn, \\\n",
    "                       Vav, \\\n",
    "                       ia, \\\n",
    "                       ib, \\\n",
    "                       ic, \\\n",
    "                       iav, \\\n",
    "                       kw, \\\n",
    "                       kvar, \\\n",
    "                       kwan, \\\n",
    "                       kwbn, \\\n",
    "                       kwcn, \\\n",
    "                       kvaran, \\\n",
    "                       kvarbn, \\\n",
    "                       kvarcn, \\\n",
    "                       f, \\\n",
    "                       fp, \\\n",
    "                       thdvan, \\\n",
    "                       thdvbn, \\\n",
    "                       thdvcn, \\\n",
    "                       thdia, \\\n",
    "                       thdib, \\\n",
    "                       thdic, \\\n",
    "                       desbV, \\\n",
    "                       desbI, \\\n",
    "                       kwhE, \\\n",
    "                       kwhR, \\\n",
    "                       kvarhDel, \\\n",
    "                       kvarhrec, \\\n",
    "                       kvarhq3, \\\n",
    "                       kvarhq4 \\\n",
    "                from multidate_df_view \\\n",
    "                where substring(timestamp, 1, 10) = \"{}\" \\\n",
    "                order by substring(timestamp, 1, 10)'.format(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select string(timestamp) as timestamp,                        Van,                        Vbn,                        Vcn,                        Vav,                        ia,                        ib,                        ic,                        iav,                        kw,                        kvar,                        kwan,                        kwbn,                        kwcn,                        kvaran,                        kvarbn,                        kvarcn,                        f,                        fp,                        thdvan,                        thdvbn,                        thdvcn,                        thdia,                        thdib,                        thdic,                        desbV,                        desbI,                        kwhE,                        kwhR,                        kvarhDel,                        kvarhrec,                        kvarhq3,                        kvarhq4                 from multidate_df_view                 where substring(timestamp, 1, 10) = \"2016-01-01\"                 order by substring(timestamp, 1, 10)'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lectures of a single date from the original DataFrame\n",
    "buffer_df = spark.sql(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the directory where date-based parquet archives reside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEST_PARQUET_PATH = '/home/developer/On_Premises/MIRD_ROOT/data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_string = '{}/{}/{}.parquet/{}'.format(DEST_PARQUET_PATH, RESOLUTION, device, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/On_Premises/MIRD_ROOT/data/raw/hourly/CPE04015.parquet/2016-01-01'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now persist buffer_df, with a given max number of partitions\n",
    "buffer_df.coalesce(2).write.parquet(archive_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the brand new DataFrame to verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.parquet(archive_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- Van: double (nullable = true)\n",
      " |-- Vbn: double (nullable = true)\n",
      " |-- Vcn: double (nullable = true)\n",
      " |-- Vav: double (nullable = true)\n",
      " |-- ia: double (nullable = true)\n",
      " |-- ib: double (nullable = true)\n",
      " |-- ic: double (nullable = true)\n",
      " |-- iav: double (nullable = true)\n",
      " |-- kw: double (nullable = true)\n",
      " |-- kvar: double (nullable = true)\n",
      " |-- kwan: double (nullable = true)\n",
      " |-- kwbn: double (nullable = true)\n",
      " |-- kwcn: double (nullable = true)\n",
      " |-- kvaran: double (nullable = true)\n",
      " |-- kvarbn: double (nullable = true)\n",
      " |-- kvarcn: double (nullable = true)\n",
      " |-- f: double (nullable = true)\n",
      " |-- fp: double (nullable = true)\n",
      " |-- thdvan: double (nullable = true)\n",
      " |-- thdvbn: double (nullable = true)\n",
      " |-- thdvcn: double (nullable = true)\n",
      " |-- thdia: double (nullable = true)\n",
      " |-- thdib: double (nullable = true)\n",
      " |-- thdic: double (nullable = true)\n",
      " |-- desbV: double (nullable = true)\n",
      " |-- desbI: double (nullable = true)\n",
      " |-- kwhE: double (nullable = true)\n",
      " |-- kwhR: double (nullable = true)\n",
      " |-- kvarhDel: double (nullable = true)\n",
      " |-- kvarhrec: double (nullable = true)\n",
      " |-- kvarhq3: double (nullable = true)\n",
      " |-- kvarhq4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|          timestamp|                kw|              thdia|\n",
      "+-------------------+------------------+-------------------+\n",
      "|2016-01-01 00:00:00|1324.2366666666667|            0.09315|\n",
      "|2016-01-01 01:00:00|1245.2316666666666|0.09356666666666667|\n",
      "|2016-01-01 02:00:00|1145.6866666666667|             0.0936|\n",
      "|2016-01-01 03:00:00|1044.6466666666668|             0.0919|\n",
      "|2016-01-01 04:00:00| 960.6921666666667|0.08845000000000001|\n",
      "|2016-01-01 05:00:00| 911.6516666666666|0.08644999999999999|\n",
      "|2016-01-01 06:00:00| 871.7088333333332|0.08438333333333332|\n",
      "|2016-01-01 07:00:00| 775.2703333333334|0.09126666666666666|\n",
      "|2016-01-01 08:00:00| 756.7946666666667|0.09646666666666666|\n",
      "|2016-01-01 09:00:00| 821.0078333333332|0.09953333333333335|\n",
      "|2016-01-01 10:00:00| 876.7158333333333|0.09881666666666668|\n",
      "|2016-01-01 11:00:00| 948.4540000000001|0.09711666666666667|\n",
      "|2016-01-01 12:00:00| 971.0126666666666|            0.09475|\n",
      "|2016-01-01 13:00:00| 996.5504999999999|0.09486666666666667|\n",
      "|2016-01-01 14:00:00| 995.3193333333334|0.09466666666666668|\n",
      "|2016-01-01 15:00:00|1002.3786666666666|0.09488333333333333|\n",
      "|2016-01-01 16:00:00|1010.3699999999999|0.09554999999999998|\n",
      "|2016-01-01 17:00:00|          1033.005|0.09689999999999999|\n",
      "|2016-01-01 18:00:00|1188.4233333333334|0.09466666666666668|\n",
      "|2016-01-01 19:00:00|1478.9833333333333|0.09201666666666668|\n",
      "|2016-01-01 20:00:00|1545.6049999999998|0.09706666666666668|\n",
      "|2016-01-01 21:00:00|1530.3500000000001|0.10101666666666666|\n",
      "|2016-01-01 22:00:00|1409.9350000000002|0.10068333333333333|\n",
      "|2016-01-01 23:00:00|1228.8816666666664|0.10031666666666668|\n",
      "+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify the persisted DataFrame\n",
    "test_df.select('timestamp', 'kw', 'thdia').show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: trim this dataset to 2018-07-31 23:50:00 to adjust lenght to entire months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: save the dataset to 952 folders named after the date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
