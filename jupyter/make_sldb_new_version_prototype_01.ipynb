{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code to build SLDBs for DMSLSTM architecture.\n",
    "## It superseedes the make_sldb.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the variable 'labels' with 'targets', as the latter is more adequate for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale datasets to improve neural networks performance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the time series directory\n",
    "# scaler.save\n",
    "# ts.json\n",
    "# ts.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the SLDB directory:\n",
    "# train.tfrecord\n",
    "# eval.tfrecord\n",
    "# test.tfrecord\n",
    "# sldb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to configure the SLDB\n",
    "# ToDo: transfer this dictionary to dplstm/configs/sldb_config.py\n",
    "\n",
    "# modify the dictionary structure:\n",
    "# no_targets must be the same for all components, then move it to an upper level\n",
    "# remove components and use the same structure as in architecture_parameters\n",
    "sldb = {\n",
    "    'ts': 'CPE04115_H_kw_20201021084001',\n",
    "    'components': {\n",
    "        'hourly': {\n",
    "            'm': 8,\n",
    "            'tau': 1,\n",
    "            'no_targets': 24\n",
    "        },\n",
    "        'daily': {\n",
    "            'm': 8,\n",
    "            'tau': 24,\n",
    "            'no_targets': 24\n",
    "        },\n",
    "        'weekly': {\n",
    "            'm': 4,\n",
    "            'tau': 168,\n",
    "            'no_targets': 24\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series was built and persisted in a different code\n",
    "# SLDB constructions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required time series\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(sldb['ts'])\n",
    "pickle_filename = '{}/ts.pkl'.format(time_series_folder)\n",
    "ts_df = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation stage is not used for TPU-based training,\n",
    "# however, evaluation dataset might be useful to get stats from CPU-based training\n",
    "stages = ['train', 'eval', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into train/eval/test at time series level\n",
    "# to avoid data overlapping at SLDB level\n",
    "split = np.array([0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of the scaled time series for train, validation, and test thresholds\n",
    "train_eval_limit = np.int(ts_df.count()*split[0])\n",
    "eval_test_limit = np.int(ts_df.count()*split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage the time series for the different model stages\n",
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18103 lectures in train time series from 2016-01-01 00:00:00 to 2018-01-24 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for train set\n",
    "ts['train'] = ts_df[:train_eval_limit]\n",
    "print('{0} lectures in train time series from {1} to {2}'.format(ts['train'].count()[0],\n",
    "                                                                 ts['train'].index[0],\n",
    "                                                                 ts['train'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in eval time series from 2018-01-24 09:00:00 to 2018-04-28 16:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for eval set\n",
    "ts['eval'] = ts_df[train_eval_limit:eval_test_limit]\n",
    "print('{0} lectures in eval time series from {1} to {2}'.format(ts['eval'].count()[0],\n",
    "                                                                ts['eval'].index[0],\n",
    "                                                                ts['eval'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in test time series from 2018-04-28 17:00:00 to 2018-07-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for test set\n",
    "ts['test'] = ts_df[eval_test_limit:]\n",
    "print('{} lectures in test time series from {} to {}'.format(ts['test'].count()[0],\n",
    "                                                             ts['test'].index[0],\n",
    "                                                             ts['test'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to one-hot encode a timestamp\n",
    "def one_hot_encode(timestamp):\n",
    "    # input: a timestamp\n",
    "    # output: a 7-bit list encoding the week-day, and a 24-bit list encoding the day-hour\n",
    "    fv_weekday = np.zeros(7)\n",
    "    fv_hour = np.zeros(24)\n",
    "    fv_weekday[timestamp.weekday()] = 1.\n",
    "    fv_hour[timestamp.hour] = 1.\n",
    "    return list(fv_weekday), list(fv_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_targets_timestamps_ohvs(time_series, m, tau, n_targets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "           time series: original time series\n",
    "           m: embedding dimension\n",
    "           tau: lag\n",
    "           n_targets: number of targets to predict\n",
    "    Output:\n",
    "           features: list of features\n",
    "           targets: list of targets\n",
    "           timestamps: list of target (target) timestamps\n",
    "           oh_wds: list of one-hot vectors describing weekday of timestamp\n",
    "           oh_dhs: list of one-hot vectors describing hour of the day of timestamp\n",
    "    \"\"\"\n",
    "    # a couple of empty lists to store feature vectors and targets\n",
    "    features = []\n",
    "    targets = []\n",
    "    timestamps = []\n",
    "    oh_wds = []\n",
    "    oh_dhs = []\n",
    "    sequence = range(m * tau, time_series.shape[0] - n_targets + 1)\n",
    "    for i in sequence:\n",
    "        # uncomment the following line to preview features sequence timestamps (to verify the functionality)\n",
    "        # features.append(list(time_series.iloc[(i - m * tau):i:tau].index))\n",
    "        features.append(list(time_series.iloc[(i - m * tau):i:tau]))\n",
    "        # uncomment the following line to preview targets sequence timestamps (to verify the functionality)\n",
    "        # targets.append(list(time_series.iloc[i:(i + n_targets):1].index))\n",
    "        targets.append(list(time_series.iloc[i:(i + n_targets):1]))\n",
    "        # get the timestamps for the target values (just one for the first experiment)\n",
    "        targets_timestamps_list = list(time_series.index[i:(i + n_targets):1])\n",
    "        # EXTRACT TIMESTAMPS AS BYTES FOR TFRECORD PERSISTENCE\n",
    "        targets_timestamps_list_as_bytes = [timestamp.strftime(\"%Y-%m-%d %H:%M:%S\").encode() for timestamp in\n",
    "                                           targets_timestamps_list]\n",
    "        timestamps.append(targets_timestamps_list_as_bytes)\n",
    "        # build one-hot vectors for week-day and day-hour\n",
    "        # pass the timestamp(s) in the list, not the list!\n",
    "        oh_wd_vectors, oh_dh_vectors = one_hot_encode(targets_timestamps_list[0])\n",
    "        # the one-hot-encode function already returns lists, then,\n",
    "        oh_wds.append(oh_wd_vectors)\n",
    "        oh_dhs.append(oh_dh_vectors)\n",
    "\n",
    "    # uncomment the following line to return NumPy arrays instead of Python lists\n",
    "    # features, targets, timestamps = np.array(features), np.array(targets), np.array(timestamps)\n",
    "\n",
    "    return features, targets, timestamps, oh_wds, oh_dhs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the above function was successfully tested for a fraction of the training set\n",
    "small_ts = ts['train'][:10].kw_scaled\n",
    "small_sldb = make_features_targets_timestamps_ohvs(small_ts, 3, 1, 2)\n",
    "small_sldb[0] # features\n",
    "small_sldb[1] # targets\n",
    "small_sldb[2] # target timestamps as byte-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to temporarily store the following SLDBs:\n",
    "# train (hourly, daily, weekly, targets, timestamps)\n",
    "# test (hourly, daily, weekly, targets, timestamps)\n",
    "# no eval(uation) dataset as the model will be trained on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_full = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variable to build the forecast over\n",
    "variable = 'kw_scaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD ALL THE SLDBs!!!\n",
    "for stage in stages:\n",
    "    # train, eval, test\n",
    "    for component_key in sldb['components'].keys():\n",
    "        # hourly, daily, weekly\n",
    "        sldb_full[stage][component_key]['features'], sldb_full[stage][component_key]['targets'], sldb_full[stage][component_key]['timestamps'], sldb_full[stage][component_key]['oh_wds'], sldb_full[stage][component_key]['oh_dhs'] = make_features_targets_timestamps_ohvs(\n",
    "            ts[stage][variable],\n",
    "            sldb['components'][component_key]['m'],\n",
    "            sldb['components'][component_key]['tau'],\n",
    "            sldb['components'][component_key]['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6525345336504569,\n",
       " 0.6731292174438517,\n",
       " 0.69301192311566,\n",
       " 0.7302509354881197,\n",
       " 0.7897185400962217,\n",
       " 0.7017191288920568]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the target is stored as a no_targets-element list\n",
    "sldb_full['test']['weekly']['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list with forecasting resolutions\n",
    "resolutions = [key for key in sldb['components'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['features', 'targets', 'timestamps', 'oh_wds', 'oh_dhs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to collect statistics\n",
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {},\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {},\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090 features / train / hourly from 2016-01-01 08:00:00 to 2018-01-24 03:00:00\n",
      "18090 targets / train / hourly from 2016-01-01 08:00:00 to 2018-01-24 03:00:00\n",
      "18090 timestamps / train / hourly from 2016-01-01 08:00:00 to 2018-01-24 03:00:00\n",
      "18090 oh_wds / train / hourly from 2016-01-01 08:00:00 to 2018-01-24 03:00:00\n",
      "18090 oh_dhs / train / hourly from 2016-01-01 08:00:00 to 2018-01-24 03:00:00\n",
      "17906 features / train / daily from 2016-01-09 00:00:00 to 2018-01-24 03:00:00\n",
      "17906 targets / train / daily from 2016-01-09 00:00:00 to 2018-01-24 03:00:00\n",
      "17906 timestamps / train / daily from 2016-01-09 00:00:00 to 2018-01-24 03:00:00\n",
      "17906 oh_wds / train / daily from 2016-01-09 00:00:00 to 2018-01-24 03:00:00\n",
      "17906 oh_dhs / train / daily from 2016-01-09 00:00:00 to 2018-01-24 03:00:00\n",
      "17426 features / train / weekly from 2016-01-29 00:00:00 to 2018-01-24 03:00:00\n",
      "17426 targets / train / weekly from 2016-01-29 00:00:00 to 2018-01-24 03:00:00\n",
      "17426 timestamps / train / weekly from 2016-01-29 00:00:00 to 2018-01-24 03:00:00\n",
      "17426 oh_wds / train / weekly from 2016-01-29 00:00:00 to 2018-01-24 03:00:00\n",
      "17426 oh_dhs / train / weekly from 2016-01-29 00:00:00 to 2018-01-24 03:00:00\n",
      "2250 features / eval / hourly from 2018-01-24 17:00:00 to 2018-04-28 11:00:00\n",
      "2250 targets / eval / hourly from 2018-01-24 17:00:00 to 2018-04-28 11:00:00\n",
      "2250 timestamps / eval / hourly from 2018-01-24 17:00:00 to 2018-04-28 11:00:00\n",
      "2250 oh_wds / eval / hourly from 2018-01-24 17:00:00 to 2018-04-28 11:00:00\n",
      "2250 oh_dhs / eval / hourly from 2018-01-24 17:00:00 to 2018-04-28 11:00:00\n",
      "2066 features / eval / daily from 2018-02-01 09:00:00 to 2018-04-28 11:00:00\n",
      "2066 targets / eval / daily from 2018-02-01 09:00:00 to 2018-04-28 11:00:00\n",
      "2066 timestamps / eval / daily from 2018-02-01 09:00:00 to 2018-04-28 11:00:00\n",
      "2066 oh_wds / eval / daily from 2018-02-01 09:00:00 to 2018-04-28 11:00:00\n",
      "2066 oh_dhs / eval / daily from 2018-02-01 09:00:00 to 2018-04-28 11:00:00\n",
      "1586 features / eval / weekly from 2018-02-21 09:00:00 to 2018-04-28 11:00:00\n",
      "1586 targets / eval / weekly from 2018-02-21 09:00:00 to 2018-04-28 11:00:00\n",
      "1586 timestamps / eval / weekly from 2018-02-21 09:00:00 to 2018-04-28 11:00:00\n",
      "1586 oh_wds / eval / weekly from 2018-02-21 09:00:00 to 2018-04-28 11:00:00\n",
      "1586 oh_dhs / eval / weekly from 2018-02-21 09:00:00 to 2018-04-28 11:00:00\n",
      "2250 features / test / hourly from 2018-04-29 01:00:00 to 2018-07-31 18:00:00\n",
      "2250 targets / test / hourly from 2018-04-29 01:00:00 to 2018-07-31 18:00:00\n",
      "2250 timestamps / test / hourly from 2018-04-29 01:00:00 to 2018-07-31 18:00:00\n",
      "2250 oh_wds / test / hourly from 2018-04-29 01:00:00 to 2018-07-31 18:00:00\n",
      "2250 oh_dhs / test / hourly from 2018-04-29 01:00:00 to 2018-07-31 18:00:00\n",
      "2066 features / test / daily from 2018-05-06 17:00:00 to 2018-07-31 18:00:00\n",
      "2066 targets / test / daily from 2018-05-06 17:00:00 to 2018-07-31 18:00:00\n",
      "2066 timestamps / test / daily from 2018-05-06 17:00:00 to 2018-07-31 18:00:00\n",
      "2066 oh_wds / test / daily from 2018-05-06 17:00:00 to 2018-07-31 18:00:00\n",
      "2066 oh_dhs / test / daily from 2018-05-06 17:00:00 to 2018-07-31 18:00:00\n",
      "1586 features / test / weekly from 2018-05-26 17:00:00 to 2018-07-31 18:00:00\n",
      "1586 targets / test / weekly from 2018-05-26 17:00:00 to 2018-07-31 18:00:00\n",
      "1586 timestamps / test / weekly from 2018-05-26 17:00:00 to 2018-07-31 18:00:00\n",
      "1586 oh_wds / test / weekly from 2018-05-26 17:00:00 to 2018-07-31 18:00:00\n",
      "1586 oh_dhs / test / weekly from 2018-05-26 17:00:00 to 2018-07-31 18:00:00\n"
     ]
    }
   ],
   "source": [
    "# report statistics on stages and resolutions of SLDBs\n",
    "# and persist them to the sldb['stats'] level\n",
    "for stage in stages:\n",
    "    for resolution in resolutions:\n",
    "        for item in items:\n",
    "            # fill the values in the stats sub-dictionary\n",
    "            sldb['stats'][stage][resolution][item] = len(sldb_full[stage][resolution][item])\n",
    "            # timestamps are persisted as bytes, as in b'YYYY-MM-DD HH:MM;SS'\n",
    "            # but are required as strings, as in 'YYYY-MM-DD HH:MM;SS'\n",
    "            from_timestamp_str = sldb_full[stage][resolution]['timestamps'][0][0].decode()\n",
    "            sldb['stats'][stage][resolution]['from'] = from_timestamp_str\n",
    "            to_timestamp_str = sldb_full[stage][resolution]['timestamps'][-1][0].decode()\n",
    "            sldb['stats'][stage][resolution]['to'] = to_timestamp_str\n",
    "            # and log them\n",
    "            print('{0} {3} / {1} / {2} from {4} to {5}'.format(len(sldb_full[stage][resolution][item]),\n",
    "                                                               stage,\n",
    "                                                               resolution,\n",
    "                                                               item,\n",
    "                                                               from_timestamp_str,\n",
    "                                                               to_timestamp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in train set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['train']['hourly']['to'] == sldb['stats']['train']['daily']['to'] == sldb['stats']['train']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in eval set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['eval']['hourly']['to'] == sldb['stats']['eval']['daily']['to'] == sldb['stats']['eval']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in test set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['test']['hourly']['to'] == sldb['stats']['test']['daily']['to'] == sldb['stats']['test']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset on train stage was trimmed to 17426 rows.\n",
      "Dataset on eval stage was trimmed to 1586 rows.\n",
      "Dataset on test stage was trimmed to 1586 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows in the smaller resolution-based dataset, for alignment purposes\n",
    "for stage in stages:\n",
    "    sldb['stats'][stage]['trimmed_to_count'] = min([sldb['stats'][stage][resolution]['features'] for resolution in resolutions])\n",
    "    print('Dataset on {} stage was trimmed to {} rows.'.format(stage, sldb['stats'][stage]['trimmed_to_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary with final, trimmed data\n",
    "tfrecords = {\n",
    "    'train': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'eval': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'test': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # isolate this value, just for readability\n",
    "    value_to_trim = sldb['stats'][stage]['trimmed_to_count']\n",
    "    tfrecords[stage]['hourly'] = sldb_full[stage]['hourly']['features'][-value_to_trim:]\n",
    "    tfrecords[stage]['daily'] = sldb_full[stage]['daily']['features'][-value_to_trim:]\n",
    "    tfrecords[stage]['weekly'] = sldb_full[stage]['weekly']['features'][-value_to_trim:]\n",
    "    # targets and timestamps can be acquired from any resolution-based, temporary dataset (hourly, daily, weekly)\n",
    "    tfrecords[stage]['targets'] = sldb_full[stage]['hourly']['targets'][-value_to_trim:]\n",
    "    # find out the adequate way to persist timestamps (string?, bytes?)\n",
    "    # in the meantime, do not persist them to tfrecord files\n",
    "    tfrecords[stage]['timestamps'] = sldb_full[stage]['hourly']['timestamps'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_wds'] = sldb_full[stage]['hourly']['oh_wds'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_dhs'] = sldb_full[stage]['hourly']['oh_dhs'][-value_to_trim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6525345336504569,\n",
       " 0.6731292174438517,\n",
       " 0.69301192311566,\n",
       " 0.7302509354881197,\n",
       " 0.7897185400962217,\n",
       " 0.7017191288920568]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify again specs for the contents in tfrecords dictionary\n",
    "tfrecords['test']['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode byte values for serialized examples\n",
    "def _bytes_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a bytes_list from a list of strings / bytes.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: pass one-hot vectors as _int_features and decode when reading dataset???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = '{:03d}{:03d}{:03d}_' \\\n",
    "             '{:03d}{:03d}{:03d}_' \\\n",
    "             '{:03d}{:03d}{:03d}'.format(sldb['components']['hourly']['m'],\n",
    "                                         sldb['components']['hourly']['tau'],\n",
    "                                         sldb['components']['hourly']['no_targets'],\n",
    "                                         sldb['components']['daily']['m'],\n",
    "                                         sldb['components']['daily']['tau'],\n",
    "                                         sldb['components']['daily']['no_targets'],\n",
    "                                         sldb['components']['weekly']['m'],\n",
    "                                         sldb['components']['weekly']['tau'],\n",
    "                                         sldb['components']['weekly']['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04115_H_kw_20201021084001_008001006_008024006_004168006'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a time-based identifer for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001006_008024006_004168006 was created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now persist SLDBs as TFRecords\n",
    "for stage in stages:\n",
    "    N_ROWS = sldb['stats'][stage]['trimmed_to_count']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        # get an iterable with the indexes of the NumPy array to be stored in the TFRecord file\n",
    "        for row in np.arange(N_ROWS):\n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    # [lecture[0] for lecture in Xadj_train[row]] flattens the adjacent hours array\n",
    "                    feature={'hourly': _float_feature_from_list_of_values(tfrecords[stage]['hourly'][row]),\n",
    "                             'daily': _float_feature_from_list_of_values(tfrecords[stage]['daily'][row]),\n",
    "                             'weekly': _float_feature_from_list_of_values(tfrecords[stage]['weekly'][row]),\n",
    "                             'target': _float_feature_from_list_of_values(tfrecords[stage]['targets'][row]),\n",
    "                             'oh_wd': _float_feature_from_list_of_values(tfrecords[stage]['oh_wds'][row]),\n",
    "                             'oh_dh': _float_feature_from_list_of_values(tfrecords[stage]['oh_dhs'][row]),\n",
    "                             # timestamps to be incorporated later as _byte_feature???\n",
    "                             'timestamp': _bytes_feature_from_list_of_values(tfrecords[stage]['timestamps'][row])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path for the json file\n",
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the final, compact dictionary to JSON\n",
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
