{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototype for new version of predict_and_plot.py\n",
    "# now called make_predictions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# replace it later with Abseil Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.contrib.predictor import from_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to get the last saved model in model_dir\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "_BUCKET_NAME = 'cbidmltsf'\n",
    "bucket = storage_client.get_bucket(_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dplstm.data import _parse_dataset_function\n",
    "# define here the _parse_dataset_function, just for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shapes['daily'])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shapes['weekly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    # do not parse the timestamp to TPUEstimator, as it does not support string types!\n",
    "    # ToDo: code timestamps into features, as numbers\n",
    "    #  so they can be parsed to training\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'hourly': hourly,\n",
    "        'daily': daily,\n",
    "        'weekly': weekly,\n",
    "        'oh_wd': oh_wd,\n",
    "        'oh_dh': oh_dh,\n",
    "    }\n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    return feature_dict, target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the model dir as flag... later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model identifier\n",
    "MODEL_ID = 'DMSLSTM_TPU_70_00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'gs://cbidmltsf/models/{}'.format(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/models/DMSLSTM_TPU_70_00'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the data dir as flag... later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='gs://cbidmltsf/sldbs/CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D008024001_W004168001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep model_dir as used in training script\n",
    "# but remove the particle 'cbidmltsf' in saved model path\n",
    "# because the name of the bucket was already passed to Storage API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = '{}/export/exporter'.format(model_dir.replace('gs://cbidmltsf/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/DMSLSTM_TPU_70_00/export/exporter'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(bucket_folder):\n",
    "    \"\"\"List all files in GCP bucket.\"\"\"\n",
    "    # ToDo: restructure the main function as this method uses the global variable 'bucket'\n",
    "    files = bucket.list_blobs(prefix=bucket_folder)\n",
    "    files_list = [file.name for file in files if '.' in file.name]\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list_files(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/DMSLSTM_TPU_70_00/export/exporter/1601003254/saved_model.pb',\n",
       " 'models/DMSLSTM_TPU_70_00/export/exporter/1601003254/variables/variables.data-00000-of-00001',\n",
       " 'models/DMSLSTM_TPU_70_00/export/exporter/1601003254/variables/variables.index']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the names of the subdirectories in export/exporter (one for each training process)\n",
    "prefix_length = len(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_saved_model_id = sorted(list(set([file[prefix_length+1:prefix_length+11] for file in all_files])))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1601003254'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_saved_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LATEST_SAVED_MODEL_DIR = 'gs://{0}/{1}/{2}'.format(_BUCKET_NAME,\n",
    "                                                    saved_model_path,\n",
    "                                                    latest_saved_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/models/DMSLSTM_TPU_70_00/export/exporter/1601003254'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LATEST_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from gs://cbidmltsf/models/DMSLSTM_TPU_70_00/export/exporter/1601003254/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# build a prediction function\n",
    "predict_fn = from_saved_model(_LATEST_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass dataset as a flag, later...\n",
    "dataset = 'test.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '{}/{}'.format(data_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset = tfrecord_dataset.map(lambda row: row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: find a way to perform two operations on map iterator:\n",
    "#  first, get the prediction with predict_fn, and\n",
    "#  second, get the target and the timestamp with a custom parsing\n",
    "#  (in order to avoid double iteration)\n",
    "iterator = tf.data.make_one_shot_iterator(mapped_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine this iterator\n",
    "# it is extremely slow!!!\n",
    "predictions_list = []\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            example = sess.run(next_element)\n",
    "            # print(example)\n",
    "            predictions_list.append(predict_fn({'example_bytes': example}))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scalar values from predictions list\n",
    "predictions = [p['forecast'][0][0] for p in predictions_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass predictions to an array\n",
    "predictions_array = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5441618 , 0.51966685, 0.4989715 , 0.30466068, 0.43263328,\n",
       "       0.485386  , 0.63434696, 0.5707552 , 0.36966532, 0.39843956,\n",
       "       0.23514184, 0.35869274, 0.299948  , 0.26620835, 0.28182584,\n",
       "       0.27639094, 0.3767143 , 0.35652915, 0.3824992 , 0.26447755,\n",
       "       0.46342632, 0.6080722 , 0.6079163 , 0.59445155, 0.54838866,\n",
       "       0.57579786, 0.48318502, 0.3260118 , 0.17182802, 0.65886503,\n",
       "       0.47180066, 0.6183016 , 0.5201679 , 0.36135674, 0.3050614 ,\n",
       "       0.37105584, 0.25564414, 0.28394815, 0.25962022, 0.2327499 ,\n",
       "       0.31193796, 0.36735025, 0.4288793 , 0.32161957, 0.43811166,\n",
       "       0.24301349, 0.13896659, 0.39845747, 0.65929943, 0.56995124,\n",
       "       0.4806725 , 0.39882636, 0.5090357 , 0.53697914, 0.3980365 ,\n",
       "       0.537365  , 0.40750906, 0.41606393, 0.3672334 , 0.2501844 ,\n",
       "       0.27875525, 0.31571215, 0.3384695 , 0.23450202, 0.39851338,\n",
       "       0.56957823, 0.35865355, 0.49883685, 0.51750875, 0.63353014,\n",
       "       0.65299577, 0.47594863, 0.5254238 , 0.45741257, 0.536555  ,\n",
       "       0.47880566, 0.41173476, 0.70532894, 0.681203  , 0.52582365,\n",
       "       0.40687773, 0.2254586 , 0.24482468, 0.24448155, 0.23430741,\n",
       "       0.3532546 , 0.31501752, 0.33480504, 0.36263606, 0.4947867 ,\n",
       "       0.33369574, 0.47175428, 0.2193084 , 0.30168256, 0.44399458,\n",
       "       0.54472846, 0.5112545 , 0.601574  , 0.36530754, 0.4321936 ,\n",
       "       0.47546715, 0.6169021 , 0.5163631 , 0.5352725 , 0.44657215,\n",
       "       0.34694332, 0.27263698, 0.26544538, 0.23699488, 0.21782917,\n",
       "       0.21789774, 0.24216796, 0.28807068, 0.4598863 , 0.27395612,\n",
       "       0.36282247, 0.3703082 , 0.50119627, 0.36895466, 0.46173823,\n",
       "       0.432641  , 0.4313145 , 0.5707673 , 0.5118334 , 0.4948613 ,\n",
       "       0.5279328 , 0.55199695, 0.5559032 , 0.34370193, 0.21023814],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it possible to load the scaler directly from Google Storage?\n",
    "# unfortunately, not...\n",
    "# scaler = joblib.load('scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the scaler file in GCP\n",
    "scaler_path = '{}/scaler.save'.format(data_dir.replace('gs://cbidmltsf/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily download the scaler to VM's block storage, use the original name\n",
    "blob.download_to_filename('temp/scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = joblib.load('temp/scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse-scale predictions\n",
    "predictions = scaler.inverse_transform(predictions_array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all dimensions equal to 1 in the predictions array\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass final prediction values to list for json serialization\n",
    "prediction_values_list = predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding dimensions of SLDB can be retrieved from sldb/sldb.json\n",
    "# or from the stats/sldb_parameters.json (both in Google Storage)\n",
    "# use the json file in stats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the json file in GCP\n",
    "sldb_json_path = '{}/sldb_parameters.json'.format(model_dir.replace('gs://cbidmltsf/models', 'stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats/DMSLSTM_TPU_70_00/sldb_parameters.json'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(sldb_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily download the sldb json to VM's block storage, use the original name\n",
    "blob.download_to_filename('temp/sldb_parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the sldb dictionary from the sldb json file in vm's block storage\n",
    "with open('temp/sldb_parameters.json', 'r') as filename:\n",
    "    sldb_parameters = json.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the objective shapes for reshaping tensors in a dictionary\n",
    "_EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "    'hourly': [sldb_parameters['embedding']['hourly'], 1],\n",
    "    'daily': [sldb_parameters['embedding']['daily'], 1],\n",
    "    'weekly': [sldb_parameters['embedding']['weekly'], 1],\n",
    "    # number of targets is included in hourly, daily, and weekly features, take anyone of them\n",
    "    # ToDo: un-wire this!\n",
    "    # 'target': [sldb['components']['hourly']['no_targets'], 1],\n",
    "    'target': [1, 1],\n",
    "    'oh_wd': [7, 1],  # Monday to Sunday\n",
    "    'oh_dh': [24, 1],  # midnight to 23:00\n",
    "    # number of targets is included in hourly, daily, and weekly features, take anyone of them\n",
    "    # ToDo: un-wire this!\n",
    "    # 'timestamp': [sldb['components']['hourly']['no_targets'], 1]\n",
    "    'timestamp': [1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset was previously acquired from tfrecord file\n",
    "# use it again to build arrays for targets and timestamps\n",
    "parsed_dataset = tfrecord_dataset.map(lambda row: _parse_dataset_function(example_proto=row,\n",
    "                                                                          objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "                                                                          parse_timestamp=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.make_one_shot_iterator(parsed_dataset)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: merge the operations in this iterator in the previous use of the same iterator???\n",
    "timestamps_list = []\n",
    "targets_list = []\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            parsed_example = sess.run(next_element)\n",
    "            # print(example)\n",
    "            # parsed_example = _parse_dataset_function(example_proto=example,\n",
    "            #                                          objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "            #                                          parse_timestamp=True)\n",
    "            # print(parsed_example)\n",
    "            # dplstm.data._parse_dataset_function returns (feature_dict, target[0]), then\n",
    "            # parsed_example[0] = feature_dict,\n",
    "            # parsed_example[1] = target[0]\n",
    "            # get the timestamp, then the scalar value, then the string value, then convert it to datetime\n",
    "            # ToDo: isolate the string timestamp from parsed_example first,\n",
    "            #  then apply decoding and conversion to datetime.\n",
    "            #  This way a string timestamp list is obtained for use in json results file\n",
    "            timestamps_list.append(to_datetime(parsed_example[0]['timestamp'][0][0].decode()))\n",
    "            # get the target, then the scalar value\n",
    "            # targets_list.append(item['target'][0][0])\n",
    "            targets_list.append(parsed_example[1][0])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.asarray(timestamps_list)\n",
    "# get the timestamps_list in strings, to better persist values\n",
    "string_timestamps_list = [str(item) for item in timestamps_list]\n",
    "targets = np.asarray(targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = scaler.inverse_transform(targets.reshape(-1, 1))\n",
    "targets = np.squeeze(targets)\n",
    "# NumPy array to list for json serialization\n",
    "target_values_list = targets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage prediction results\n",
    "prediction_results = {\n",
    "    'string_timestamps': string_timestamps_list,\n",
    "    'predictions': prediction_values_list,\n",
    "    'targets': target_values_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: replace this dictionary with a data frame? (Pandas?, csv?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the results dictionary to json\n",
    "output_file_name = 'temp/prediction_results_on_{}.json'.format(dataset.replace('.', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/prediction_results_on_test_tfrecord.json'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name, 'w') as outfile:\n",
    "    json.dump(prediction_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now upload the resulting json file in /temp to gs://cbidmltsf/stats/MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to stats directory in Google Storage\n",
    "# use the model dir as a base\n",
    "stats_path = model_dir.replace('gs://cbidmltsf/models/', 'stats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats/DMSLSTM_TPU_70_00'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_blob_id = '{}/{}'.format(stats_path, output_file_name.replace('temp/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats/DMSLSTM_TPU_70_00/prediction_results_on_test_tfrecord.json'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_blob_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now upload prediction results json file from vm block storage to hparams['model_dir']\n",
    "results_blob = bucket.blob(results_blob_id)\n",
    "results_blob.upload_from_filename(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prototype is now ready for coding the final script, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
