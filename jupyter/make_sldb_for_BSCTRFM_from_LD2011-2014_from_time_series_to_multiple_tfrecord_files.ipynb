{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
    "  warnings.warn(msg, category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365\n",
    "# weeks_of_year and month_of_year become redundant when using days_of_year, do not evaluate them\n",
    "# weeks_in_year = 52\n",
    "# months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 15:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "# 243 weeks for seen data, 1 week for unseen data\n",
    "no_lectures_seen_data = 243*24 # 5832\n",
    "\n",
    "# seen data is divided as 90% for training and 10% for evaluation\n",
    "train_eval_limit = 0.9\n",
    "\n",
    "train_interval_end = int(no_lectures_seen_data*train_eval_limit) # 5248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_FULL',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_FULL',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_FULL_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168 already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a directory for the complete SLDB\n",
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168/train already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the training TFRecord files\n",
    "try:\n",
    "    os.mkdir('{}/train'.format(sldb_dir))\n",
    "    print('Directory {}/train was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {}/train already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168/eval already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the evaluation TFRecord files\n",
    "try:\n",
    "    os.mkdir('{}/eval'.format(sldb_dir))\n",
    "    print('Directory {}/eval was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {}/eval already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168/scalers'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a path to the scalers sub-directory\n",
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_FULL_BSCTRFM_168_168/scalers already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the scalers\n",
    "try:\n",
    "    os.mkdir(scalers_dir)\n",
    "    print('Directory {} was created.'.format(scalers_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(scalers_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not read the complete dataset!\n",
    "\n",
    "# output = pd.read_pickle('{}/hourly_electricity_complete.pkl'.format(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not filter the complete dataset!\n",
    "\n",
    "# filter to match range used by other academic papers\n",
    "# filtered_output = output[(output['days_from_start'] >= 1096) & (output['days_from_start'] < 1346)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not persist the filtered dataset again!\n",
    "\n",
    "# persist the filtered dataset to avoid using memory for the complete dataset\n",
    "# filtered_output.to_pickle('{}/hourly_electricity_filtered_academic_papers.pkl'.format(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just read the filtered_output dataframe\n",
    "filtered_output = pd.read_pickle('{}/hourly_electricity_filtered_academic_papers.pkl'.format(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage a MinMaxScaler per individual customer_id\n",
    "min_max = dict()\n",
    "# a dictionary to manage a StandardScaler per individual customer_id\n",
    "standard = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of cores available for training in Cloud TPU\n",
    "num_cores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 301, 370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [token_id for token_id in np.arange(start, end + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for SLDB generation, run this unified code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing for MT_301\n",
      "MinMax scaler generated on training data for MT_301\n",
      "MinMax scaler persisted for MT_301\n",
      "MT_301 processed. The number of examples in train dataset is 4913\n",
      "MT_301 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_301 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_301 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_301\n",
      "Persisted eval TFRecord file for MT_301\n",
      "Started processing for MT_302\n",
      "MinMax scaler generated on training data for MT_302\n",
      "MinMax scaler persisted for MT_302\n",
      "MT_302 processed. The number of examples in train dataset is 4913\n",
      "MT_302 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_302 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_302 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_302\n",
      "Persisted eval TFRecord file for MT_302\n",
      "Started processing for MT_303\n",
      "MinMax scaler generated on training data for MT_303\n",
      "MinMax scaler persisted for MT_303\n",
      "MT_303 processed. The number of examples in train dataset is 4913\n",
      "MT_303 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_303 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_303 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_303\n",
      "Persisted eval TFRecord file for MT_303\n",
      "Started processing for MT_304\n",
      "MinMax scaler generated on training data for MT_304\n",
      "MinMax scaler persisted for MT_304\n",
      "MT_304 processed. The number of examples in train dataset is 4913\n",
      "MT_304 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_304 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_304 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_304\n",
      "Persisted eval TFRecord file for MT_304\n",
      "Started processing for MT_305\n",
      "MinMax scaler generated on training data for MT_305\n",
      "MinMax scaler persisted for MT_305\n",
      "MT_305 processed. The number of examples in train dataset is 4913\n",
      "MT_305 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_305 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_305 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_305\n",
      "Persisted eval TFRecord file for MT_305\n",
      "Started processing for MT_306\n",
      "MinMax scaler generated on training data for MT_306\n",
      "MinMax scaler persisted for MT_306\n",
      "MT_306 processed. The number of examples in train dataset is 4913\n",
      "MT_306 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_306 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_306 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_306\n",
      "Persisted eval TFRecord file for MT_306\n",
      "Started processing for MT_307\n",
      "MinMax scaler generated on training data for MT_307\n",
      "MinMax scaler persisted for MT_307\n",
      "MT_307 processed. The number of examples in train dataset is 4913\n",
      "MT_307 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_307 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_307 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_307\n",
      "Persisted eval TFRecord file for MT_307\n",
      "Started processing for MT_308\n",
      "MinMax scaler generated on training data for MT_308\n",
      "MinMax scaler persisted for MT_308\n",
      "MT_308 processed. The number of examples in train dataset is 4913\n",
      "MT_308 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_308 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_308 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_308\n",
      "Persisted eval TFRecord file for MT_308\n",
      "Started processing for MT_309\n",
      "MinMax scaler generated on training data for MT_309\n",
      "MinMax scaler persisted for MT_309\n",
      "MT_309 processed. The number of examples in train dataset is 4913\n",
      "MT_309 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_309 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_309 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_309\n",
      "Persisted eval TFRecord file for MT_309\n",
      "Started processing for MT_310\n",
      "MinMax scaler generated on training data for MT_310\n",
      "MinMax scaler persisted for MT_310\n",
      "MT_310 processed. The number of examples in train dataset is 4913\n",
      "MT_310 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_310 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_310 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_310\n",
      "Persisted eval TFRecord file for MT_310\n",
      "Started processing for MT_311\n",
      "MinMax scaler generated on training data for MT_311\n",
      "MinMax scaler persisted for MT_311\n",
      "MT_311 processed. The number of examples in train dataset is 4913\n",
      "MT_311 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_311 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_311 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_311\n",
      "Persisted eval TFRecord file for MT_311\n",
      "Started processing for MT_312\n",
      "MinMax scaler generated on training data for MT_312\n",
      "MinMax scaler persisted for MT_312\n",
      "MT_312 processed. The number of examples in train dataset is 4913\n",
      "MT_312 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_312 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_312 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_312\n",
      "Persisted eval TFRecord file for MT_312\n",
      "Started processing for MT_313\n",
      "MinMax scaler generated on training data for MT_313\n",
      "MinMax scaler persisted for MT_313\n",
      "MT_313 processed. The number of examples in train dataset is 4913\n",
      "MT_313 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_313 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_313 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_313\n",
      "Persisted eval TFRecord file for MT_313\n",
      "Started processing for MT_314\n",
      "MinMax scaler generated on training data for MT_314\n",
      "MinMax scaler persisted for MT_314\n",
      "MT_314 processed. The number of examples in train dataset is 4913\n",
      "MT_314 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_314 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_314 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_314\n",
      "Persisted eval TFRecord file for MT_314\n",
      "Started processing for MT_315\n",
      "MinMax scaler generated on training data for MT_315\n",
      "MinMax scaler persisted for MT_315\n",
      "MT_315 processed. The number of examples in train dataset is 4913\n",
      "MT_315 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_315 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_315 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_315\n",
      "Persisted eval TFRecord file for MT_315\n",
      "Started processing for MT_316\n",
      "MinMax scaler generated on training data for MT_316\n",
      "MinMax scaler persisted for MT_316\n",
      "MT_316 processed. The number of examples in train dataset is 4913\n",
      "MT_316 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_316 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_316 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_316\n",
      "Persisted eval TFRecord file for MT_316\n",
      "Started processing for MT_317\n",
      "MinMax scaler generated on training data for MT_317\n",
      "MinMax scaler persisted for MT_317\n",
      "MT_317 processed. The number of examples in train dataset is 4913\n",
      "MT_317 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_317 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_317 was adjusted to 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted train TFRecord file for MT_317\n",
      "Persisted eval TFRecord file for MT_317\n",
      "Started processing for MT_318\n",
      "MinMax scaler generated on training data for MT_318\n",
      "MinMax scaler persisted for MT_318\n",
      "MT_318 processed. The number of examples in train dataset is 4913\n",
      "MT_318 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_318 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_318 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_318\n",
      "Persisted eval TFRecord file for MT_318\n",
      "Started processing for MT_319\n",
      "MinMax scaler generated on training data for MT_319\n",
      "MinMax scaler persisted for MT_319\n",
      "MT_319 processed. The number of examples in train dataset is 4913\n",
      "MT_319 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_319 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_319 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_319\n",
      "Persisted eval TFRecord file for MT_319\n",
      "Started processing for MT_320\n",
      "MinMax scaler generated on training data for MT_320\n",
      "MinMax scaler persisted for MT_320\n",
      "MT_320 processed. The number of examples in train dataset is 4913\n",
      "MT_320 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_320 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_320 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_320\n",
      "Persisted eval TFRecord file for MT_320\n",
      "Started processing for MT_321\n",
      "MinMax scaler generated on training data for MT_321\n",
      "MinMax scaler persisted for MT_321\n",
      "MT_321 processed. The number of examples in train dataset is 4913\n",
      "MT_321 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_321 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_321 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_321\n",
      "Persisted eval TFRecord file for MT_321\n",
      "Started processing for MT_322\n",
      "MinMax scaler generated on training data for MT_322\n",
      "MinMax scaler persisted for MT_322\n",
      "MT_322 processed. The number of examples in train dataset is 4913\n",
      "MT_322 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_322 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_322 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_322\n",
      "Persisted eval TFRecord file for MT_322\n",
      "Started processing for MT_323\n",
      "MinMax scaler generated on training data for MT_323\n",
      "MinMax scaler persisted for MT_323\n",
      "MT_323 processed. The number of examples in train dataset is 4913\n",
      "MT_323 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_323 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_323 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_323\n",
      "Persisted eval TFRecord file for MT_323\n",
      "Started processing for MT_324\n",
      "MinMax scaler generated on training data for MT_324\n",
      "MinMax scaler persisted for MT_324\n",
      "MT_324 processed. The number of examples in train dataset is 4913\n",
      "MT_324 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_324 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_324 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_324\n",
      "Persisted eval TFRecord file for MT_324\n",
      "Started processing for MT_325\n",
      "MinMax scaler generated on training data for MT_325\n",
      "MinMax scaler persisted for MT_325\n",
      "MT_325 processed. The number of examples in train dataset is 4913\n",
      "MT_325 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_325 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_325 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_325\n",
      "Persisted eval TFRecord file for MT_325\n",
      "Started processing for MT_326\n",
      "MinMax scaler generated on training data for MT_326\n",
      "MinMax scaler persisted for MT_326\n",
      "MT_326 processed. The number of examples in train dataset is 4913\n",
      "MT_326 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_326 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_326 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_326\n",
      "Persisted eval TFRecord file for MT_326\n",
      "Started processing for MT_327\n",
      "MinMax scaler generated on training data for MT_327\n",
      "MinMax scaler persisted for MT_327\n",
      "MT_327 processed. The number of examples in train dataset is 4913\n",
      "MT_327 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_327 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_327 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_327\n",
      "Persisted eval TFRecord file for MT_327\n",
      "Started processing for MT_328\n",
      "MinMax scaler generated on training data for MT_328\n",
      "MinMax scaler persisted for MT_328\n",
      "MT_328 processed. The number of examples in train dataset is 4913\n",
      "MT_328 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_328 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_328 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_328\n",
      "Persisted eval TFRecord file for MT_328\n",
      "Started processing for MT_329\n",
      "MinMax scaler generated on training data for MT_329\n",
      "MinMax scaler persisted for MT_329\n",
      "MT_329 processed. The number of examples in train dataset is 4913\n",
      "MT_329 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_329 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_329 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_329\n",
      "Persisted eval TFRecord file for MT_329\n",
      "Started processing for MT_330\n",
      "MinMax scaler generated on training data for MT_330\n",
      "MinMax scaler persisted for MT_330\n",
      "MT_330 processed. The number of examples in train dataset is 4913\n",
      "MT_330 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_330 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_330 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_330\n",
      "Persisted eval TFRecord file for MT_330\n",
      "Started processing for MT_331\n",
      "MinMax scaler generated on training data for MT_331\n",
      "MinMax scaler persisted for MT_331\n",
      "MT_331 processed. The number of examples in train dataset is 4913\n",
      "MT_331 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_331 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_331 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_331\n",
      "Persisted eval TFRecord file for MT_331\n",
      "Started processing for MT_332\n",
      "MinMax scaler generated on training data for MT_332\n",
      "MinMax scaler persisted for MT_332\n",
      "MT_332 processed. The number of examples in train dataset is 4913\n",
      "MT_332 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_332 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_332 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_332\n",
      "Persisted eval TFRecord file for MT_332\n",
      "Started processing for MT_333\n",
      "MinMax scaler generated on training data for MT_333\n",
      "MinMax scaler persisted for MT_333\n",
      "MT_333 processed. The number of examples in train dataset is 4913\n",
      "MT_333 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_333 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_333 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_333\n",
      "Persisted eval TFRecord file for MT_333\n",
      "Started processing for MT_334\n",
      "MinMax scaler generated on training data for MT_334\n",
      "MinMax scaler persisted for MT_334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_334 processed. The number of examples in train dataset is 4913\n",
      "MT_334 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_334 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_334 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_334\n",
      "Persisted eval TFRecord file for MT_334\n",
      "Started processing for MT_335\n",
      "MinMax scaler generated on training data for MT_335\n",
      "MinMax scaler persisted for MT_335\n",
      "MT_335 processed. The number of examples in train dataset is 4913\n",
      "MT_335 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_335 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_335 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_335\n",
      "Persisted eval TFRecord file for MT_335\n",
      "Started processing for MT_336\n",
      "MinMax scaler generated on training data for MT_336\n",
      "MinMax scaler persisted for MT_336\n",
      "MT_336 processed. The number of examples in train dataset is 4913\n",
      "MT_336 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_336 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_336 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_336\n",
      "Persisted eval TFRecord file for MT_336\n",
      "Started processing for MT_337\n",
      "MinMax scaler generated on training data for MT_337\n",
      "MinMax scaler persisted for MT_337\n",
      "MT_337 processed. The number of examples in train dataset is 4913\n",
      "MT_337 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_337 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_337 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_337\n",
      "Persisted eval TFRecord file for MT_337\n",
      "Started processing for MT_338\n",
      "MinMax scaler generated on training data for MT_338\n",
      "MinMax scaler persisted for MT_338\n",
      "MT_338 processed. The number of examples in train dataset is 4913\n",
      "MT_338 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_338 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_338 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_338\n",
      "Persisted eval TFRecord file for MT_338\n",
      "Started processing for MT_339\n",
      "MinMax scaler generated on training data for MT_339\n",
      "MinMax scaler persisted for MT_339\n",
      "MT_339 processed. The number of examples in train dataset is 4913\n",
      "MT_339 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_339 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_339 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_339\n",
      "Persisted eval TFRecord file for MT_339\n",
      "Started processing for MT_340\n",
      "MinMax scaler generated on training data for MT_340\n",
      "MinMax scaler persisted for MT_340\n",
      "MT_340 processed. The number of examples in train dataset is 4913\n",
      "MT_340 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_340 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_340 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_340\n",
      "Persisted eval TFRecord file for MT_340\n",
      "Started processing for MT_341\n",
      "MinMax scaler generated on training data for MT_341\n",
      "MinMax scaler persisted for MT_341\n",
      "MT_341 processed. The number of examples in train dataset is 4913\n",
      "MT_341 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_341 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_341 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_341\n",
      "Persisted eval TFRecord file for MT_341\n",
      "Started processing for MT_342\n",
      "MinMax scaler generated on training data for MT_342\n",
      "MinMax scaler persisted for MT_342\n",
      "MT_342 processed. The number of examples in train dataset is 4913\n",
      "MT_342 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_342 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_342 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_342\n",
      "Persisted eval TFRecord file for MT_342\n",
      "Started processing for MT_343\n",
      "MinMax scaler generated on training data for MT_343\n",
      "MinMax scaler persisted for MT_343\n",
      "MT_343 processed. The number of examples in train dataset is 4913\n",
      "MT_343 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_343 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_343 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_343\n",
      "Persisted eval TFRecord file for MT_343\n",
      "Started processing for MT_344\n",
      "MinMax scaler generated on training data for MT_344\n",
      "MinMax scaler persisted for MT_344\n",
      "MT_344 processed. The number of examples in train dataset is 4913\n",
      "MT_344 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_344 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_344 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_344\n",
      "Persisted eval TFRecord file for MT_344\n",
      "Started processing for MT_345\n",
      "MinMax scaler generated on training data for MT_345\n",
      "MinMax scaler persisted for MT_345\n",
      "MT_345 processed. The number of examples in train dataset is 4913\n",
      "MT_345 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_345 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_345 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_345\n",
      "Persisted eval TFRecord file for MT_345\n",
      "Started processing for MT_346\n",
      "MinMax scaler generated on training data for MT_346\n",
      "MinMax scaler persisted for MT_346\n",
      "MT_346 processed. The number of examples in train dataset is 4913\n",
      "MT_346 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_346 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_346 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_346\n",
      "Persisted eval TFRecord file for MT_346\n",
      "Started processing for MT_347\n",
      "MinMax scaler generated on training data for MT_347\n",
      "MinMax scaler persisted for MT_347\n",
      "MT_347 processed. The number of examples in train dataset is 4913\n",
      "MT_347 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_347 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_347 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_347\n",
      "Persisted eval TFRecord file for MT_347\n",
      "Started processing for MT_348\n",
      "MinMax scaler generated on training data for MT_348\n",
      "MinMax scaler persisted for MT_348\n",
      "MT_348 processed. The number of examples in train dataset is 4913\n",
      "MT_348 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_348 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_348 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_348\n",
      "Persisted eval TFRecord file for MT_348\n",
      "Started processing for MT_349\n",
      "MinMax scaler generated on training data for MT_349\n",
      "MinMax scaler persisted for MT_349\n",
      "MT_349 processed. The number of examples in train dataset is 4913\n",
      "MT_349 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_349 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_349 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_349\n",
      "Persisted eval TFRecord file for MT_349\n",
      "Started processing for MT_350\n",
      "MinMax scaler generated on training data for MT_350\n",
      "MinMax scaler persisted for MT_350\n",
      "MT_350 processed. The number of examples in train dataset is 4913\n",
      "MT_350 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_350 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_350 was adjusted to 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted train TFRecord file for MT_350\n",
      "Persisted eval TFRecord file for MT_350\n",
      "Started processing for MT_351\n",
      "MinMax scaler generated on training data for MT_351\n",
      "MinMax scaler persisted for MT_351\n",
      "MT_351 processed. The number of examples in train dataset is 4913\n",
      "MT_351 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_351 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_351 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_351\n",
      "Persisted eval TFRecord file for MT_351\n",
      "Started processing for MT_352\n",
      "MinMax scaler generated on training data for MT_352\n",
      "MinMax scaler persisted for MT_352\n",
      "MT_352 processed. The number of examples in train dataset is 4913\n",
      "MT_352 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_352 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_352 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_352\n",
      "Persisted eval TFRecord file for MT_352\n",
      "Started processing for MT_353\n",
      "MinMax scaler generated on training data for MT_353\n",
      "MinMax scaler persisted for MT_353\n",
      "MT_353 processed. The number of examples in train dataset is 4913\n",
      "MT_353 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_353 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_353 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_353\n",
      "Persisted eval TFRecord file for MT_353\n",
      "Started processing for MT_354\n",
      "MinMax scaler generated on training data for MT_354\n",
      "MinMax scaler persisted for MT_354\n",
      "MT_354 processed. The number of examples in train dataset is 4913\n",
      "MT_354 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_354 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_354 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_354\n",
      "Persisted eval TFRecord file for MT_354\n",
      "Started processing for MT_355\n",
      "MinMax scaler generated on training data for MT_355\n",
      "MinMax scaler persisted for MT_355\n",
      "MT_355 processed. The number of examples in train dataset is 4913\n",
      "MT_355 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_355 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_355 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_355\n",
      "Persisted eval TFRecord file for MT_355\n",
      "Started processing for MT_356\n",
      "MinMax scaler generated on training data for MT_356\n",
      "MinMax scaler persisted for MT_356\n",
      "MT_356 processed. The number of examples in train dataset is 4913\n",
      "MT_356 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_356 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_356 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_356\n",
      "Persisted eval TFRecord file for MT_356\n",
      "Started processing for MT_357\n",
      "MinMax scaler generated on training data for MT_357\n",
      "MinMax scaler persisted for MT_357\n",
      "MT_357 processed. The number of examples in train dataset is 4913\n",
      "MT_357 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_357 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_357 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_357\n",
      "Persisted eval TFRecord file for MT_357\n",
      "Started processing for MT_358\n",
      "MinMax scaler generated on training data for MT_358\n",
      "MinMax scaler persisted for MT_358\n",
      "MT_358 processed. The number of examples in train dataset is 4913\n",
      "MT_358 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_358 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_358 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_358\n",
      "Persisted eval TFRecord file for MT_358\n",
      "Started processing for MT_359\n",
      "MinMax scaler generated on training data for MT_359\n",
      "MinMax scaler persisted for MT_359\n",
      "MT_359 processed. The number of examples in train dataset is 4913\n",
      "MT_359 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_359 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_359 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_359\n",
      "Persisted eval TFRecord file for MT_359\n",
      "Started processing for MT_360\n",
      "MinMax scaler generated on training data for MT_360\n",
      "MinMax scaler persisted for MT_360\n",
      "MT_360 processed. The number of examples in train dataset is 4913\n",
      "MT_360 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_360 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_360 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_360\n",
      "Persisted eval TFRecord file for MT_360\n",
      "Started processing for MT_361\n",
      "MinMax scaler generated on training data for MT_361\n",
      "MinMax scaler persisted for MT_361\n",
      "MT_361 processed. The number of examples in train dataset is 4913\n",
      "MT_361 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_361 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_361 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_361\n",
      "Persisted eval TFRecord file for MT_361\n",
      "Started processing for MT_362\n",
      "MinMax scaler generated on training data for MT_362\n",
      "MinMax scaler persisted for MT_362\n",
      "MT_362 processed. The number of examples in train dataset is 4913\n",
      "MT_362 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_362 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_362 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_362\n",
      "Persisted eval TFRecord file for MT_362\n",
      "Started processing for MT_363\n",
      "MinMax scaler generated on training data for MT_363\n",
      "MinMax scaler persisted for MT_363\n",
      "MT_363 processed. The number of examples in train dataset is 4913\n",
      "MT_363 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_363 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_363 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_363\n",
      "Persisted eval TFRecord file for MT_363\n",
      "Started processing for MT_364\n",
      "MinMax scaler generated on training data for MT_364\n",
      "MinMax scaler persisted for MT_364\n",
      "MT_364 processed. The number of examples in train dataset is 4913\n",
      "MT_364 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_364 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_364 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_364\n",
      "Persisted eval TFRecord file for MT_364\n",
      "Started processing for MT_365\n",
      "MinMax scaler generated on training data for MT_365\n",
      "MinMax scaler persisted for MT_365\n",
      "MT_365 processed. The number of examples in train dataset is 4913\n",
      "MT_365 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_365 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_365 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_365\n",
      "Persisted eval TFRecord file for MT_365\n",
      "Started processing for MT_366\n",
      "MinMax scaler generated on training data for MT_366\n",
      "MinMax scaler persisted for MT_366\n",
      "MT_366 processed. The number of examples in train dataset is 4913\n",
      "MT_366 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_366 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_366 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_366\n",
      "Persisted eval TFRecord file for MT_366\n",
      "Started processing for MT_367\n",
      "MinMax scaler generated on training data for MT_367\n",
      "MinMax scaler persisted for MT_367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_367 processed. The number of examples in train dataset is 4913\n",
      "MT_367 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_367 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_367 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_367\n",
      "Persisted eval TFRecord file for MT_367\n",
      "Started processing for MT_368\n",
      "MinMax scaler generated on training data for MT_368\n",
      "MinMax scaler persisted for MT_368\n",
      "MT_368 processed. The number of examples in train dataset is 4913\n",
      "MT_368 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_368 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_368 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_368\n",
      "Persisted eval TFRecord file for MT_368\n",
      "Started processing for MT_369\n",
      "MinMax scaler generated on training data for MT_369\n",
      "MinMax scaler persisted for MT_369\n",
      "MT_369 processed. The number of examples in train dataset is 4913\n",
      "MT_369 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_369 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_369 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_369\n",
      "Persisted eval TFRecord file for MT_369\n",
      "Started processing for MT_370\n",
      "MinMax scaler generated on training data for MT_370\n",
      "MinMax scaler persisted for MT_370\n",
      "MT_370 processed. The number of examples in train dataset is 4913\n",
      "MT_370 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_370 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_370 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_370\n",
      "Persisted eval TFRecord file for MT_370\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    \n",
    "    # initialize the examples dictionary for each customer\n",
    "    examples = {\n",
    "        'train': [],\n",
    "        'eval': [],\n",
    "        # test dataset is not passed to SLDB\n",
    "        # 'test': []\n",
    "    }\n",
    "\n",
    "    # get the customer identifier\n",
    "    customer_id = 'MT_{:03d}'.format(token_id)\n",
    "    customer_id\n",
    "    print('Started processing for {}'.format(customer_id))\n",
    "\n",
    "    # a temporary dataframe with data per customer_id to build the sub-series/examples\n",
    "    data_df = filtered_output[filtered_output['token_id'] == token_id].copy()\n",
    "\n",
    "    # expand with positional encodings\n",
    "    data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['sin_day_month'] = np.sin(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['cos_day_month'] = np.cos(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['sin_day_year'] = np.sin(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "    data_df['cos_day_year'] = np.cos(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "\n",
    "    # get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "    lectures_train_data = data_df['power_usage'][:train_interval_end]\n",
    "\n",
    "    # fit a scaler only on train data\n",
    "    # it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "    lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)\n",
    "\n",
    "    # get MinMaxScaler on train data, store it in a dictionary\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max = min_max_scaler.fit(lectures_train_data_array)\n",
    "    print('MinMax scaler generated on training data for {}'.format(customer_id))\n",
    "\n",
    "    # persist the scaler\n",
    "    scaler_filename = '{}/{}_min_max.save'.format(scalers_dir, customer_id)\n",
    "    joblib.dump(min_max, scaler_filename)\n",
    "    print('MinMax scaler persisted for {}'.format(customer_id))\n",
    "\n",
    "    \n",
    "    # get an array from the variable time series (seen and unseen)\n",
    "    all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)\n",
    "\n",
    "    # apply the scaler over all data (seen and unseen)\n",
    "    # rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "    data_df['kw_scaled'] = np.squeeze(min_max.transform(all_data_variable_array))\n",
    "\n",
    "    # get an iterable with all the possible sub-series for training examples\n",
    "    for starting_point in np.arange(train_interval_end - (m + t) + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + (m + t)]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['train'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'train', len(examples['train'])))\n",
    "    \n",
    "    \n",
    "    # ToDo: remove evaluation step from Cloud TPU training and use all seen data for training stage\n",
    "    build_eval_set = True\n",
    "    \n",
    "    if build_eval_set:\n",
    "\n",
    "        # get an iterable with all the possible sub-series for evaluation examples\n",
    "        for starting_point in np.arange(train_interval_end, no_lectures_seen_data - (m + t) + 1):\n",
    "\n",
    "            sub_series_df = data_df[sldb_columns][starting_point:starting_point + (m + t)]\n",
    "\n",
    "            encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "            decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "            target_df = sub_series_df[target_columns][m:m+t]\n",
    "            id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "            encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "            examples['eval'].append(\n",
    "                {\n",
    "                    'encoder_input': encoder_input_list,\n",
    "                    'decoder_input': decoder_input_list,\n",
    "                    'target': target_list,\n",
    "                    'id': id_list,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "              format(customer_id, 'eval', len(examples['eval'])))\n",
    "    \n",
    "    \n",
    "    # DO NOT PRODUCE A TEST DATASET FOR SLDB, AS INFERENCE PROCESS IS NOT DIRECT\n",
    "    # (IT IS ITERATIVE OVER UNSEEN DATA TIME SERIES)\n",
    "    \n",
    "    build_test_set = False\n",
    "    \n",
    "    if build_test_set:\n",
    "\n",
    "        # remember that conditional range of test dataset overlaps with evaluation dataset\n",
    "        # for this experiment design# get an iterable with all the possible sub-series for test examples\n",
    "        for starting_point in no_lectures_seen_data - (m + t) + 1 + np.arange(168):\n",
    "\n",
    "            sub_series_df = data_df[sldb_columns][starting_point:starting_point + (m + t)]\n",
    "\n",
    "            encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "            decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "            target_df = sub_series_df[target_columns][m:m+t]\n",
    "            id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "            encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "            examples['test'].append(\n",
    "                {\n",
    "                    'encoder_input': encoder_input_list,\n",
    "                    'decoder_input': decoder_input_list,\n",
    "                    'target': target_list,\n",
    "                    'id': id_list,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "              format(customer_id, 'test', len(examples['test'])))\n",
    "    \n",
    "    \n",
    "    # on each customer dataset, adjust the number of examples to the number of training cores\n",
    "    for stage in ['train', 'eval']:\n",
    "        # how many examples/rows must be removed from examples[stage] to comply with the number of cores\n",
    "        examples_to_remove = len(examples[stage])%num_cores\n",
    "\n",
    "        # remove the last 'examples_to_remove' examples from the dataset\n",
    "        for _ in np.arange(examples_to_remove):\n",
    "            examples[stage].pop(-1)\n",
    "\n",
    "        print('For {} cores in Cloud TPU, the number of {} examples for {} was adjusted to {}'.\\\n",
    "             format(num_cores, stage, customer_id, len(examples[stage])))\n",
    "\n",
    "\n",
    "    # serialize the rows in examples['train'] and, if present, examples['eval']\n",
    "    # to avoid excesive memory consumption\n",
    "    \n",
    "    # write a TFRecord file for each consumer_id/stage\n",
    "    for stage in ['train', 'eval']:\n",
    "        # N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "        N_ROWS = len(examples[stage])\n",
    "        filename = '{}/{}/{}.tfrecord'.format(sldb_dir, stage, customer_id)\n",
    "\n",
    "        with tf.io.TFRecordWriter(filename) as writer:\n",
    "            for row in np.arange(N_ROWS):\n",
    "\n",
    "                example = tf.train.Example(\n",
    "                    # features within the example\n",
    "                    features=tf.train.Features(\n",
    "                        # individual feature definition\n",
    "                        feature={'encoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['encoder_input']),\n",
    "                                 'decoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['decoder_input']),\n",
    "                                 'target':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['target']),\n",
    "                                 'id':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['id'])\n",
    "                                 }\n",
    "                    )\n",
    "                )\n",
    "                serialized_example = example.SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "                \n",
    "            # report TFRecord file as completed\n",
    "            print('Persisted {} TFRecord file for {}'.format(stage, customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the sldb dictionary with final statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'n_rows': len(examples['train'])\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_rows': len(examples['eval'])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_FULL',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'stats': {'train': {'n_rows': 4912}, 'eval': {'n_rows': 248}}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the sldb from the examples dictionary (keys are stages, values are lists of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo: read TFRecord file into a Dataset and confirm the values in the source dataframe!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "- [4 files][130.9 MiB/130.9 MiB]    4.2 MiB/s                                   \n",
      "Operation completed over 4 objects/130.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!\n",
    "# !gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: random sample the serialization of examples to TFRecord SLDB!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
