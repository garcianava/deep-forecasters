{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for TFA MultiHeadAttention\n",
    "import typing\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA class from TensorFlow AddOns source\n",
    "# it is compatible with TF 1.15 for CloudTPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    r\"\"\"MultiHead Attention layer.\n",
    "    Defines the MultiHead Attention operation as described in\n",
    "    [Attention Is All You Need](https://arxiv.org/abs/1706.03762) which takes\n",
    "    in the tensors `query`, `key`, and `value`, and returns the dot-product attention\n",
    "    between them:\n",
    "    >>> mha = MultiHeadAttention(head_size=128, num_heads=12)\n",
    "    >>> query = np.random.rand(3, 5, 4) # (batch_size, query_elements, query_depth)\n",
    "    >>> key = np.random.rand(3, 6, 5) # (batch_size, key_elements, key_depth)\n",
    "    >>> value = np.random.rand(3, 6, 6) # (batch_size, key_elements, value_depth)\n",
    "    >>> attention = mha([query, key, value]) # (batch_size, query_elements, value_depth)\n",
    "    >>> attention.shape\n",
    "    TensorShape([3, 5, 6])\n",
    "    If `value` is not given then internally `value = key` will be used:\n",
    "    >>> mha = MultiHeadAttention(head_size=128, num_heads=12)\n",
    "    >>> query = np.random.rand(3, 5, 5) # (batch_size, query_elements, query_depth)\n",
    "    >>> key = np.random.rand(3, 6, 10) # (batch_size, key_elements, key_depth)\n",
    "    >>> attention = mha([query, key]) # (batch_size, query_elements, key_depth)\n",
    "    >>> attention.shape\n",
    "    TensorShape([3, 5, 10])\n",
    "    Args:\n",
    "        head_size: int, dimensionality of the `query`, `key` and `value` tensors\n",
    "            after the linear transformation.\n",
    "        num_heads: int, number of attention heads.\n",
    "        output_size: int, dimensionality of the output space, if `None` then the\n",
    "            input dimension of `value` or `key` will be used,\n",
    "            default `None`.\n",
    "        dropout: float, `rate` parameter for the dropout layer that is\n",
    "            applied to attention after softmax,\n",
    "        default `0`.\n",
    "        use_projection_bias: bool, whether to use a bias term after the linear\n",
    "            output projection.\n",
    "        return_attn_coef: bool, if `True`, return the attention coefficients as\n",
    "            an additional output argument.\n",
    "        kernel_initializer: initializer, initializer for the kernel weights.\n",
    "        kernel_regularizer: regularizer, regularizer for the kernel weights.\n",
    "        kernel_constraint: constraint, constraint for the kernel weights.\n",
    "        bias_initializer: initializer, initializer for the bias weights.\n",
    "        bias_regularizer: regularizer, regularizer for the bias weights.\n",
    "        bias_constraint: constraint, constraint for the bias weights.\n",
    "    Call Args:\n",
    "        inputs:  List of `[query, key, value]` where\n",
    "            * `query`: Tensor of shape `(..., query_elements, query_depth)`\n",
    "            * `key`: `Tensor of shape '(..., key_elements, key_depth)`\n",
    "            * `value`: Tensor of shape `(..., key_elements, value_depth)`, optional, if not given `key` will be used.\n",
    "        mask: a binary Tensor of shape `[batch_size?, num_heads?, query_elements, key_elements]`\n",
    "        which specifies which query elements can attend to which key elements,\n",
    "        `1` indicates attention and `0` indicates no attention.\n",
    "    Output shape:\n",
    "        * `(..., query_elements, output_size)` if `output_size` is given, else\n",
    "        * `(..., query_elements, value_depth)` if `value` is given, else\n",
    "        * `(..., query_elements, key_depth)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        num_heads: int,\n",
    "        output_size: int = None,\n",
    "        dropout: float = 0.0,\n",
    "        use_projection_bias: bool = True,\n",
    "        return_attn_coef: bool = False,\n",
    "        kernel_initializer: typing.Union[str, typing.Callable] = \"glorot_uniform\",\n",
    "        kernel_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        kernel_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        bias_initializer: typing.Union[str, typing.Callable] = \"zeros\",\n",
    "        bias_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        bias_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        warnings.warn(\n",
    "            \"`MultiHeadAttention` will be deprecated in Addons 0.13. \"\n",
    "            \"Please use `tf.keras.layers.MultiHeadAttention` instead.\",\n",
    "            DeprecationWarning,\n",
    "        )\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if output_size is not None and output_size < 1:\n",
    "            raise ValueError(\"output_size must be a positive number\")\n",
    "\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.output_size = output_size\n",
    "        self.use_projection_bias = use_projection_bias\n",
    "        self.return_attn_coef = return_attn_coef\n",
    "\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self._dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        num_query_features = input_shape[0][-1]\n",
    "        num_key_features = input_shape[1][-1]\n",
    "        num_value_features = (\n",
    "            input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n",
    "        )\n",
    "        output_size = (\n",
    "            self.output_size if self.output_size is not None else num_value_features\n",
    "        )\n",
    "\n",
    "        self.query_kernel = self.add_weight(\n",
    "            name=\"query_kernel\",\n",
    "            shape=[self.num_heads, num_query_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.key_kernel = self.add_weight(\n",
    "            name=\"key_kernel\",\n",
    "            shape=[self.num_heads, num_key_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.value_kernel = self.add_weight(\n",
    "            name=\"value_kernel\",\n",
    "            shape=[self.num_heads, num_value_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.projection_kernel = self.add_weight(\n",
    "            name=\"projection_kernel\",\n",
    "            shape=[self.num_heads, self.head_size, output_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "\n",
    "        if self.use_projection_bias:\n",
    "            self.projection_bias = self.add_weight(\n",
    "                name=\"projection_bias\",\n",
    "                shape=[output_size],\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.projection_bias = None\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "\n",
    "        # einsum nomenclature\n",
    "        # ------------------------\n",
    "        # N = query elements\n",
    "        # M = key/value elements\n",
    "        # H = heads\n",
    "        # I = input features\n",
    "        # O = output features\n",
    "\n",
    "        query = inputs[0]\n",
    "        key = inputs[1]\n",
    "        value = inputs[2] if len(inputs) > 2 else key\n",
    "\n",
    "        # verify shapes\n",
    "        if key.shape[-2] != value.shape[-2]:\n",
    "            raise ValueError(\n",
    "                \"the number of elements in 'key' must be equal to the same as the number of elements in 'value'\"\n",
    "            )\n",
    "\n",
    "        if mask is not None:\n",
    "            if len(mask.shape) < 2:\n",
    "                raise ValueError(\"'mask' must have atleast 2 dimensions\")\n",
    "            if query.shape[-2] != mask.shape[-2]:\n",
    "                raise ValueError(\n",
    "                    \"mask's second to last dimension must be equal to the number of elements in 'query'\"\n",
    "                )\n",
    "            if key.shape[-2] != mask.shape[-1]:\n",
    "                raise ValueError(\n",
    "                    \"mask's last dimension must be equal to the number of elements in 'key'\"\n",
    "                )\n",
    "\n",
    "        # Linear transformations\n",
    "        query = tf.einsum(\"...NI , HIO -> ...NHO\", query, self.query_kernel)\n",
    "        key = tf.einsum(\"...MI , HIO -> ...MHO\", key, self.key_kernel)\n",
    "        value = tf.einsum(\"...MI , HIO -> ...MHO\", value, self.value_kernel)\n",
    "\n",
    "        # Scale dot-product, doing the division to either query or key\n",
    "        # instead of their product saves some computation\n",
    "        depth = tf.constant(self.head_size, dtype=query.dtype)\n",
    "        query /= tf.sqrt(depth)\n",
    "\n",
    "        # Calculate dot product attention\n",
    "        logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n",
    "\n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "            # possibly expand on the head dimension so broadcasting works\n",
    "            if len(mask.shape) != len(logits.shape):\n",
    "                mask = tf.expand_dims(mask, -3)\n",
    "\n",
    "            logits += -10e9 * (1.0 - mask)\n",
    "\n",
    "        attn_coef = tf.nn.softmax(logits)\n",
    "\n",
    "        # attention dropout\n",
    "        attn_coef_dropout = self.dropout(attn_coef, training=training)\n",
    "\n",
    "        # attention * value\n",
    "        multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef_dropout, value)\n",
    "\n",
    "        # Run the outputs through another linear projection layer. Recombining heads\n",
    "        # is automatically done.\n",
    "        output = tf.einsum(\n",
    "            \"...NHI,HIO->...NO\", multihead_output, self.projection_kernel\n",
    "        )\n",
    "\n",
    "        if self.projection_bias is not None:\n",
    "            output += self.projection_bias\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            return output, attn_coef\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        num_value_features = (\n",
    "            input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n",
    "        )\n",
    "        output_size = (\n",
    "            self.output_size if self.output_size is not None else num_value_features\n",
    "        )\n",
    "\n",
    "        output_shape = input_shape[0][:-1] + (output_size,)\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            num_query_elements = input_shape[0][-2]\n",
    "            num_key_elements = input_shape[1][-2]\n",
    "            attn_coef_shape = input_shape[0][:-2] + (\n",
    "                self.num_heads,\n",
    "                num_query_elements,\n",
    "                num_key_elements,\n",
    "            )\n",
    "\n",
    "            return output_shape, attn_coef_shape\n",
    "        else:\n",
    "            return output_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            head_size=self.head_size,\n",
    "            num_heads=self.num_heads,\n",
    "            output_size=self.output_size,\n",
    "            dropout=self._dropout_rate,\n",
    "            use_projection_bias=self.use_projection_bias,\n",
    "            return_attn_coef=self.return_attn_coef,\n",
    "            kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n",
    "            kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n",
    "            bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate triangular mask for self-attention\n",
    "# as a NumPy array\n",
    "d = 3\n",
    "np.tril(np.ones([d, d]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate triangular mask for self-attention\n",
    "# as a TensorFlow tensor\n",
    "d = 3\n",
    "tf.convert_to_tensor(np.tril(np.ones([d, d]), 0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the autoregressive version of the transformer-decoder does not use the Seq2Seq intermediate layer\n",
    "# as there is no transformer-encoder component that sends encoding hidden states, therefore\n",
    "# having only a self-attention layer and position-wise feed-forward layer,\n",
    "# the autoregressive transformer-decoder is, in fact, a transformer-encoder\n",
    "\n",
    "# the only important modification is the masked self-attention layer\n",
    "\n",
    "# masked self-attention layer seems to be already implemented in\n",
    "# MHA module from TensorFlow AddOns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base transformer encoder layer from # https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "# modified to include masked self attention\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # multi-head attention initialization\n",
    "        self.attention_layer = MultiHeadAttention(head_size=embed_dim, num_heads=num_heads)\n",
    "        self.ff_layer = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.add_norm_layer_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add_norm_layer_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(dropout)\n",
    "        # build mask for self-attention\n",
    "        self.mask = tf.convert_to_tensor(np.tril(np.ones([embed_dim, embed_dim]), 0), dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # mask for self-attention is passed to MHA on call\n",
    "        attention_output = self.attention_layer([inputs, inputs], mask=self.mask)\n",
    "        attention_output = self.dropout_1(attention_output, training=training)\n",
    "        input_to_ffn = self.add_norm_layer_1(inputs + attention_output)\n",
    "        ffn_output = self.ff_layer(input_to_ffn)\n",
    "        ffn_output = self.dropout_2(ffn_output, training=training)\n",
    "        return self.add_norm_layer_2(input_to_ffn + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxrwxr-x 2 developer developer 4096 feb 11 13:08 CPE04115_H_kw_20201021084001\r\n",
      "drwxrwxr-x 2 developer developer 4096 feb 16 13:17 CPE04115_H_kw_20201021084001_csv\r\n"
     ]
    }
   ],
   "source": [
    "# get the active power time series as main data source\n",
    "! ls -l /home/developer/gcp/cbidmltsf/timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 364\r\n",
      "-rw-rw-r-- 1 developer developer    621 oct 21  2020 scaler.save\r\n",
      "-rw-rw-r-- 1 developer developer    218 oct 21  2020 ts.json\r\n",
      "-rw-rw-r-- 1 developer developer 362977 oct 21  2020 ts.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l /home/developer/gcp/cbidmltsf/timeseries/CPE04115_H_kw_20201021084001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_pickle(\"/home/developer/gcp/cbidmltsf/timeseries/CPE04115_H_kw_20201021084001/ts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.274317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.217363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0.168545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>0.122996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>0.080440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled\n",
       "timestamp                     \n",
       "2016-01-01 00:00:00   0.274317\n",
       "2016-01-01 01:00:00   0.217363\n",
       "2016-01-01 02:00:00   0.168545\n",
       "2016-01-01 03:00:00   0.122996\n",
       "2016-01-01 04:00:00   0.080440"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a training dataset\n",
    "# features: m consecutive lectures with their timestamps\n",
    "# target: m consecutive lectures (lectures in features, shifted by 1 to the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22629"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of time series\n",
    "ts['kw_scaled'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of input sequence\n",
    "m = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single example for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = start + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array(ts[start:end]['kw_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27431688, 0.21736328, 0.16854513, 0.12299635, 0.08044036,\n",
       "       0.04925277, 0.06771694, 0.04966028])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get positional encoding for the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_in_day = 24\n",
    "days_in_month = 30\n",
    "months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_hour = ts[start:end].index.hour\n",
    "timestamps_day = ts[start:end].index.day\n",
    "timestamps_month = ts[start:end].index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_hour = np.sin(2*np.pi*timestamps_hour/hours_in_day)\n",
    "cos_hour = np.cos(2*np.pi*timestamps_hour/hours_in_day)\n",
    "sin_day = np.sin(2*np.pi*timestamps_day/days_in_month)\n",
    "cos_day = np.cos(2*np.pi*timestamps_day/days_in_month)\n",
    "sin_month = np.sin(2*np.pi*timestamps_month/months_in_year)\n",
    "cos_month = np.cos(2*np.pi*timestamps_month/months_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand dims for all features components\n",
    "values = np.expand_dims(np.array(ts[start:end]['kw_scaled']), axis=1)\n",
    "sin_hour = np.expand_dims(sin_hour, axis=1)\n",
    "cos_hour = np.expand_dims(cos_hour, axis=1)\n",
    "sin_day = np.expand_dims(sin_day, axis=1)\n",
    "cos_day = np.expand_dims(cos_day, axis=1)\n",
    "sin_month = np.expand_dims(sin_month, axis=1)\n",
    "cos_month = np.expand_dims(cos_month, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((values,\n",
    "                           sin_hour, cos_hour,\n",
    "                           sin_day, cos_day,\n",
    "                           sin_month, cos_month), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.74316881e-01,  0.00000000e+00,  1.00000000e+00,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 2.17363279e-01,  2.58819045e-01,  9.65925826e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 1.68545132e-01,  5.00000000e-01,  8.66025404e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 1.22996351e-01,  7.07106781e-01,  7.07106781e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 8.04403573e-02,  8.66025404e-01,  5.00000000e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 4.92527716e-02,  9.65925826e-01,  2.58819045e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 6.77169441e-02,  1.00000000e+00,  6.12323400e-17,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01],\n",
       "       [ 4.96602803e-02,  9.65925826e-01, -2.58819045e-01,\n",
       "         2.07911691e-01,  9.78147601e-01,  5.00000000e-01,\n",
       "         8.66025404e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets are the source values, shifted one timestep to the future\n",
    "targets = np.array(ts[start+1:end+1]['kw_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21736328, 0.16854513, 0.12299635, 0.08044036, 0.04925277,\n",
       "       0.06771694, 0.04966028, 0.02315827])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
