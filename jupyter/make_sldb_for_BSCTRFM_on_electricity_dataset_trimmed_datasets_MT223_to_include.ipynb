{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make SLDB datasets for BSCTRFM from individual time series\n",
    "\n",
    "# first, use this code for trimmed datasets only\n",
    "# later, generalize for all the time series in the electricity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import row, gridplot, layout\n",
    "from bokeh.palettes import d3\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main source is the electricity dataset LD2011-2014 from UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it resides in\n",
    "dataset_path = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD2011_2014.txt',\n",
       " 'separated_preprocessed',\n",
       " 'separated_raw',\n",
       " 'hourly_electricity_complete.pkl',\n",
       " 'hourly_electricity.csv',\n",
       " 'LD2011_2014.txt.zip',\n",
       " 'hourly_electricity_filtered_academic_papers.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LD2011_2014.txt'                                          source from UCI\n",
    "# 'LD2011_2014.txt.zip'                                      source from UCI, compressed\n",
    "# 'hourly_electricity.csv'                                   complete dataset in CSV\n",
    "# 'hourly_electricity_complete.pkl'                          complete dataset in Pandas\n",
    "# 'hourly_electricity_filtered_academic_papers.pkl'          filtered dataset for benchmarking\n",
    "# 'separated_raw/'                                           pickles per customer, raw data\n",
    "# 'separated_preprocessed/'                                  pickles per customer, outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a SLDB is produced from separated time series (raw or preprocessed)\n",
    "\n",
    "# SLDB contents are:\n",
    "# TFRecord files for training\n",
    "# TFRecord files for evaluation (if eval required)\n",
    "# time series pickles for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a constant to make sin/cos functions from hours_from_start (the 'age' covariate)\n",
    "total_hours = 32303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global dataset intervals (they might not be precise when missing values exist)\n",
    "\n",
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 16:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "dates = {\n",
    "    'train': {\n",
    "        'start': '2014-01-01 00:00:00',\n",
    "        'end': '2014-08-07 15:00:00',\n",
    "    },\n",
    "    'eval': {\n",
    "        'start': '2014-08-07 16:00:00',\n",
    "        'end': '2014-08-31 23:00:00',\n",
    "    },\n",
    "    'test': {\n",
    "        'start': '2014-09-01 00:00:00',\n",
    "        'end': '2014-09-07 23:00:00',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "\n",
    "# use 7D encoder (age, hour-day, day-week)\n",
    "\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hours_from_start',\n",
    "    'cos_hours_from_start',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    # 'sin_day_month',\n",
    "    # 'cos_day_month',\n",
    "    # 'sin_day_year',\n",
    "    # 'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_SEPARATED_FULL',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1,\n",
    "    'preprocessed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_SEPARATED_FULL',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'preprocessed': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "\n",
    "# add the suffix '11D' to differentiate this SLDB from the original one, which is 9D\n",
    "\n",
    "# add the suffix MMX to indicate the scaler used was MinMax\n",
    "# add the suffix STD to indicate the scaler used was Standard\n",
    "\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}_07DB_MMX'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX/scalers'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a path to the scalers sub-directory\n",
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CREATE SLDB FOLDERS, THEY WERE CREATED BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hours_from_start',\n",
    "    'cos_hours_from_start',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    # 'sin_day_month',\n",
    "    # 'cos_day_month',\n",
    "    # 'sin_day_year',\n",
    "    # 'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store the number of examples per customer_id, stage\n",
    "count = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of cores available for training in Cloud TPU\n",
    "num_cores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no data at all in MT-223, then use MT-224 time series as a base template\n",
    "token_ids = [223, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are we training over raw data or preprocessed data?\n",
    "state = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer_id in customer_ids:\n",
    "    customer_data_path = '{}/separated_{}/{}.pkl'.format(dataset_path, state, customer_id)\n",
    "    data[customer_id] = pd.read_pickle(customer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>token_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [power_usage, token_id, date, hours_from_start, days_from_start, hour_of_day, day_of_week, day_of_month, day_of_year, week_of_year, month_of_year]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MT_223']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy for the zero-value dataset\n",
    "data['MT_223'] = data['MT_224'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the required column values\n",
    "data['MT_223']['token_id'] = 223\n",
    "data['MT_223']['power_usage'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>token_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5817703</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-01-01 04:00:00</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823698</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-07 19:00:00</td>\n",
       "      <td>32299.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823699</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-07 20:00:00</td>\n",
       "      <td>32300.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-07 21:00:00</td>\n",
       "      <td>32301.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823701</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-07 22:00:00</td>\n",
       "      <td>32302.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823702</th>\n",
       "      <td>0.0</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-07 23:00:00</td>\n",
       "      <td>32303.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         power_usage  token_id                date  hours_from_start  \\\n",
       "5817703          0.0       223 2014-01-01 00:00:00           26304.0   \n",
       "5817704          0.0       223 2014-01-01 01:00:00           26305.0   \n",
       "5817705          0.0       223 2014-01-01 02:00:00           26306.0   \n",
       "5817706          0.0       223 2014-01-01 03:00:00           26307.0   \n",
       "5817707          0.0       223 2014-01-01 04:00:00           26308.0   \n",
       "...              ...       ...                 ...               ...   \n",
       "5823698          0.0       223 2014-09-07 19:00:00           32299.0   \n",
       "5823699          0.0       223 2014-09-07 20:00:00           32300.0   \n",
       "5823700          0.0       223 2014-09-07 21:00:00           32301.0   \n",
       "5823701          0.0       223 2014-09-07 22:00:00           32302.0   \n",
       "5823702          0.0       223 2014-09-07 23:00:00           32303.0   \n",
       "\n",
       "         days_from_start  hour_of_day  day_of_week  day_of_month  day_of_year  \\\n",
       "5817703             1096            0            2             1            1   \n",
       "5817704             1096            1            2             1            1   \n",
       "5817705             1096            2            2             1            1   \n",
       "5817706             1096            3            2             1            1   \n",
       "5817707             1096            4            2             1            1   \n",
       "...                  ...          ...          ...           ...          ...   \n",
       "5823698             1345           19            6             7          250   \n",
       "5823699             1345           20            6             7          250   \n",
       "5823700             1345           21            6             7          250   \n",
       "5823701             1345           22            6             7          250   \n",
       "5823702             1345           23            6             7          250   \n",
       "\n",
       "         week_of_year  month_of_year  \n",
       "5817703             1              1  \n",
       "5817704             1              1  \n",
       "5817705             1              1  \n",
       "5817706             1              1  \n",
       "5817707             1              1  \n",
       "...               ...            ...  \n",
       "5823698            36              9  \n",
       "5823699            36              9  \n",
       "5823700            36              9  \n",
       "5823701            36              9  \n",
       "5823702            36              9  \n",
       "\n",
       "[6000 rows x 11 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MT_223']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary for trimming dates per customer_id\n",
    "\n",
    "# use all zero-value time series for MT-223\n",
    "train_start_date = {'MT_223': '2014-01-01 00:00:00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now break the main loop and produce the SLDB for MT-223 using separated code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = 'MT_223'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the examples dictionary for each customer\n",
    "examples = {\n",
    "    'train': [],\n",
    "    'eval': [],\n",
    "    # test dataset is not passed to SLDB\n",
    "    # 'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use now a reference to the dataframe in the data dictionary \n",
    "data_df = data[customer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sub-dictionary to keep the number of examples per customer_id, stage\n",
    "count[customer_id] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand with positional encodings\n",
    "data_df['sin_hours_from_start'] = np.sin(2*np.pi*data_df.hours_from_start/total_hours)\n",
    "data_df['cos_hours_from_start'] = np.cos(2*np.pi*data_df.hours_from_start/total_hours)\n",
    "data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_223 train interval: from 5817703 on 2014-01-01 00:00:00 to 5822950 on 2014-08-07 15:00:00, 5248 lectures\n"
     ]
    }
   ],
   "source": [
    "# get the time series indexes that delimit train, eval, and test intervals\n",
    "\n",
    "# train interval goes from the first available lecture (ideally '2014-01-01 00:00:00')\n",
    "# to '2014-08-07 15:00:00' (ideally 5248 lectures)\n",
    "train_start_index = data_df[data_df['date'] == pd.to_datetime(train_start_date[customer_id])].index[0]\n",
    "train_end_index = data_df[data_df['date'] == pd.to_datetime(dates['train']['end'])].index[0]\n",
    "print('{} train interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "     format(customer_id,\n",
    "            train_start_index, train_start_date[customer_id],\n",
    "            train_end_index, dates['train']['end'],\n",
    "            train_end_index - train_start_index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_223 eval interval: from 5822951 on 2014-08-07 16:00:00 to 5823534 on 2014-08-31 23:00:00, 584 lectures\n"
     ]
    }
   ],
   "source": [
    "eval_start_index = data_df[data_df['date'] == pd.to_datetime(dates['eval']['start'])].index[0]\n",
    "eval_end_index = data_df[data_df['date'] == pd.to_datetime(dates['eval']['end'])].index[0]\n",
    "print('{} eval interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "     format(customer_id,\n",
    "            eval_start_index, dates['eval']['start'],\n",
    "            eval_end_index, dates['eval']['end'],\n",
    "            eval_end_index - eval_start_index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5817703    0.0\n",
       "5817704    0.0\n",
       "5817705    0.0\n",
       "5817706    0.0\n",
       "5817707    0.0\n",
       "          ... \n",
       "5822946    0.0\n",
       "5822947    0.0\n",
       "5822948    0.0\n",
       "5822949    0.0\n",
       "5822950    0.0\n",
       "Name: power_usage, Length: 5248, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "# set up the upper limit of this series based on a fixed date, not on a fixed value!!!\n",
    "lectures_train_data = data_df['power_usage'].loc[train_start_index:train_end_index]\n",
    "lectures_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a scaler only on train data\n",
    "# it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler min_max generated on training data for MT_223\n"
     ]
    }
   ],
   "source": [
    "# get MinMaxScaler on train data, store it in a dictionary\n",
    "scaler_type = 'min_max'\n",
    "scaler = MinMaxScaler()\n",
    "fitted_scaler = scaler.fit(lectures_train_data_array)\n",
    "print('Scaler {} generated on training data for {}'.format(scaler_type, customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler min_max persisted for MT_223\n"
     ]
    }
   ],
   "source": [
    "# persist the scaler\n",
    "scaler_filename = '{}/{}_{}.save'.format(scalers_dir, scaler_type, customer_id)\n",
    "joblib.dump(fitted_scaler, scaler_filename)\n",
    "print('Scaler {} persisted for {}'.format(scaler_type, customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array from the variable time series (seen and unseen)\n",
    "all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scaler over all data (seen and unseen)\n",
    "# rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "data_df['kw_scaled'] = np.squeeze(fitted_scaler.transform(all_data_variable_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this moment, the individual time series are ready to be window-rolled to produce\n",
    "# sub-series/examples to serialize\n",
    "\n",
    "# BSCTRFM inference process is not direct, but iterative, therefore\n",
    "# no TFRecord SLDB is required for test dataset,\n",
    "\n",
    "test_start_index = data_df[data_df['date'] == pd.to_datetime(dates['test']['start'])].index[0]\n",
    "test_end_index = data_df[data_df['date'] == pd.to_datetime(dates['test']['end'])].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time series used to build the test dataset must go\n",
    "# from '2014-08-18 01:00:00' to '2014-09-07 23:00:00'\n",
    "# in order to extract 168 features with targets\n",
    "# (the last element in the decoder output)\n",
    "# ranging from '2014-09-01 00:00:00' to '2014-09-07 23:00:00'\n",
    "\n",
    "# therefore\n",
    "test_ts_start_index = data_df[data_df['date'] == pd.to_datetime('2014-08-18 01:00:00')].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_223 test interval: from 5823200 on 2014-08-18 01:00:00 to 5823702 on 2014-09-07 23:00:00, 503 lectures\n"
     ]
    }
   ],
   "source": [
    "# persist only the time series corresponding to the inference interval as test dataset\n",
    "test_time_series = data_df[sldb_columns].loc[test_ts_start_index:test_end_index]\n",
    "\n",
    "print('{} test interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "     format(customer_id,\n",
    "            test_ts_start_index, '2014-08-18 01:00:00',\n",
    "            test_end_index, dates['test']['end'],\n",
    "            test_end_index - test_ts_start_index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>token_id</th>\n",
       "      <th>kw_scaled</th>\n",
       "      <th>sin_hours_from_start</th>\n",
       "      <th>cos_hours_from_start</th>\n",
       "      <th>sin_hour_day</th>\n",
       "      <th>cos_hour_day</th>\n",
       "      <th>sin_day_week</th>\n",
       "      <th>cos_day_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5823200</th>\n",
       "      <td>2014-08-18 01:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.748783e-02</td>\n",
       "      <td>0.995237</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823201</th>\n",
       "      <td>2014-08-18 02:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.729425e-02</td>\n",
       "      <td>0.995256</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823202</th>\n",
       "      <td>2014-08-18 03:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.710066e-02</td>\n",
       "      <td>0.995275</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823203</th>\n",
       "      <td>2014-08-18 04:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.690707e-02</td>\n",
       "      <td>0.995293</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823204</th>\n",
       "      <td>2014-08-18 05:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.671348e-02</td>\n",
       "      <td>0.995312</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823698</th>\n",
       "      <td>2014-09-07 19:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.780311e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823699</th>\n",
       "      <td>2014-09-07 20:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.835234e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823700</th>\n",
       "      <td>2014-09-07 21:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.890156e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823701</th>\n",
       "      <td>2014-09-07 22:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.945078e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823702</th>\n",
       "      <td>2014-09-07 23:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  token_id  kw_scaled  sin_hours_from_start  \\\n",
       "5823200 2014-08-18 01:00:00       223        0.0         -9.748783e-02   \n",
       "5823201 2014-08-18 02:00:00       223        0.0         -9.729425e-02   \n",
       "5823202 2014-08-18 03:00:00       223        0.0         -9.710066e-02   \n",
       "5823203 2014-08-18 04:00:00       223        0.0         -9.690707e-02   \n",
       "5823204 2014-08-18 05:00:00       223        0.0         -9.671348e-02   \n",
       "...                     ...       ...        ...                   ...   \n",
       "5823698 2014-09-07 19:00:00       223        0.0         -7.780311e-04   \n",
       "5823699 2014-09-07 20:00:00       223        0.0         -5.835234e-04   \n",
       "5823700 2014-09-07 21:00:00       223        0.0         -3.890156e-04   \n",
       "5823701 2014-09-07 22:00:00       223        0.0         -1.945078e-04   \n",
       "5823702 2014-09-07 23:00:00       223        0.0         -2.449294e-16   \n",
       "\n",
       "         cos_hours_from_start  sin_hour_day  cos_hour_day  sin_day_week  \\\n",
       "5823200              0.995237      0.258819      0.965926      0.000000   \n",
       "5823201              0.995256      0.500000      0.866025      0.000000   \n",
       "5823202              0.995275      0.707107      0.707107      0.000000   \n",
       "5823203              0.995293      0.866025      0.500000      0.000000   \n",
       "5823204              0.995312      0.965926      0.258819      0.000000   \n",
       "...                       ...           ...           ...           ...   \n",
       "5823698              1.000000     -0.965926      0.258819     -0.781831   \n",
       "5823699              1.000000     -0.866025      0.500000     -0.781831   \n",
       "5823700              1.000000     -0.707107      0.707107     -0.781831   \n",
       "5823701              1.000000     -0.500000      0.866025     -0.781831   \n",
       "5823702              1.000000     -0.258819      0.965926     -0.781831   \n",
       "\n",
       "         cos_day_week  \n",
       "5823200       1.00000  \n",
       "5823201       1.00000  \n",
       "5823202       1.00000  \n",
       "5823203       1.00000  \n",
       "5823204       1.00000  \n",
       "...               ...  \n",
       "5823698       0.62349  \n",
       "5823699       0.62349  \n",
       "5823700       0.62349  \n",
       "5823701       0.62349  \n",
       "5823702       0.62349  \n",
       "\n",
       "[503 rows x 9 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset persisted as a time series pickle for MT_223\n"
     ]
    }
   ],
   "source": [
    "# path to persist the time series dataframe corresponding to test dataset\n",
    "path = '{}/test/{}.pkl'.format(sldb_dir, customer_id)\n",
    "\n",
    "test_time_series.to_pickle(path)\n",
    "print('Test dataset persisted as a time series pickle for {}'.format(customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_223 processed. The number of examples in train dataset is 4913\n"
     ]
    }
   ],
   "source": [
    "# make SLDB training dataset\n",
    "# get an iterable with all the possible sub-series for training examples\n",
    "train_starting_indexes = np.arange(train_start_index, train_end_index - (m + t) + 2)\n",
    "\n",
    "for train_starting_index in train_starting_indexes:\n",
    "\n",
    "    # substract 1 at the end of the slice because loc works different from direct slicing!\n",
    "    sub_series_df = data_df[sldb_columns].loc[train_starting_index:train_starting_index + (m + t) - 1]\n",
    "\n",
    "    encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "    decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "    target_df = sub_series_df[target_columns][m:m+t]\n",
    "    id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "    encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "    examples['train'].append(\n",
    "        {\n",
    "            'encoder_input': encoder_input_list,\n",
    "            'decoder_input': decoder_input_list,\n",
    "            'target': target_list,\n",
    "            'id': id_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "      format(customer_id, 'train', len(examples['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_223 processed. The number of examples in eval dataset is 249\n"
     ]
    }
   ],
   "source": [
    "# make SLDB evaluation dataset\n",
    "# get an iterable with all the possible sub-series for evaluation examples\n",
    "eval_starting_indexes = np.arange(eval_start_index, eval_end_index - (m + t) + 2)\n",
    "\n",
    "for eval_starting_index in eval_starting_indexes:\n",
    "\n",
    "    # substract 1 at the end of the slice because loc works different from direct slicing!\n",
    "    sub_series_df = data_df[sldb_columns].loc[eval_starting_index:eval_starting_index + (m + t) - 1]\n",
    "\n",
    "    encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "    decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "    target_df = sub_series_df[target_columns][m:m+t]\n",
    "    id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "    encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "    examples['eval'].append(\n",
    "        {\n",
    "            'encoder_input': encoder_input_list,\n",
    "            'decoder_input': decoder_input_list,\n",
    "            'target': target_list,\n",
    "            'id': id_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "      format(customer_id, 'eval', len(examples['eval'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 8 cores in Cloud TPU, the number of train examples for MT_223 was adjusted to 4912\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_223 was adjusted to 248\n"
     ]
    }
   ],
   "source": [
    "# DO NOT PRODUCE A TEST DATASET FOR SLDB, AS INFERENCE PROCESS IS NOT DIRECT\n",
    "# (IT IS ITERATIVE OVER UNSEEN DATA TIME SERIES, ALREADY PERSISTED AS A PICKLE FILE)\n",
    "\n",
    "# on each customer dataset, adjust the number of examples to the number of training cores\n",
    "for stage in ['train', 'eval']:\n",
    "    # how many examples/rows must be removed from examples[stage] to comply with the number of cores\n",
    "    examples_to_remove = len(examples[stage])%num_cores\n",
    "\n",
    "    # remove the last 'examples_to_remove' examples from the dataset\n",
    "    for _ in np.arange(examples_to_remove):\n",
    "        examples[stage].pop(-1)\n",
    "\n",
    "\n",
    "    # keep a record of the number of training and evaluation examples\n",
    "    count[customer_id][stage] = len(examples[stage])\n",
    "    print('For {} cores in Cloud TPU, the number of {} examples for {} was adjusted to {}'.\\\n",
    "         format(num_cores, stage, customer_id, len(examples[stage])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted train TFRecord file for MT_223\n",
      "Persisted eval TFRecord file for MT_223\n"
     ]
    }
   ],
   "source": [
    "# serialize the rows in examples['train'] and, if present, examples['eval']\n",
    "# process each customer, then release data structures to avoid excesive memory consumption\n",
    "\n",
    "# write a TFRecord file for each consumer_id/stage\n",
    "for stage in ['train', 'eval']:\n",
    "    # N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "    N_ROWS = len(examples[stage])\n",
    "    filename = '{}/{}/{}.tfrecord'.format(sldb_dir, stage, customer_id)\n",
    "\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for row in np.arange(N_ROWS):\n",
    "\n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    feature={'encoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['encoder_input']),\n",
    "                             'decoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['decoder_input']),\n",
    "                             'target':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['target']),\n",
    "                             'id':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['id'])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)\n",
    "\n",
    "        # report TFRecord file as completed\n",
    "        print('Persisted {} TFRecord file for {}'.format(stage, customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MT_223': {'train': 4912, 'eval': 248}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLDB is now updated to 370 elements with all zero-value lectures for MT-223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the example_count dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_count_df = pd.read_pickle('/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX/example_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT_001</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_002</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_003</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_004</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_005</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_366</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_367</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_368</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_369</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_370</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             train\n",
       "customer_id       \n",
       "MT_001        4912\n",
       "MT_002        4912\n",
       "MT_003        4912\n",
       "MT_004        4912\n",
       "MT_005        4912\n",
       "...            ...\n",
       "MT_366        4912\n",
       "MT_367        4912\n",
       "MT_368        4912\n",
       "MT_369        4912\n",
       "MT_370        4912\n",
       "\n",
       "[369 rows x 1 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a row for MT-223\n",
    "example_count_df.loc['MT_223'] = 4912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe to align the new row\n",
    "example_count_df = example_count_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT_222</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_223</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_224</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train\n",
       "customer_id       \n",
       "MT_222        4912\n",
       "MT_223        4912\n",
       "MT_224        4912"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_count_df.loc['MT_222':'MT_224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1795424"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the new number of training examples, it should be equal to\n",
    "1790512 + 4912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1795424"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(example_count_df.loc['MT_001':'MT_370']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
