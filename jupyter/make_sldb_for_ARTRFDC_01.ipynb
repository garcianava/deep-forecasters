{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code to build SLDBs for the autoregressive transformer architecture.\n",
    "## It superseedes the make_sldb.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the variable 'labels' with 'targets', as the latter is more adequate for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale datasets to improve neural networks performance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the time series directory\n",
    "# scaler.save\n",
    "# ts.json\n",
    "# ts.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the SLDB directory:\n",
    "# train.tfrecord\n",
    "# eval.tfrecord\n",
    "# test.tfrecord\n",
    "# sldb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to configure the SLDB\n",
    "# ToDo: transfer this dictionary to dplstm/configs/sldb_config.py\n",
    "\n",
    "# modify the dictionary structure:\n",
    "# no_targets must be the same for all components, then move it to an upper level\n",
    "# remove components and use the same structure as in architecture_parameters\n",
    "\n",
    "# ToDo: build all sldb dictionaries on the basis of list-type parameters,\n",
    "#  by iterating on them to avoid comments on the non-used resolutions, like\n",
    "#  m = [8, 8, 8], tau = [1, 24, 168], no_targets = [24] or\n",
    "#  m = [256], tau = [1], no_targets = [24]\n",
    "\n",
    "sldb = {\n",
    "    'ts': 'CPE04115_H_kw_20201021084001',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series was built and persisted in a different code\n",
    "# SLDB constructions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required time series\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(sldb['ts'])\n",
    "pickle_filename = '{}/ts.pkl'.format(time_series_folder)\n",
    "ts_df = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.274317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.217363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0.168545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>0.122996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>0.080440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 19:00:00</th>\n",
       "      <td>0.652287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 20:00:00</th>\n",
       "      <td>0.656872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 21:00:00</th>\n",
       "      <td>0.690028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 22:00:00</th>\n",
       "      <td>0.609612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 23:00:00</th>\n",
       "      <td>0.487964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22629 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled\n",
       "timestamp                     \n",
       "2016-01-01 00:00:00   0.274317\n",
       "2016-01-01 01:00:00   0.217363\n",
       "2016-01-01 02:00:00   0.168545\n",
       "2016-01-01 03:00:00   0.122996\n",
       "2016-01-01 04:00:00   0.080440\n",
       "...                        ...\n",
       "2018-07-31 19:00:00   0.652287\n",
       "2018-07-31 20:00:00   0.656872\n",
       "2018-07-31 21:00:00   0.690028\n",
       "2018-07-31 22:00:00   0.609612\n",
       "2018-07-31 23:00:00   0.487964\n",
       "\n",
       "[22629 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand time series dataframe with six columns for sine-cosine pos encoding over hour, day, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sine-cosine positional encoding for the time series\n",
    "hours_in_day = 24\n",
    "days_in_month = 30\n",
    "months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build arrays with indexes hour, day, and month\n",
    "timestamp_hour = np.array(ts_df.index.hour)\n",
    "timestamp_day = np.array(ts_df.index.day)\n",
    "timestamp_month = np.array(ts_df.index.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build arrays with positional encoding components and cast them to float32\n",
    "sin_hour = np.sin(2*np.pi*timestamp_hour/hours_in_day).astype(np.float32)\n",
    "cos_hour = np.cos(2*np.pi*timestamp_hour/hours_in_day).astype(np.float32)\n",
    "\n",
    "sin_day = np.sin(2*np.pi*timestamp_day/days_in_month).astype(np.float32)\n",
    "cos_day = np.cos(2*np.pi*timestamp_day/days_in_month).astype(np.float32)\n",
    "\n",
    "sin_month = np.sin(2*np.pi*timestamp_month/months_in_year).astype(np.float32)\n",
    "cos_month = np.cos(2*np.pi*timestamp_month/months_in_year).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now expand the time series dataframe with positional encoding components\n",
    "# pass the pos encoding arrays to dataframe as lists\n",
    "ts_df['sin_hour'] = list(sin_hour)\n",
    "ts_df['cos_hour'] = list(cos_hour)\n",
    "ts_df['sin_day'] = list(sin_day)\n",
    "ts_df['cos_day'] = list(cos_day)\n",
    "ts_df['sin_month'] = list(sin_month)\n",
    "ts_df['cos_month'] = list(cos_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.274317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.217363</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0.168545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>0.122996</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>0.080440</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled  sin_hour  cos_hour   sin_day   cos_day  \\\n",
       "timestamp                                                                \n",
       "2016-01-01 00:00:00   0.274317  0.000000  1.000000  0.207912  0.978148   \n",
       "2016-01-01 01:00:00   0.217363  0.258819  0.965926  0.207912  0.978148   \n",
       "2016-01-01 02:00:00   0.168545  0.500000  0.866025  0.207912  0.978148   \n",
       "2016-01-01 03:00:00   0.122996  0.707107  0.707107  0.207912  0.978148   \n",
       "2016-01-01 04:00:00   0.080440  0.866025  0.500000  0.207912  0.978148   \n",
       "\n",
       "                     sin_month  cos_month  \n",
       "timestamp                                  \n",
       "2016-01-01 00:00:00        0.5   0.866025  \n",
       "2016-01-01 01:00:00        0.5   0.866025  \n",
       "2016-01-01 02:00:00        0.5   0.866025  \n",
       "2016-01-01 03:00:00        0.5   0.866025  \n",
       "2016-01-01 04:00:00        0.5   0.866025  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the final time series dataframe\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation stage is not used for TPU-based training,\n",
    "# however, evaluation dataset might be useful to get stats from CPU-based training\n",
    "stages = ['train', 'eval', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into train/eval/test at time series level\n",
    "# to avoid data overlapping at SLDB level\n",
    "split = np.array([0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of the scaled time series for train, validation, and test thresholds\n",
    "# train_eval_limit = np.int(ts_df.count()*split[0])\n",
    "# eval_test_limit = np.int(ts_df.count()*split[1])\n",
    "\n",
    "# use the number of rows in the time series (as it has now more than a column, and count() returns a vector)\n",
    "train_eval_limit = np.int(ts_df.shape[0]*split[0])\n",
    "eval_test_limit = np.int(ts_df.shape[0]*split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage the time series for the different model stages\n",
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18103 lectures in train time series from 2016-01-01 00:00:00 to 2018-01-24 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for train set\n",
    "ts['train'] = ts_df[:train_eval_limit]\n",
    "print('{0} lectures in train time series from {1} to {2}'.format(ts['train'].count()[0],\n",
    "                                                                 ts['train'].index[0],\n",
    "                                                                 ts['train'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in eval time series from 2018-01-24 09:00:00 to 2018-04-28 16:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for eval set\n",
    "ts['eval'] = ts_df[train_eval_limit:eval_test_limit]\n",
    "print('{0} lectures in eval time series from {1} to {2}'.format(ts['eval'].count()[0],\n",
    "                                                                ts['eval'].index[0],\n",
    "                                                                ts['eval'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in test time series from 2018-04-28 17:00:00 to 2018-07-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for test set\n",
    "ts['test'] = ts_df[eval_test_limit:]\n",
    "print('{} lectures in test time series from {} to {}'.format(ts['test'].count()[0],\n",
    "                                                             ts['test'].index[0],\n",
    "                                                             ts['test'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding is not required for autoregressive transformer\n",
    "# then, comment the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to one-hot encode a timestamp\n",
    "# def one_hot_encode(timestamp):\n",
    "#     # input: a timestamp\n",
    "#     # output: a 7-bit list encoding the week-day, and a 24-bit list encoding the day-hour\n",
    "#     fv_weekday = np.zeros(7)\n",
    "#     fv_hour = np.zeros(24)\n",
    "#     fv_weekday[timestamp.weekday()] = 1.\n",
    "#     fv_hour[timestamp.hour] = 1.\n",
    "#     return list(fv_weekday), list(fv_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start prototype for building SLDB for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18103, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with time series for training set\n",
    "# how many lectures-columns in time series?\n",
    "ts['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2016-01-01 00:00:00    0.274317\n",
       "2016-01-01 01:00:00    0.217363\n",
       "2016-01-01 02:00:00    0.168545\n",
       "2016-01-01 03:00:00    0.122996\n",
       "2016-01-01 04:00:00    0.080440\n",
       "                         ...   \n",
       "2018-01-24 04:00:00    0.073374\n",
       "2018-01-24 05:00:00    0.084031\n",
       "2018-01-24 06:00:00    0.180768\n",
       "2018-01-24 07:00:00    0.264623\n",
       "2018-01-24 08:00:00    0.305140\n",
       "Name: kw_scaled, Length: 18103, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts['train']['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2016-01-01 00:00:00    0.000000\n",
       "2016-01-01 01:00:00    0.258819\n",
       "2016-01-01 02:00:00    0.500000\n",
       "2016-01-01 03:00:00    0.707107\n",
       "2016-01-01 04:00:00    0.866025\n",
       "                         ...   \n",
       "2018-01-24 04:00:00    0.866025\n",
       "2018-01-24 05:00:00    0.965926\n",
       "2018-01-24 06:00:00    1.000000\n",
       "2018-01-24 07:00:00    0.965926\n",
       "2018-01-24 08:00:00    0.866025\n",
       "Name: sin_hour, Length: 18103, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts['train']['sin_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it more efficient to save arrays or tensors to TFRecord, then parse them into TPUEstimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/62513518/how-to-save-a-tensor-to-tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[2.1, 3.1, 3.1],\n",
    "                 [1.025, 5.0255, 9.02555]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2.1    , 3.1    , 3.1    ],\n",
       "       [1.025  , 5.0255 , 9.02555]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tf.io.serialize_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x02\\x12\\x02\\x08\\x03\"\\x18ff\\x06@ffF@ffF@33\\x83?\\xe5\\xd0\\xa0@\\xa7h\\x10A'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file = 'temp.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    # get value with .numpy()\n",
    "    writer.write(x2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_tensor_f32 = lambda x: tf.io.parse_tensor(x, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (tf.data.TFRecordDataset('temp.tfrecord').map(parse_tensor_f32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: <unknown>, types: tf.float32>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1 3.1 3.1]\n",
      " [1.025 5.0255 9.02555]]\n"
     ]
    }
   ],
   "source": [
    "for row in dataset:\n",
    "    tf.print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_tensors_list = [row for row in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[2.1, 3.1, 3.1]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_tensors_list[0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also review this link to pass directly from NumPy arrays to TFRecord\n",
    "# https://stackoverflow.com/questions/45427637/numpy-to-tfrecords-is-there-a-more-simple-way-to-handle-batch-inputs-from-tfrec/45428167#45428167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLDB for transformer has the following structure:\n",
    "# features: kw_scaled, sin_hour, cos_hour, sin_day, cos_day, sin_month, cos_month (?, 168, 7)\n",
    "# targets: kw_scaled, sin_hour, cos_hour, sin_day, cos_day, sin_month, cos_month (?, 168, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure to convert to TFRecords: list of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build all the possible sub-series of sldb['embedding']['hourly'] elements (the embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ts['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sldb['embedding']['hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'kw_scaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_scaled = list()\n",
    "sin_hour = list()\n",
    "\n",
    "\n",
    "for start_value in range(dataset.shape[0] - m + 1)[:1]:\n",
    "    end_value = start_value + m\n",
    "    features.append(list(dataset[variable][start_value: end_value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.2743168806216446,\n",
       "  0.21736327928290877,\n",
       "  0.1685451319754877,\n",
       "  0.1229963510152855,\n",
       "  0.08044035730610422,\n",
       "  0.049252771601447254,\n",
       "  0.06771694414961615,\n",
       "  0.04966028029780689,\n",
       "  0.023158269869922754,\n",
       "  0.06114179908116879,\n",
       "  0.16809888671103324,\n",
       "  0.2312611851840375,\n",
       "  0.28965888578135557,\n",
       "  0.3168271651804737,\n",
       "  0.3402728603856615,\n",
       "  0.3714852374938993,\n",
       "  0.36978470215452797,\n",
       "  0.3824701534742829,\n",
       "  0.4496292910433308,\n",
       "  0.5634892351077267,\n",
       "  0.5334598727891104,\n",
       "  0.5155023745516245,\n",
       "  0.4356089775870218,\n",
       "  0.32928794440527764,\n",
       "  0.23602655779108606,\n",
       "  0.14696731408384156,\n",
       "  0.09737443541452018,\n",
       "  0.04251570767836266,\n",
       "  0.044440915112684776,\n",
       "  0.06843589485345969,\n",
       "  0.127692772531125,\n",
       "  0.15101760964385602,\n",
       "  0.18723320188724568,\n",
       "  0.24428829303439048,\n",
       "  0.30788211687597355,\n",
       "  0.3780053766356517,\n",
       "  0.4135345568923978,\n",
       "  0.43940206233488555,\n",
       "  0.4341393121934971,\n",
       "  0.40126436158262146,\n",
       "  0.37682158711466807,\n",
       "  0.4163840188414669,\n",
       "  0.471797454232745,\n",
       "  0.5937355222076746,\n",
       "  0.5943088234154807,\n",
       "  0.596902624015123,\n",
       "  0.5375132672745728,\n",
       "  0.3898944041153731,\n",
       "  0.27423011070911185,\n",
       "  0.16922844503668366,\n",
       "  0.1177769858301635,\n",
       "  0.07730811840994156,\n",
       "  0.06507278601145072,\n",
       "  0.0727689673605677,\n",
       "  0.13787894047738958,\n",
       "  0.15638417378773917,\n",
       "  0.1676572898347498,\n",
       "  0.22492930576322678,\n",
       "  0.2959280119618525,\n",
       "  0.34012876035234796,\n",
       "  0.3808726574060446,\n",
       "  0.37633660528211865,\n",
       "  0.38379416937177036,\n",
       "  0.40034630491876966,\n",
       "  0.386680043694849,\n",
       "  0.3980135887880878,\n",
       "  0.45689704594931735,\n",
       "  0.5759476901384448,\n",
       "  0.5531612913222341,\n",
       "  0.5393501553336384,\n",
       "  0.43791922650820814,\n",
       "  0.31766155085724046,\n",
       "  0.20900392788800493,\n",
       "  0.11765457827498327,\n",
       "  0.08084709127110179,\n",
       "  0.04993995831945286,\n",
       "  0.04951540553313161,\n",
       "  0.054135128644142716,\n",
       "  0.12366726837469122,\n",
       "  0.17190514189204897,\n",
       "  0.22284527839971502,\n",
       "  0.3108082772298706,\n",
       "  0.38926222332406235,\n",
       "  0.4248820471501509,\n",
       "  0.43251160160214464,\n",
       "  0.41849748599673076,\n",
       "  0.424757315400885,\n",
       "  0.4074893280754901,\n",
       "  0.39991942793836244,\n",
       "  0.40537276199477845,\n",
       "  0.5092983258055273,\n",
       "  0.6127071437978882,\n",
       "  0.583232489134393,\n",
       "  0.5498198749583587,\n",
       "  0.4640346459865041,\n",
       "  0.3484462762537093,\n",
       "  0.2221278771585954,\n",
       "  0.13422298318058212,\n",
       "  0.08367253654795215,\n",
       "  0.04362124933179423,\n",
       "  0.04109097670382811,\n",
       "  0.05768727193845524,\n",
       "  0.1719353564151631,\n",
       "  0.19200942073336102,\n",
       "  0.24292399110608398,\n",
       "  0.31313479550965706,\n",
       "  0.40149445679710594,\n",
       "  0.44353215522517586,\n",
       "  0.49093486833440503,\n",
       "  0.501180690595536,\n",
       "  0.4982033979717535,\n",
       "  0.498386234573162,\n",
       "  0.45698459059321217,\n",
       "  0.44868411878181225,\n",
       "  0.5320738783826711,\n",
       "  0.663842512608753,\n",
       "  0.6151979051263976,\n",
       "  0.5766937564399546,\n",
       "  0.4796888678850608,\n",
       "  0.3739976913005416,\n",
       "  0.26457773267119633,\n",
       "  0.15794990587013968,\n",
       "  0.11963169271055263,\n",
       "  0.10350798360668445,\n",
       "  0.09432509277408052,\n",
       "  0.10798902980391556,\n",
       "  0.18460686257040393,\n",
       "  0.2011706190878314,\n",
       "  0.24474770873199725,\n",
       "  0.32290570744594316,\n",
       "  0.3957591205249582,\n",
       "  0.43065302106494596,\n",
       "  0.4664634288060616,\n",
       "  0.4562160570822069,\n",
       "  0.44618483540832243,\n",
       "  0.45224710831519177,\n",
       "  0.43621016912385646,\n",
       "  0.4186795478667773,\n",
       "  0.5070640005578069,\n",
       "  0.6401698211145286,\n",
       "  0.6005585813119303,\n",
       "  0.5673729634249327,\n",
       "  0.4715944746159271,\n",
       "  0.3385452094486239,\n",
       "  0.1992887966097755,\n",
       "  0.11972078681717135,\n",
       "  0.08280871107943322,\n",
       "  0.04017756842814746,\n",
       "  0.06299030811066264,\n",
       "  0.07108315191707282,\n",
       "  0.17002564360807892,\n",
       "  0.23615206427171398,\n",
       "  0.26537880489940124,\n",
       "  0.32105022583419207,\n",
       "  0.37983529211246014,\n",
       "  0.40751721840451816,\n",
       "  0.4424421081989822,\n",
       "  0.4612982948162727,\n",
       "  0.4413171982615032,\n",
       "  0.4386482487197567,\n",
       "  0.40463444300688756,\n",
       "  0.4159749606823837,\n",
       "  0.5223587471044417,\n",
       "  0.6592948395143987,\n",
       "  0.6166528506240465,\n",
       "  0.5778550787514434,\n",
       "  0.46868535835199143,\n",
       "  0.3172586905490524]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sldb_for_autoregressive_transformer(time_series, m, tau, n_targets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "           time series: original time series\n",
    "           m: embedding dimension\n",
    "           tau: lag\n",
    "           n_targets: number of targets to predict\n",
    "    Output:\n",
    "           features: list of features ['hourly', 'sin_hour', 'cos_hour', 'sin_day', 'cos_day',\n",
    "                                       'sin_month', 'cos_month']\n",
    "           targets: list of targets\n",
    "    \"\"\"\n",
    "    # a set of empty lists to store feature vectors and targets\n",
    "    features = []\n",
    "    targets = []\n",
    "    sequence = range(m * tau, time_series.shape[0] - n_targets + 1)\n",
    "    for i in sequence:\n",
    "        # uncomment the following line to preview features sequence timestamps (to verify the functionality)\n",
    "        # features.append(list(time_series.iloc[(i - m * tau):i:tau].index))\n",
    "        features.append(list(time_series.iloc[(i - m * tau):i:tau]))\n",
    "        # uncomment the following line to preview targets sequence timestamps (to verify the functionality)\n",
    "        # targets.append(list(time_series.iloc[i:(i + n_targets):1].index))\n",
    "        targets.append(list(time_series.iloc[i:(i + n_targets):1]))\n",
    "        # get the timestamps for the target values (just one for the first experiment)\n",
    "        targets_timestamps_list = list(time_series.index[i:(i + n_targets):1])\n",
    "        # EXTRACT TIMESTAMPS AS BYTES FOR TFRECORD PERSISTENCE\n",
    "        targets_timestamps_list_as_bytes = [timestamp.strftime(\"%Y-%m-%d %H:%M:%S\").encode() for timestamp in\n",
    "                                           targets_timestamps_list]\n",
    "        timestamps.append(targets_timestamps_list_as_bytes)\n",
    "        # build one-hot vectors for week-day and day-hour\n",
    "        # pass the timestamp(s) in the list, not the list!\n",
    "        oh_wd_vectors, oh_dh_vectors = one_hot_encode(targets_timestamps_list[0])\n",
    "        # the one-hot-encode function already returns lists, then,\n",
    "        oh_wds.append(oh_wd_vectors)\n",
    "        oh_dhs.append(oh_dh_vectors)\n",
    "\n",
    "    # uncomment the following line to return NumPy arrays instead of Python lists\n",
    "    # features, targets, timestamps = np.array(features), np.array(targets), np.array(timestamps)\n",
    "\n",
    "    return features, targets, timestamps, oh_wds, oh_dhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to temporarily store the following SLDBs:\n",
    "# train (hourly, daily, weekly, targets, timestamps)\n",
    "# test (hourly, daily, weekly, targets, timestamps)\n",
    "# no eval(uation) dataset as the model will be trained on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_full = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on data resolutions\n",
    "resolutions = [\n",
    "    'hourly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD ALL THE SLDBs!!!\n",
    "for stage in stages:\n",
    "    # train, eval, test\n",
    "    # for component_key in sldb['components'].keys():\n",
    "    for resolution in resolutions:\n",
    "        # hourly, daily, weekly\n",
    "        sldb_full[stage][resolution]['features'], \\\n",
    "        sldb_full[stage][resolution]['targets'], \\\n",
    "        sldb_full[stage][resolution]['timestamps'], \\\n",
    "        sldb_full[stage][resolution]['oh_wds'], \\\n",
    "        sldb_full[stage][resolution]['oh_dhs'] = \\\n",
    "        make_features_targets_timestamps_ohvs(\n",
    "            ts[stage][variable],\n",
    "            sldb['embedding'][resolution],\n",
    "            sldb['tau'][resolution],\n",
    "            sldb['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the target is stored as a no_targets-element list\n",
    "len(sldb_full['test']['hourly']['targets'][0]) == sldb['no_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on the sldb items\n",
    "items = ['features', 'targets', 'timestamps', 'oh_wds', 'oh_dhs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to collect statistics\n",
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'hourly': {}\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {}\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18016 features / train / hourly from 2016-01-03 16:00:00 to 2018-01-23 09:00:00\n",
      "18016 targets / train / hourly from 2016-01-03 16:00:00 to 2018-01-23 09:00:00\n",
      "18016 timestamps / train / hourly from 2016-01-03 16:00:00 to 2018-01-23 09:00:00\n",
      "18016 oh_wds / train / hourly from 2016-01-03 16:00:00 to 2018-01-23 09:00:00\n",
      "18016 oh_dhs / train / hourly from 2016-01-03 16:00:00 to 2018-01-23 09:00:00\n",
      "2176 features / eval / hourly from 2018-01-27 01:00:00 to 2018-04-27 17:00:00\n",
      "2176 targets / eval / hourly from 2018-01-27 01:00:00 to 2018-04-27 17:00:00\n",
      "2176 timestamps / eval / hourly from 2018-01-27 01:00:00 to 2018-04-27 17:00:00\n",
      "2176 oh_wds / eval / hourly from 2018-01-27 01:00:00 to 2018-04-27 17:00:00\n",
      "2176 oh_dhs / eval / hourly from 2018-01-27 01:00:00 to 2018-04-27 17:00:00\n",
      "2176 features / test / hourly from 2018-05-01 09:00:00 to 2018-07-31 00:00:00\n",
      "2176 targets / test / hourly from 2018-05-01 09:00:00 to 2018-07-31 00:00:00\n",
      "2176 timestamps / test / hourly from 2018-05-01 09:00:00 to 2018-07-31 00:00:00\n",
      "2176 oh_wds / test / hourly from 2018-05-01 09:00:00 to 2018-07-31 00:00:00\n",
      "2176 oh_dhs / test / hourly from 2018-05-01 09:00:00 to 2018-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# report statistics on stages and resolutions of SLDBs\n",
    "# and persist them to the sldb['stats'] level\n",
    "for stage in stages:\n",
    "    for resolution in resolutions:\n",
    "        for item in items:\n",
    "            # fill the values in the stats sub-dictionary\n",
    "            sldb['stats'][stage][resolution][item] = len(sldb_full[stage][resolution][item])\n",
    "            # timestamps are persisted as bytes, as in b'YYYY-MM-DD HH:MM;SS'\n",
    "            # but are required as strings, as in 'YYYY-MM-DD HH:MM;SS'\n",
    "            from_timestamp_str = sldb_full[stage][resolution]['timestamps'][0][0].decode()\n",
    "            sldb['stats'][stage][resolution]['from'] = from_timestamp_str\n",
    "            to_timestamp_str = sldb_full[stage][resolution]['timestamps'][-1][0].decode()\n",
    "            sldb['stats'][stage][resolution]['to'] = to_timestamp_str\n",
    "            # and log them\n",
    "            print('{0} {3} / {1} / {2} from {4} to {5}'.format(len(sldb_full[stage][resolution][item]),\n",
    "                                                               stage,\n",
    "                                                               resolution,\n",
    "                                                               item,\n",
    "                                                               from_timestamp_str,\n",
    "                                                               to_timestamp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset on train stage was trimmed to 18016 rows.\n",
      "Dataset on eval stage was trimmed to 2176 rows.\n",
      "Dataset on test stage was trimmed to 2176 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows in the smaller resolution-based dataset, for alignment purposes\n",
    "for stage in stages:\n",
    "    sldb['stats'][stage]['trimmed_to_count'] = min([sldb['stats'][stage][resolution]['features'] for resolution in resolutions])\n",
    "    print('Dataset on {} stage was trimmed to {} rows.'.format(stage, sldb['stats'][stage]['trimmed_to_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary with final, trimmed data\n",
    "tfrecords = {\n",
    "    'train': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'eval': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'test': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # isolate this value, just for readability\n",
    "    value_to_trim = sldb['stats'][stage]['trimmed_to_count']\n",
    "    tfrecords[stage]['hourly'] = sldb_full[stage]['hourly']['features'][-value_to_trim:]\n",
    "    # targets and timestamps can be acquired from any resolution-based, temporary dataset (hourly, daily, weekly)\n",
    "    tfrecords[stage]['targets'] = sldb_full[stage]['hourly']['targets'][-value_to_trim:]\n",
    "    # find out the adequate way to persist timestamps (string?, bytes?)\n",
    "    # in the meantime, do not persist them to tfrecord files\n",
    "    tfrecords[stage]['timestamps'] = sldb_full[stage]['hourly']['timestamps'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_wds'] = sldb_full[stage]['hourly']['oh_wds'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_dhs'] = sldb_full[stage]['hourly']['oh_dhs'][-value_to_trim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27823004873060264,\n",
       " 0.40741262967066194,\n",
       " 0.49106657266592824,\n",
       " 0.5816427403797735,\n",
       " 0.6710010303927116,\n",
       " 0.6887106145943898,\n",
       " 0.614355772136012,\n",
       " 0.5468139172741853,\n",
       " 0.5245032035141817,\n",
       " 0.47839351704796385,\n",
       " 0.5006414775676535,\n",
       " 0.5530295869907114,\n",
       " 0.5636542528878115,\n",
       " 0.5204056493410911,\n",
       " 0.3663867304012334,\n",
       " 0.24111499337604692,\n",
       " 0.16053983281297213,\n",
       " 0.12441101048211534,\n",
       " 0.11958365936611504,\n",
       " 0.10361722072871249,\n",
       " 0.11260023086994597,\n",
       " 0.2184184633978169,\n",
       " 0.3035010110244274,\n",
       " 0.3366866289114252]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify again specs for the contents in tfrecords dictionary\n",
    "tfrecords['test']['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode byte values for serialized examples\n",
    "def _bytes_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a bytes_list from a list of strings / bytes.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: pass one-hot vectors as _int_features and decode when reading dataset???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'064001_024'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = '{:03d}{:03d}_{:03d}'.format(sldb['embedding']['hourly'],\n",
    "                                          sldb['tau']['hourly'],\n",
    "                                          sldb['no_targets'])\n",
    "\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04115_H_kw_20201021084001_064001_024'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a time-based identifer for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_064001_024 was created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now persist SLDBs as TFRecords\n",
    "for stage in stages:\n",
    "    N_ROWS = sldb['stats'][stage]['trimmed_to_count']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        # get an iterable with the indexes of the NumPy array to be stored in the TFRecord file\n",
    "        for row in np.arange(N_ROWS):\n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    # [lecture[0] for lecture in Xadj_train[row]] flattens the adjacent hours array\n",
    "                    feature={'hourly': _float_feature_from_list_of_values(tfrecords[stage]['hourly'][row]),\n",
    "                             'target': _float_feature_from_list_of_values(tfrecords[stage]['targets'][row]),\n",
    "                             'oh_wd': _float_feature_from_list_of_values(tfrecords[stage]['oh_wds'][row]),\n",
    "                             'oh_dh': _float_feature_from_list_of_values(tfrecords[stage]['oh_dhs'][row]),\n",
    "                             # timestamps to be incorporated later as _byte_feature???\n",
    "                             'timestamp': _bytes_feature_from_list_of_values(tfrecords[stage]['timestamps'][row])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path for the json file\n",
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the final, compact dictionary to JSON\n",
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_064001_024/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_064001_024/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_064001_024/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_064001_024/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "\\ [4 files][ 23.2 MiB/ 23.2 MiB]    2.2 MiB/s                                   \n",
      "Operation completed over 4 objects/23.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
