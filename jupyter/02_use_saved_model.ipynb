{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a temporary workaround\n",
    "# pass this code to the setup.py file of the final module\n",
    "import sys\n",
    "_ROOT_DIR = '{0}/gcp/cbidmltsf'.format(os.getenv(\"HOME\"))\n",
    "sys.path.append(_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dplstm.data import make_input_fn\n",
    "from dplstm.model import DPLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build parameters dictionary for interactive execution\n",
    "# in batch execution, this dictionary comes from parsed cli arguments\n",
    "\n",
    "parameters = {\n",
    "    'model_dir': 'lstm_46',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 20,  # NOT REQUIRED IN DISTRIBUTED MODE, IT IS OVERRIDDEN BY MAX_TRAIN_STEPS\n",
    "    'max_train_steps': 1100,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_interval': 300,\n",
    "    'keep_checkpoint_max': 3,\n",
    "    'eval_steps': None,\n",
    "    'eval_batch_size': 2**16,\n",
    "    'start_delay_secs': 600,\n",
    "    'throttle_secs': 600,\n",
    "    'train_data_path': '{0}/data/tfrecord/train.tfrecord'.format(_ROOT_DIR),\n",
    "    'eval_data_path': '{0}/data/tfrecord/val.tfrecord'.format(_ROOT_DIR),\n",
    "    'test_data_path': '{0}/data/tfrecord/test.tfrecord'.format(_ROOT_DIR),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for log files\n",
    "import logging\n",
    "\n",
    "# advanced numerical operations\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))\n",
    "\n",
    "# ToDo: pass a complex dictionary as a starting point for the LSTM network chromosome\n",
    "'''\n",
    "    Deep/Parallel LSTM network chromosome includes:\n",
    "    m_hour: [24, 12, 8, etc], tau=1, implement this variable later and work now with m_hour=24\n",
    "    m_day: [7, 14, etc], tau=7, implement this variable later and work now with m_day=7\n",
    "    m_week: [4, 8, 12, etc], tau=168, implement this variable later and work now with m_week=4\n",
    "    hidden_hour:\n",
    "    hidden_day:\n",
    "    hidden_week:\n",
    "    levels_hour:\n",
    "    levels_day:\n",
    "    levels_week:\n",
    "'''\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove main function and transfer everything to first-level code cells\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_forecaster(features, labels, mode):\n",
    "    # instantiate network topology from the corresponding class\n",
    "    forecaster_topology = DPLSTM()\n",
    "\n",
    "    # ToDo: global_step might be moved to TRAIN scope\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # call operator to forecaster_topology, over features\n",
    "    forecast = forecaster_topology(features)\n",
    "\n",
    "    # predictions are stored in a dictionary for further use at inference stage\n",
    "    predictions = {\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "\n",
    "    # CHANGE MODEL FUNCTION STRUCTURE ACCORDING TO GILLARD'S ARCHITECTURE\n",
    "\n",
    "    # Estimator in TRAIN or EVAL mode\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # use labels and predictions to define training loss and evaluation loss\n",
    "        # generate summaries for TensorBoard\n",
    "        with tf.name_scope('loss'):\n",
    "            mean_squared_error = tf.losses.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, scope='loss')\n",
    "            tf.summary.scalar('loss', mean_squared_error)\n",
    "\n",
    "        with tf.name_scope('val_loss'):\n",
    "            val_loss = tf.metrics.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, name='mse')\n",
    "            tf.summary.scalar('val_loss', val_loss[1])\n",
    "\n",
    "        # this is only required for PREDICT mode\n",
    "        prediction_hooks = None\n",
    "\n",
    "        # Estimator in TRAIN mode ONLY\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss=mean_squared_error,\n",
    "                    global_step=global_step,\n",
    "                    learning_rate=parameters['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "            # Create a hook to print acc, loss & global step every 100 iter\n",
    "            train_hook_list = []\n",
    "            train_tensors_log = {'val_loss': val_loss[1],\n",
    "                                 'loss': mean_squared_error,\n",
    "                                 'global_step': global_step}\n",
    "            train_hook_list.append(tf.train.LoggingTensorHook(\n",
    "                tensors=train_tensors_log, every_n_iter=100))\n",
    "\n",
    "            predictions = None  # this is not required in TRAIN mode\n",
    "            loss = mean_squared_error\n",
    "            eval_metric_ops = None\n",
    "            training_hooks = train_hook_list\n",
    "            evaluation_hooks = None\n",
    "\n",
    "        else:  # Estimator in EVAL mode ONLY\n",
    "            loss = mean_squared_error\n",
    "            train_op = None\n",
    "            training_hooks = None\n",
    "            eval_metric_ops = {'val_loss': val_loss}\n",
    "            evaluation_hooks = None\n",
    "\n",
    "    # Estimator in PREDICT mode ONLY\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    "        training_hooks = None\n",
    "        evaluation_hooks = None\n",
    "        prediction_hooks = None  # this might change as we are in PREDICT mode\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        # export_outputs=not_used_yet (for TensorFlow Serving, redirected from predictions if omitted)\n",
    "        # training_chief_hooks=not_used_yet\n",
    "        # ToDo: verify use of training_hooks\n",
    "        # temporarily disable training hooks\n",
    "        # training_hooks=training_hooks,\n",
    "        training_hooks=training_hooks,\n",
    "        # scaffold=not_used_yet\n",
    "        evaluation_hooks=evaluation_hooks,\n",
    "        prediction_hooks=prediction_hooks\n",
    "    )\n",
    "\n",
    "# ToDo: evaluate the convenience of packaging execution in a tf.app\n",
    "# in the meantime, run the estimator outside tf.app package\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure file writer cache is clear for TensorBoard events file\n",
    "tf.summary.FileWriterCache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters required for RunConfig()\n",
    "# _EVAL_INTERVAL = 300  # how often checkpoints are written out, given in seconds\n",
    "# _KEEP_CHECKPOINT_MAX = 3  # how many checkpoints to keep\n",
    "# _MAX_TRAIN_STEPS = 16000\n",
    "# _TRAIN_BATCH_SIZE = 2**5\n",
    "# _EVAL_STEPS = None  # if None, evaluate on entire dataset\n",
    "# _EVAL_BATCH_SIZE = 2**16\n",
    "# _START_DELAY_SECONDS = 600\n",
    "# _THROTTLE_SECONDS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_id': 0, '_num_ps_replicas': 0, '_protocol': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_save_summary_steps': 100, '_eval_distribute': None, '_train_distribute': None, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 300, '_session_creation_timeout_secs': 7200, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4d6427b978>, '_num_worker_replicas': 1, '_master': '', '_model_dir': '/home/jupyter/gcp/cbidmltsf/dplstm/lstm_46', '_save_checkpoints_steps': None, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_device_fn': None, '_is_chief': True, '_keep_checkpoint_max': 3, '_global_id_in_cluster': 0, '_task_type': 'worker', '_evaluation_master': '', '_tf_random_seed': None}\n"
     ]
    }
   ],
   "source": [
    "# instantiate base estimator class for custom model function\n",
    "tsf_estimator = tf.estimator.Estimator(\n",
    "    model_fn=time_series_forecaster,\n",
    "    # no parameters passed at this point\n",
    "    # params=hparams,\n",
    "    # parameters passed in original model include: train_data_path, batch_size, augment, train_steps\n",
    "    config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=parameters['eval_interval'],\n",
    "        keep_checkpoint_max=parameters['keep_checkpoint_max']\n",
    "    ),\n",
    "    model_dir='{0}/dplstm/{1}'.format(_ROOT_DIR, parameters['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's train_spec to use train_input_fn and train for so many steps\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['train_data_path'],\n",
    "        batch_size=parameters['train_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.TRAIN\n",
    "    ),\n",
    "    max_steps=parameters['max_train_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: activate TensorFlow Serving for distributed inference\n",
    "# Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "# exporter = tf.estimator.LatestExporter(\n",
    "#     name=\"exporter\",\n",
    "#     serving_input_receiver_fn=serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the customized input function for serving predictions on dplstm\n",
    "# target dictionary is {'hourly': [?, 24, 1], 'daily': [?, 7, 1], 'weekly': [?, 4, 1]}\n",
    "feature_placeholders = {\n",
    "    'hourly': tf.placeholder(tf.float32, [None, 24]),\n",
    "    'daily': tf.placeholder(tf.float32, [None, 7]),\n",
    "    'weekly': tf.placeholder(tf.float32, [None, 4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "\n",
    "    feature_placeholders = {\n",
    "    'hourly': tf.placeholder(tf.float32, [None, 24]),\n",
    "    'daily': tf.placeholder(tf.float32, [None, 7]),\n",
    "    'weekly': tf.placeholder(tf.float32, [None, 4])\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create exporter that uses serving_input_fn to create saved_model for serving\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "    name = \"exporter\", \n",
    "    serving_input_receiver_fn = serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's eval_spec to use eval_input_fn and export saved_model\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['eval_data_path'],\n",
    "        batch_size=parameters['eval_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.EVAL\n",
    "    ),\n",
    "    steps=parameters['eval_steps'],  # use None to evaluate on the entire dataset\n",
    "    exporters=exporter,\n",
    "    start_delay_secs=parameters['start_delay_secs'],  # delay first evaluation\n",
    "    throttle_secs=parameters['throttle_secs']  # evaluate at a different rate (usually longer) than checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /home/jupyter/gcp/cbidmltsf/dplstm/data.py:92: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0054320283, step = 1001\n",
      "INFO:tensorflow:global_step = 1001, loss = 0.0054320283, val_loss = 0.0054320283\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T17:30:32Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-17:30:35\n",
      "INFO:tensorflow:Saving dict for global step 1100: global_step = 1100, loss = 0.0054646125, val_loss = 0.0054646125\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1100: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1100\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/export/exporter/temp-b'1575999035'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.0033034808.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 1100, 'loss': 0.0054646125, 'val_loss': 0.0054646125},\n",
       " [b'/home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/export/exporter/1575999035'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run train_and_evaluate loop\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=tsf_estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the final step... predict using the saved model over TFRecord test dataset\n",
    "# first, locate and load the saved model\n",
    "_LAST_EXPORTER = '1575999035'\n",
    "_SAVED_MODEL_DIR = '{0}/dplstm/{1}/export/exporter/{2}'.format(_ROOT_DIR, parameters['model_dir'], _LAST_EXPORTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/export/exporter/1575999035/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# build a prediction function\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/data/tfrecord/test.tfrecord'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now locate TFRecord test dataset and load it, need to parse the examples to a dictionary\n",
    "test_dataset_filename = '{0}/data/tfrecord/test.tfrecord'.format(_ROOT_DIR)\n",
    "test_dataset_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV1 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now read the dataset from TFRecord file using non-deprecated methods from tf.data module\n",
    "test_raw_dataset = tf.data.TFRecordDataset(test_dataset_filename)\n",
    "test_raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-73-b74d56784b79>:1: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tf.string"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-74-395c1e5fb6c7>:1: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xcf\\x01\\n\\x1e\\n\\x06weekly\\x12\\x14\\x12\\x12\\n\\x10\\x15(\\xda>\\xd2y\\xf4>\\x1b\\x11\\x03?7\\xa7\\x0b?\\n\\x12\\n\\x06target\\x12\\x08\\x12\\x06\\n\\x04W\\xcb\\xfc>\\n)\\n\\x05daily\\x12 \\x12\\x1e\\n\\x1c7\\xa7\\x0b?\\x85\\x9a\\x0c?\\x81\\xf5\\x05?sO\\xd6>\\x9d\\xaa\\xcd>\\x81y\\xc6>\\x01\\x92\\x15?\\nn\\n\\x06hourly\\x12d\\x12b\\n`\\x01\\x92\\x15?\\xcbo\\x1f?\\xe4A/?\\xc4\\xcc(?\\xe1\\x961?\\xce\\x00=?\\xee\\xbcJ?d\\xc1L?\\xea\\xe2+?\\xff+\"?D\\x93\\xed>G\\xc4\\x98>\\xc4d\\xa5>\\xbc\\xd4\\x85>\\\\Og>\\x80\\xf9\\x8f>\\x9d\\x8ep>\\x9f\\xeb\\x8d>I\\xc2\\x88>[b^>\\xb8\\xf8)>N\\xff\\xbb>6\\xb0\\xd4>\\x91H\\xf0>'\n"
     ]
    }
   ],
   "source": [
    "# can I access to the binary, string-based, raw dataset using a one-shot iterator?\n",
    "iterator = test_raw_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# there is only one row in the raw dataset\n",
    "value = sess.run(next_element)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xcf\\x01\\n\\x1e\\n\\x06weekly\\x12\\x14\\x12\\x12\\n\\x10\\x15(\\xda>\\xd2y\\xf4>\\x1b\\x11\\x03?7\\xa7\\x0b?\\n\\x12\\n\\x06target\\x12\\x08\\x12\\x06\\n\\x04W\\xcb\\xfc>\\n)\\n\\x05daily\\x12 \\x12\\x1e\\n\\x1c7\\xa7\\x0b?\\x85\\x9a\\x0c?\\x81\\xf5\\x05?sO\\xd6>\\x9d\\xaa\\xcd>\\x81y\\xc6>\\x01\\x92\\x15?\\nn\\n\\x06hourly\\x12d\\x12b\\n`\\x01\\x92\\x15?\\xcbo\\x1f?\\xe4A/?\\xc4\\xcc(?\\xe1\\x961?\\xce\\x00=?\\xee\\xbcJ?d\\xc1L?\\xea\\xe2+?\\xff+\"?D\\x93\\xed>G\\xc4\\x98>\\xc4d\\xa5>\\xbc\\xd4\\x85>\\\\Og>\\x80\\xf9\\x8f>\\x9d\\x8ep>\\x9f\\xeb\\x8d>I\\xc2\\x88>[b^>\\xb8\\xf8)>N\\xff\\xbb>6\\xb0\\xd4>\\x91H\\xf0>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xcf\\x01\\n\\x1e\\n\\x06wee'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features dictionary for parsing TFrecord files\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.FixedLenFeature([], dtype=tf.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features dictionary for parsing TFrecord files as numpy arrays\n",
    "read_features_as_numpy = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=np.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=np.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=np.float32),\n",
    "    'target': tf.io.FixedLenFeature([], dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another parsing function with no target\n",
    "def _parse_dataset_function_no_target_as_numpy(example_proto, objective_shape):\n",
    "    # parse the input tf.Example proto using the dictionary called read_features (above defined)\n",
    "    row = tf.io.parse_single_example(example_proto, read_features_as_numpy)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shape[0])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shape[1])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shape[2])\n",
    "    return {'hourly': hourly, 'daily': daily, 'weekly': weekly}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset_as_numpy = test_raw_dataset.map(lambda row: _parse_dataset_function_no_target_as_numpy(row, [[24, 1], [7, 1], [4, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': tf.float32, 'hourly': tf.float32, 'weekly': tf.float32}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dataset_as_numpy.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dataset_function(example_proto, objective_shape):\n",
    "    # parse the input tf.Example proto using the dictionary called read_features (above defined)\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shape[0])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shape[1])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shape[2])\n",
    "    target = tf.reshape(row['target'], [1, ])\n",
    "    return {'hourly': hourly, 'daily': daily, 'weekly': weekly}, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another parsing function with no target\n",
    "def _parse_dataset_function_no_target(example_proto, objective_shape):\n",
    "    # parse the input tf.Example proto using the dictionary called read_features (above defined)\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shape[0])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shape[1])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shape[2])\n",
    "    return {'hourly': hourly, 'daily': daily, 'weekly': weekly}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': <tf.Tensor 'IteratorGetNext_7:0' shape=(?, 7, 1) dtype=float32>,\n",
       " 'hourly': <tf.Tensor 'IteratorGetNext_7:1' shape=(?, 24, 1) dtype=float32>,\n",
       " 'weekly': <tf.Tensor 'IteratorGetNext_7:2' shape=(?, 4, 1) dtype=float32>}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dataset.batch(batch_size=1).make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': <tf.Tensor 'IteratorGetNext_8:0' shape=(7, 1) dtype=float32>,\n",
       " 'hourly': <tf.Tensor 'IteratorGetNext_8:1' shape=(24, 1) dtype=float32>,\n",
       " 'weekly': <tf.Tensor 'IteratorGetNext_8:2' shape=(4, 1) dtype=float32>}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dataset_as_numpy.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"IteratorGetNext_10:1\", shape=(24, 1), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 24), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-98d8ce26cfd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_dataset_as_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                             \u001b[0;34m'For reference, the tensor object was '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' which was passed to the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                             'feed with key ' + str(feed) + '.')\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m           \u001b[0msubfeed_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"IteratorGetNext_10:1\", shape=(24, 1), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(?, 24), dtype=float32)."
     ]
    }
   ],
   "source": [
    "predict_fn(parsed_dataset_as_numpy.make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with a custom-generated dictionary\n",
    "fake_parsed_row = {\n",
    "    'hourly': np.zeros([1, 24]),\n",
    "    'daily': np.zeros([1, 7]),\n",
    "    'weekly': np.zeros([1, 4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_parsed_row['hourly'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forecast': array([[0.15336634]], dtype=float32)}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should I use the prediction function over the serialized examples or over the parsed dataset?\n",
    "predict_fn(fake_parsed_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look what happens in Zhang's tutorial\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inputs represented by a Pandas DataFrame.\n",
    "inputs = pd.DataFrame({\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input data into serialized Example strings.\n",
    "examples = []\n",
    "for index, row in inputs.iterrows():\n",
    "    feature = {}\n",
    "    for col, value in row.iteritems():\n",
    "        feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature=feature\n",
    "        )\n",
    "    )\n",
    "    examples.append(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\nb\\n\\x16\\n\\nSepalWidth\\x12\\x08\\x12\\x06\\n\\x0433S@\\n\\x17\\n\\x0bSepalLength\\x12\\x08\\x12\\x06\\n\\x0433\\xa3@\\n\\x17\\n\\x0bPetalLength\\x12\\x08\\x12\\x06\\n\\x04\\x9a\\x99\\xd9?\\n\\x16\\n\\nPetalWidth\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00?',\n",
       " b'\\nb\\n\\x17\\n\\x0bSepalLength\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xcc\\xbc@\\n\\x17\\n\\x0bPetalLength\\x12\\x08\\x12\\x06\\n\\x04ff\\x86@\\n\\x16\\n\\nPetalWidth\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\xc0?\\n\\x16\\n\\nSepalWidth\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00@@',\n",
       " b'\\nb\\n\\x16\\n\\nSepalWidth\\x12\\x08\\x12\\x06\\n\\x04ffF@\\n\\x17\\n\\x0bSepalLength\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xcc\\xdc@\\n\\x17\\n\\x0bPetalLength\\x12\\x08\\x12\\x06\\n\\x04\\xcd\\xcc\\xac@\\n\\x16\\n\\nPetalWidth\\x12\\x08\\x12\\x06\\n\\x04ff\\x06@']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict based on checkpoints and plot results with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anaconda Interactive Visualization\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import output_file, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# persistence for the scaler located in $DATA/scalers\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# reload scaler fitted model here\n",
    "scaler = joblib.load('{0}/data/scalers/ci_LSTM_scaler.save'.format(_ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# build predictions using the trained model\n",
    "# ToDo: use TensorFlow Serving instead of the custom estimator\n",
    "# pred = list(tsf_estimator.predict(input_fn=test_input_fn))\n",
    "pred = list(\n",
    "    tsf_estimator.predict(\n",
    "        input_fn=make_input_fn(\n",
    "            tfrecord_path=parameters['test_data_path'],\n",
    "            batch_size=32,\n",
    "            mode=tf.estimator.ModeKeys.PREDICT\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'forecast': array([0.4926325], dtype=float32)},\n",
       " {'forecast': array([0.54299], dtype=float32)},\n",
       " {'forecast': array([0.5770303], dtype=float32)},\n",
       " {'forecast': array([0.58423465], dtype=float32)},\n",
       " {'forecast': array([0.6234549], dtype=float32)},\n",
       " {'forecast': array([0.6371721], dtype=float32)},\n",
       " {'forecast': array([0.6415772], dtype=float32)},\n",
       " {'forecast': array([0.6419458], dtype=float32)},\n",
       " {'forecast': array([0.6172749], dtype=float32)},\n",
       " {'forecast': array([0.56820893], dtype=float32)}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at a predictions subset\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to array and re-scale\n",
    "pred = [p['forecast'][0] for p in pred]\n",
    "pred = np.asarray(pred)\n",
    "pred_ci = scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "pred_ci = np.squeeze(pred_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.056008 , 4.470618 , 4.7508826, 4.810199 , 5.1331124, 5.2460504,\n",
       "       5.2823186, 5.2853537, 5.0822296, 4.6782537], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ci[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: get ytarget_test array from test.tfrecord dataset to perform the following operation\n",
    "# temporarily get the array from disk\n",
    "y_test = np.load('{0}/data/arrays/y_test.npy'.format(_ROOT_DIR))\n",
    "n = 0  # first step ahead\n",
    "ytarget_test = y_test[:, n]\n",
    "ytarget_test = ytarget_test.reshape(ytarget_test.shape[0], 1)\n",
    "\n",
    "actual_ci = scaler.inverse_transform(ytarget_test)\n",
    "actual_ci = np.squeeze(actual_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EQUIPMENT = 'CPE04105'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/plots/lstm_46.html'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_predictions_fig = figure(title='Predicted Current Imbalance for ' + _EQUIPMENT,\n",
    "                            background_fill_color='#E8DDCB',\n",
    "                            plot_width=1800, plot_height=450, x_axis_type='datetime')\n",
    "\n",
    "# ToDo: yts_test array is required to get timestamps for plot, then wire it now and get it from TFRecord later...\n",
    "yts_test = np.load('{0}/data/arrays/yts_test.npy'.format(_ROOT_DIR), allow_pickle=True)\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        actual_ci, line_color='red',\n",
    "                        line_width=1, alpha=0.7, legend='Actual')\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        pred_ci, line_color='blue',\n",
    "                        line_width=1, alpha=0.7, legend='Predicted')\n",
    "\n",
    "ci_predictions_fig.legend.location = \"top_right\"\n",
    "ci_predictions_fig.legend.background_fill_color = \"darkgrey\"\n",
    "\n",
    "ci_predictions_fig.xaxis.axis_label = 'Timestamp'\n",
    "ci_predictions_fig.yaxis.axis_label = 'Current Imbalance [%]'\n",
    "\n",
    "# output_file('{0}/plots/'.format(_ROOT_DIR) + '{:03d}'.format(n) + '.html', title=_EQUIPMENT)\n",
    "output_file('{0}/plots/'.format(_ROOT_DIR) + '{}'.format(parameters['model_dir']) + '.html', title=_EQUIPMENT)\n",
    "save(ci_predictions_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here, just open the test dataset and use it for prediction\n",
    "# also, verify the cli tool for predictions, as described in Zhang, 2019\n",
    "# saved_model_cli show --dir export/1524906774 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n$ saved_model_cli show --dir 1575917715 --tag_set serve --signature_def serving_default\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['daily'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 7)\\n      name: Placeholder_1:0\\n  inputs['hourly'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 24)\\n      name: Placeholder:0\\n  inputs['weekly'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 4)\\n      name: Placeholder_2:0\\nThe given SavedModel SignatureDef contains the following output(s):\\n  outputs['forecast'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: output_4_1/Sigmoid:0\\nMethod name is: tensorflow/serving/predict\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the saved model, on the terminal\n",
    "'''\n",
    "$ saved_model_cli show --dir 1575917715 --tag_set serve --signature_def serving_default\n",
    "The given SavedModel SignatureDef contains the following input(s):\n",
    "  inputs['daily'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 7)\n",
    "      name: Placeholder_1:0\n",
    "  inputs['hourly'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 24)\n",
    "      name: Placeholder:0\n",
    "  inputs['weekly'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 4)\n",
    "      name: Placeholder_2:0\n",
    "The given SavedModel SignatureDef contains the following output(s):\n",
    "  outputs['forecast'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: output_4_1/Sigmoid:0\n",
    "Method name is: tensorflow/serving/predict\n",
    "'''\n",
    "\n",
    "# It works! Now it's time to test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
