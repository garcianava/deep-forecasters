{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for prediction and evaluation of multi-step forecasting ARTRFDC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# uncomment the following line for compatibility with TensorFlow 1.15 (on GCP)\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# uncomment the following line for TensorFlow 2.X (local execution)\n",
    "import tensorflow as tf\n",
    "\n",
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical mean absolute percentage error\n",
    "def smape(targets, predictions):\n",
    "    '''\n",
    "    predictions: a list with the predicted values\n",
    "    targets: a list with the actual values\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # lists to NumPy arrays\n",
    "    targets, predictions = np.array(targets), np.array(predictions)\n",
    "    # verify predictions and targets have the same shape\n",
    "    if predictions.shape == targets.shape:\n",
    "            return(np.sum(2*np.abs(predictions - targets) /\n",
    "                          (np.abs(targets) + np.abs(predictions)))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'\n",
    "\n",
    "# during batch prediction, the SLDB identifier is obtained via Abseil Flags\n",
    "sldb_id = 'CPE04115_H_kw_20201021084001_ARTRFDC_168'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the SLDB json file\n",
    "data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)\n",
    "\n",
    "# then get the ts_identifier from the json file in the sldb directory\n",
    "sldb_json_file = '{}/sldb.json'.format(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the json file\n",
    "with open(sldb_json_file, 'r') as inputfile:\n",
    "    sldb_dict = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and get the time series identifier\n",
    "ts_identifier = sldb_dict['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# use the time series identifier to obtain the SK-Learn scaler used on it\n",
    "scaler = joblib.load('{}/{}/{}/scaler.save'.format(PROJECT_ROOT,\n",
    "                                                    'timeseries',\n",
    "                                                    ts_identifier))\n",
    "\n",
    "print('Scaler loaded for time series {}'.format(ts_identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all the code to a single notebook cell, then to a function, later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the model identifier is obtained via Abseil Flags\n",
    "model_id = 'ARTRFDC_TPU_001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the dataset name is obtained via Abseil Flags\n",
    "dataset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the execution identifier is obtained via Abseil Flags\n",
    "execution = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model identifier and execution number to build the model directory string\n",
    "model_dir = '{}_{:02d}'.format(model_id, execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to the saved model main directory\n",
    "saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                     'models',\n",
    "                                                     model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in the saved model path, to find the most recent one\n",
    "all_files = os.listdir(saved_model_path)\n",
    "# get the path to the most recent saved model\n",
    "latest_saved_model_id = sorted(all_files)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/ARTRFDC_TPU_001_01/export/exporter/1621561126\n"
     ]
    }
   ],
   "source": [
    "# build the full path for the latest saved model dir\n",
    "export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "print ('Exported model path is {}'.format(export_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model and the prediction function\n",
    "imported = load(export_dir=export_dir, tags='serve')\n",
    "predict_fn = imported.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the dataset for prediction\n",
    "dataset_path = '{}/{}.tfrecord'.format(data_dir, dataset)\n",
    "\n",
    "# load the dataset\n",
    "tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SLDB parameters for the forecasting model\n",
    "config_json_file = '{}/{}/{}.json'.format(PROJECT_ROOT,\n",
    "                                          'parameters',\n",
    "                                          model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the sldb dictionary from the json file in parameters/\n",
    "with open(config_json_file, 'r') as inputfile:\n",
    "    configuration = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': [168, 7], 'target': [168, 7]}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the objective shapes for reshaping tensors in a dictionary\n",
    "_EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "    'source': [configuration['num_timesteps'], configuration['model_dimension']],\n",
    "    'target': [configuration['num_timesteps'], configuration['model_dimension']]\n",
    "}\n",
    "\n",
    "_EXTRACTING_OBJECTIVE_SHAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will be imported as the _parse_dataset_function\n",
    "\n",
    "read_features = {\n",
    "    'source': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    \n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    source = tf.reshape(row['source'].values, objective_shapes['source'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'source': source\n",
    "    }\n",
    "    \n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    # _parse_dataset_function returns:\n",
    "    # features as a dictionary, and\n",
    "    # target as a float vector\n",
    "    return feature_dict, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the test dataset from the TFRecord file\n",
    "# and use its features to produce predictions in a different way\n",
    "\n",
    "parsed_dataset = tfrecord_dataset.map(\n",
    "    lambda row: _parse_dataset_function(\n",
    "        example_proto=row,\n",
    "        objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "        # ToDo: parse the timestamps for plotting or additional positional encoding, later...\n",
    "        parse_timestamp=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on, inferences for ARTRFDC are produced in a very different way\n",
    "# from the used for DMSLSTM or EDALSTM models (prediction process has to be iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each predicted row is a sequence of n_timesteps values,\n",
    "# but only the first element in this sequence is used, as the first prediction,\n",
    "# then it is added (along with its positional encodings) to the end of the input sequence\n",
    "# (first entry of the input sequence is discarded to keep tensor shape)\n",
    "# to get the second prediction, and so on up to the n_timesteps-th prediction,\n",
    "# which completes the n_timesteps prediction sequence (the forecast window)\n",
    "# that starts immediately after the source input sequence ends (in time dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the iterative process for inference over the ARTRFDC saved model can be initiated now:\n",
    "# source feature (?, 168, 7) (unseen data) is on parsed_dataset[0]['source']\n",
    "# target feature (?, 168, 7) (unseen data) is on parsed_dataset[1]\n",
    "\n",
    "# uncomment and run the following two cells to confirm that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is not possible to iterate over a segment of a dataset, as required by iterative inference\n",
    "# then the complete test dataset will be passed to two NumPy arrays:\n",
    "\n",
    "# source_array, with shape (n_rows, n_timesteps, n_features), in this example (2095, 168, 7), and\n",
    "# target_array, with shape (n_rows, n_timesteps, n_features), in this example (2095, 168, 7)\n",
    "\n",
    "# remember source_array[1:, :, :] = target_array[:-2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = list()\n",
    "target_list = list()\n",
    "\n",
    "for element in parsed_dataset:\n",
    "    source_list.append(element[0]['source'])\n",
    "    target_list.append(element[1])\n",
    "\n",
    "source_array = np.array(source_list)\n",
    "target_array = np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2095, 168, 7), (2095, 168, 7))"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify shape of resulting arrays\n",
    "source_array.shape, target_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now follow the inference process detailed in Klingenbrunn to:\n",
    "# predict over the forecast window,\n",
    "# calculate prediction error metrics, and\n",
    "# plot prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a forecast window to guide the iterative prediction process\n",
    "# start with a hourly, day-ahead process\n",
    "forecast_window = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first source or input to the model is the first source row\n",
    "# that means, the true variable value, plus the six positional encodings for the timestamp\n",
    "# in the first row of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to get the source as a tensor with TensorShape([1, 168, 7])\n",
    "# source_tensor = tf.expand_dims(source_array[0, :, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: include timestamps in train, eval, and test datasets to easily keep tracking of prediction dates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now please code all the complicated previous stuff into an easy Python function, would you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important, the inference cycle was coded for tensors, not for NumPy arrays\n",
    "# then use source and prediction tensors and translate to tensor examples from float tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which row of the test dataset will be used for inference?\n",
    "row = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 168, 7])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then again, build the initial source tensor\n",
    "source_tensor = tf.expand_dims(source_array[row, :, :], axis=0)\n",
    "source_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 24, 7])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and build the initial prediction tensor\n",
    "# a forecast-window-sized tensor (1, forecast_window, 7)\n",
    "# formed with the forecast_window true values, starting at the end of the source tensor\n",
    "# that means\n",
    "\n",
    "prediction_tensor = tf.expand_dims(source_array[row + 168, :forecast_window, :], axis=0)\n",
    "prediction_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: find out how many prediction tensors can be formed from a dataset with n_rows!\n",
    "# use this value as a limit for the inference process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_tensor_example(float_tensor):\n",
    "    # first, pass the float tensor to NumPy array, then flatten it\n",
    "    flat_array = float_tensor.numpy().flatten()\n",
    "    # second, build the protobuffer example\n",
    "    example = tf.train.Example(\n",
    "        # features within the example\n",
    "        features=tf.train.Features(\n",
    "            # individual feature definition\n",
    "            feature={'source': _float_feature_from_list_of_values(flat_array)}\n",
    "        )\n",
    "    )    \n",
    "    # third, serialize the example dictionary to a string\n",
    "    serialized_example = example.SerializeToString()\n",
    "    # fourth, wrap the serialized example as a NumPy-string array\n",
    "    numpy_example = np.array(serialized_example, dtype='S')\n",
    "    # fifth, wrap the NumPy-string array as a string tensor\n",
    "    tensor_example = tf.convert_to_tensor(numpy_example)\n",
    "\n",
    "    return tensor_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_input_model = source_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize the prediction list previously used for prediction over TFRecords\n",
    "predictions_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(forecast_window):\n",
    "    \n",
    "    # from the current next_input_model tensor (1, 168, 7)\n",
    "    # get a prediction as NumPy array (1, 168, 1)\n",
    "    prediction = predict_fn(tensor_to_tensor_example(next_input_model))['forecast'].numpy()\n",
    "    \n",
    "    # get the value of the most recent prediction (last timestep) into the predictions list\n",
    "    predictions_list.append(prediction[:, -1, :][0][0])\n",
    "    \n",
    "    # from the source tensor, get the positional encodings for ti+1 to t167 (that is 168-i-1 values)\n",
    "    pos_encoding_old_values = source_tensor[:, i+1:, 1:]\n",
    "\n",
    "    # from target tensor, get the positional encodings for t168 to t168+i (that is i+1 values)\n",
    "    pos_encoding_new_val = prediction_tensor[:, :i+1, 1:]\n",
    "\n",
    "    # build new positional encodings with 168 values\n",
    "    pos_encodings = tf.concat([pos_encoding_old_values, pos_encoding_new_val], axis=1)\n",
    "    pos_encodings = tf.cast(pos_encodings, dtype=tf.float32)\n",
    "\n",
    "    # build the values feature for the next input to the model\n",
    "    # pop i+1 values at the beginning of the previous input\n",
    "    value_feature_old_values = tf.expand_dims(source_tensor[:, i+1:, 0], axis=-1)\n",
    "    value_feature_old_values = tf.cast(value_feature_old_values, dtype=tf.float32)\n",
    "\n",
    "    # current predictions_list to NumPy array\n",
    "    value_feature_new_values = np.array(predictions_list[:i+1])\n",
    "\n",
    "    # current prediction array to tensor\n",
    "    value_feature_new_values = tf.convert_to_tensor(value_feature_new_values)\n",
    "\n",
    "    # expand dimensions of current prediction tensor to single-value feature\n",
    "    value_feature_new_values = tf.expand_dims(value_feature_new_values, axis=-1)\n",
    "\n",
    "    # expand dimensions of current prediction tensor to single-value batch\n",
    "    value_feature_new_values = tf.expand_dims(value_feature_new_values, axis=0)\n",
    "    \n",
    "    # build the value feature tensor\n",
    "    next_input_model = tf.concat([value_feature_old_values, value_feature_new_values], axis=1)\n",
    "    \n",
    "    # build the next input tensor for the model\n",
    "    next_input_model = tf.concat([next_input_model, pos_encodings], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5235025 , 0.5317937 , 0.50272   , 0.5320462 , 0.5496116 ,\n",
       "       0.4500504 , 0.33263466, 0.21609625, 0.14511836, 0.10028085,\n",
       "       0.07016093, 0.06287903, 0.07648793, 0.16574663, 0.22860813,\n",
       "       0.24710315, 0.3021697 , 0.38982624, 0.4534402 , 0.483442  ,\n",
       "       0.5009064 , 0.5435877 , 0.55509764, 0.5044207 ], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterative predictions over the forecast window reside in predictions_list\n",
    "np.array(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(24,), dtype=float32, numpy=\n",
       "array([0.48842862, 0.4568397 , 0.4524253 , 0.5191421 , 0.55113226,\n",
       "       0.48998427, 0.3736661 , 0.26310343, 0.18418387, 0.12973031,\n",
       "       0.08332933, 0.09128969, 0.10174082, 0.12976441, 0.18660644,\n",
       "       0.19949254, 0.24072066, 0.27756765, 0.33287108, 0.4307677 ,\n",
       "       0.5224649 , 0.5580235 , 0.5968128 , 0.52098125], dtype=float32)>"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the true values remain in the prediction tensor\n",
    "prediction_tensor[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1618581215540568"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape(prediction_tensor[0, :, 0], np.array(predictions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the predictive performance of the ARTRFDC model is very sensitive to the input source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
