{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revision of TFT basic components\n",
    "# from https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer definitions.\n",
    "concat = tf.keras.backend.concatenate\n",
    "stack = tf.keras.backend.stack\n",
    "K = tf.keras.backend\n",
    "Add = tf.keras.layers.Add\n",
    "LayerNorm = tf.keras.layers.LayerNormalization\n",
    "Dense = tf.keras.layers.Dense\n",
    "Multiply = tf.keras.layers.Multiply\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "Activation = tf.keras.layers.Activation\n",
    "Lambda = tf.keras.layers.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size,\n",
    "                 activation=None,\n",
    "                 use_time_distributed=False,\n",
    "                 use_bias=True):\n",
    "  \"\"\"Returns simple Keras linear layer.\n",
    "  Args:\n",
    "    size: Output size\n",
    "    activation: Activation function to apply if required\n",
    "    use_time_distributed: Whether to apply layer across time\n",
    "    use_bias: Whether bias should be included in layer\n",
    "  \"\"\"\n",
    "  linear = tf.keras.layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "  if use_time_distributed:\n",
    "    linear = tf.keras.layers.TimeDistributed(linear)\n",
    "  return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(x,\n",
    "                           hidden_layer_size,\n",
    "                           output_size=None,\n",
    "                           dropout_rate=None,\n",
    "                           use_time_distributed=True,\n",
    "                           additional_context=None,\n",
    "                           return_gate=False):\n",
    "  \"\"\"Applies the gated residual network (GRN) as defined in paper.\n",
    "  Args:\n",
    "    x: Network inputs\n",
    "    hidden_layer_size: Internal state size\n",
    "    output_size: Size of output layer\n",
    "    dropout_rate: Dropout rate if dropout is applied\n",
    "    use_time_distributed: Whether to apply network across time dimension\n",
    "    additional_context: Additional context vector to use if relevant\n",
    "    return_gate: Whether to return GLU gate for diagnostic purposes\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GRN output, GLU gate)\n",
    "  \"\"\"\n",
    "\n",
    "  # Setup skip connection\n",
    "  if output_size is None:\n",
    "    output_size = hidden_layer_size\n",
    "    skip = x\n",
    "  else:\n",
    "    linear = Dense(output_size)\n",
    "    if use_time_distributed:\n",
    "      linear = tf.keras.layers.TimeDistributed(linear)\n",
    "    skip = linear(x)\n",
    "\n",
    "  # Apply feedforward network\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          x)\n",
    "  if additional_context is not None:\n",
    "    hidden = hidden + linear_layer(\n",
    "        hidden_layer_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=False)(\n",
    "            additional_context)\n",
    "  hidden = tf.keras.layers.Activation('elu')(hidden)\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          hidden)\n",
    "\n",
    "  gating_layer, gate = apply_gating_layer(\n",
    "      hidden,\n",
    "      output_size,\n",
    "      dropout_rate=dropout_rate,\n",
    "      use_time_distributed=use_time_distributed,\n",
    "      activation=None)\n",
    "\n",
    "  if return_gate:\n",
    "    return add_and_norm([skip, gating_layer]), gate\n",
    "  else:\n",
    "    return add_and_norm([skip, gating_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gating_layer(x,\n",
    "                       hidden_layer_size,\n",
    "                       dropout_rate=None,\n",
    "                       use_time_distributed=True,\n",
    "                       activation=None):\n",
    "  \"\"\"Applies a Gated Linear Unit (GLU) to an input.\n",
    "  Args:\n",
    "    x: Input to gating layer\n",
    "    hidden_layer_size: Dimension of GLU\n",
    "    dropout_rate: Dropout rate to apply if any\n",
    "    use_time_distributed: Whether to apply across time\n",
    "    activation: Activation function to apply to the linear feature transform if\n",
    "      necessary\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GLU output, gate)\n",
    "  \"\"\"\n",
    "\n",
    "  if dropout_rate is not None:\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "  if use_time_distributed:\n",
    "    activation_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=activation))(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'))(\n",
    "            x)\n",
    "  else:\n",
    "    activation_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=activation)(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation='sigmoid')(\n",
    "            x)\n",
    "\n",
    "  return tf.keras.layers.Multiply()([activation_layer,\n",
    "                                     gated_layer]), gated_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(x_list):\n",
    "  \"\"\"Applies skip connection followed by layer normalisation.\n",
    "  Args:\n",
    "    x_list: List of inputs to sum for skip connection\n",
    "  Returns:\n",
    "    Tensor output from layer.\n",
    "  \"\"\"\n",
    "  tmp = Add()(x_list)\n",
    "  tmp = LayerNorm()(tmp)\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(self_attn_inputs):\n",
    "  \"\"\"Returns causal mask to apply for self-attention layer.\n",
    "  Args:\n",
    "    self_attn_inputs: Inputs to self attention layer to determine mask shape\n",
    "  \"\"\"\n",
    "  len_s = tf.shape(self_attn_inputs)[1]\n",
    "  bs = tf.shape(self_attn_inputs)[:1]\n",
    "  mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "  \"\"\"Defines scaled dot product attention layer.\n",
    "  Attributes:\n",
    "    dropout: Dropout rate to use\n",
    "    activation: Normalisation function for scaled dot product attention (e.g.\n",
    "      softmax by default)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, attn_dropout=0.0):\n",
    "    self.dropout = Dropout(attn_dropout)\n",
    "    self.activation = Activation('softmax')\n",
    "\n",
    "  def __call__(self, q, k, v, mask):\n",
    "    \"\"\"Applies scaled dot product attention.\n",
    "    Args:\n",
    "      q: Queries\n",
    "      k: Keys\n",
    "      v: Values\n",
    "      mask: Masking if required -- sets softmax to very large value\n",
    "    Returns:\n",
    "      Tuple of (layer outputs, attention weights)\n",
    "    \"\"\"\n",
    "    temper = tf.sqrt(tf.cast(tf.shape(k)[-1], dtype='float32'))\n",
    "    attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [q, k])  # shape=(batch, q, k)\n",
    "    if mask is not None:\n",
    "      mmask = Lambda(lambda x: (-1e+9) * (1. - K.cast(x, 'float32')))(\n",
    "          mask)  # setting to infinity\n",
    "      attn = Add()([attn, mmask])\n",
    "    attn = self.activation(attn)\n",
    "    attn = self.dropout(attn)\n",
    "    output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, v])\n",
    "    return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is samples, timesteps, features (from the stacked-LSTM encoder output)\n",
    "dataset = np.array([\n",
    "        [0., 1., 2., 3., 4., 5., 6., 7.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 8], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention():\n",
    "  \"\"\"Defines interpretable multi-head attention layer.\n",
    "  Attributes:\n",
    "    n_head: Number of heads\n",
    "    d_k: Key/query dimensionality per head\n",
    "    d_v: Value dimensionality\n",
    "    dropout: Dropout rate to apply\n",
    "    qs_layers: List of queries across heads\n",
    "    ks_layers: List of keys across heads\n",
    "    vs_layers: List of values across heads\n",
    "    attention: Scaled dot product attention layer\n",
    "    w_o: Output weight matrix to project internal state to the original TFT\n",
    "      state size\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_head, d_model, dropout):\n",
    "    \"\"\"Initialises layer.\n",
    "    Args:\n",
    "      n_head: Number of heads\n",
    "      d_model: TFT state dimensionality\n",
    "      dropout: Dropout discard rate\n",
    "    \"\"\"\n",
    "\n",
    "    self.n_head = n_head\n",
    "    self.d_k = self.d_v = d_k = d_v = d_model // n_head\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.qs_layers = []\n",
    "    self.ks_layers = []\n",
    "    self.vs_layers = []\n",
    "\n",
    "    # Use same value layer to facilitate interp\n",
    "    vs_layer = Dense(d_v, use_bias=False)\n",
    "\n",
    "    for _ in range(n_head):\n",
    "      self.qs_layers.append(Dense(d_k, use_bias=False))\n",
    "      self.ks_layers.append(Dense(d_k, use_bias=False))\n",
    "      self.vs_layers.append(vs_layer)  # use same vs_layer\n",
    "\n",
    "    self.attention = ScaledDotProductAttention()\n",
    "    self.w_o = Dense(d_model, use_bias=False)\n",
    "\n",
    "  def __call__(self, q, k, v, mask=None):\n",
    "    \"\"\"Applies interpretable multihead attention.\n",
    "    Using T to denote the number of time steps fed into the transformer.\n",
    "    Args:\n",
    "      q: Query tensor of shape=(?, T, d_model)\n",
    "      k: Key of shape=(?, T, d_model)\n",
    "      v: Values of shape=(?, T, d_model)\n",
    "      mask: Masking if required with shape=(?, T, T)\n",
    "    Returns:\n",
    "      Tuple of (layer outputs, attention weights)\n",
    "    \"\"\"\n",
    "    n_head = self.n_head\n",
    "\n",
    "    heads = []\n",
    "    attns = []\n",
    "    for i in range(n_head):\n",
    "      qs = self.qs_layers[i](q)\n",
    "      ks = self.ks_layers[i](k)\n",
    "      vs = self.vs_layers[i](v)\n",
    "      head, attn = self.attention(qs, ks, vs, mask)\n",
    "\n",
    "      head_dropout = Dropout(self.dropout)(head)\n",
    "      heads.append(head_dropout)\n",
    "      attns.append(attn)\n",
    "    head = K.stack(heads) if n_head > 1 else heads[0]\n",
    "    attn = K.stack(attns)\n",
    "\n",
    "    outputs = K.mean(head, axis=0) if n_head > 1 else head\n",
    "    outputs = self.w_o(outputs)\n",
    "    outputs = Dropout(self.dropout)(outputs)  # output dropout\n",
    "\n",
    "    return outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 8], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_decoder_mask(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of heads\n",
    "n_head = 1\n",
    "# dimensionality of transformer (for a 8-timestep dataset)\n",
    "d_model = 4\n",
    "#dropout\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
