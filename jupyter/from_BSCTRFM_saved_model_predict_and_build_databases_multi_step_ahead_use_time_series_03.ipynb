{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive inference (multi-step) for BSCTRFM models\n",
    "\n",
    "### use time series instead of SLDB arrays for easier and more efficient timestamp management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# uncomment the following line for compatibility with TensorFlow 1.15 (on GCP)\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# uncomment the following line for TensorFlow 2.X (local execution)\n",
    "import tensorflow as tf\n",
    "\n",
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "# select a palette\n",
    "from bokeh.palettes import d3\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical mean absolute percentage error\n",
    "def smape(targets, predictions):\n",
    "    '''\n",
    "    predictions: a list with the predicted values\n",
    "    targets: a list with the actual values\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # lists to NumPy arrays\n",
    "    targets, predictions = np.array(targets), np.array(predictions)\n",
    "    # verify predictions and targets have the same shape\n",
    "    if predictions.shape == targets.shape:\n",
    "            return(np.sum(2*np.abs(predictions - targets) /\n",
    "                          (np.abs(targets) + np.abs(predictions)))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a set of tensors to a feature dict to a serialized example to pass it\n",
    "# to the prediction function of the saved model \n",
    "def input_tensors_to_serialized_example(encoder_input_float_tensor, decoder_input_float_tensor):\n",
    "    # first, pass the float tensors to NumPy array, then flatten them\n",
    "    encoder_input_flat_array = encoder_input_float_tensor.numpy().flatten()\n",
    "    decoder_input_flat_array = decoder_input_float_tensor.numpy().flatten()\n",
    "    # second, build the protobuffer example\n",
    "    example = tf.train.Example(\n",
    "        # features within the example\n",
    "        features=tf.train.Features(\n",
    "            # feature definition\n",
    "            feature={\n",
    "                'encoder_input': _float_feature_from_list_of_values(encoder_input_flat_array),\n",
    "                'decoder_input': _float_feature_from_list_of_values(decoder_input_flat_array)\n",
    "            }\n",
    "        )\n",
    "    )    \n",
    "    # third, serialize the example dictionary to a string\n",
    "    serialized_example = example.SerializeToString()\n",
    "    # fourth, wrap the serialized example as a NumPy-string array\n",
    "    numpy_example = np.array(serialized_example, dtype='S')\n",
    "    # fifth, wrap the NumPy-string array as a string tensor\n",
    "    serialized_example = tf.convert_to_tensor(numpy_example)\n",
    "\n",
    "    return serialized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the time series for the test dataset (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a identifier string to access to the preprocessed time series\n",
    "identifier = 'CPE04115_H_kw_20210526212214'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the time series directory\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dictionary to remain the code consistent with the SLDB building process\n",
    "# most of the times, only ts['test'] will be used for inference\n",
    "# however, ts['eval'] might also be used, as it have not really been seen by training process\n",
    "# (no tranining modification resulted from evaluation stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test set for the following operations\n",
    "stage = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[stage] = pd.read_pickle('{}/ts_{}.pkl'.format(time_series_folder, stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many lectures in the loaded time series\n",
    "ts[stage]['kw_scaled'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-05-01 00:00:00'), Timestamp('2018-07-31 23:00:00'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end timestamp of the loaded time series\n",
    "ts[stage].index[0], ts[stage].index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-01 00:00:00</th>\n",
       "      <td>0.277562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 01:00:00</th>\n",
       "      <td>0.174138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 02:00:00</th>\n",
       "      <td>0.114769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 03:00:00</th>\n",
       "      <td>0.099625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 04:00:00</th>\n",
       "      <td>0.080639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 19:00:00</th>\n",
       "      <td>0.651798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 20:00:00</th>\n",
       "      <td>0.656658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 21:00:00</th>\n",
       "      <td>0.691807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 22:00:00</th>\n",
       "      <td>0.606559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 23:00:00</th>\n",
       "      <td>0.477601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled\n",
       "timestamp                     \n",
       "2018-05-01 00:00:00   0.277562\n",
       "2018-05-01 01:00:00   0.174138\n",
       "2018-05-01 02:00:00   0.114769\n",
       "2018-05-01 03:00:00   0.099625\n",
       "2018-05-01 04:00:00   0.080639\n",
       "...                        ...\n",
       "2018-07-31 19:00:00   0.651798\n",
       "2018-07-31 20:00:00   0.656658\n",
       "2018-07-31 21:00:00   0.691807\n",
       "2018-07-31 22:00:00   0.606559\n",
       "2018-07-31 23:00:00   0.477601\n",
       "\n",
       "[2208 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new positional encodings: hour-day, day-week, week-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "weeks_in_year = 53\n",
    "\n",
    "# hour of the day: 0-23\n",
    "timestamp_hour_day = np.array(ts[stage].index.hour)\n",
    "# day of the week: 0-6\n",
    "day_week_list = [timestamp.weekday() for timestamp in ts[stage].index]\n",
    "timestamp_day_week = np.array(day_week_list)\n",
    "# pd.timestamp.week values go from 1 to 53\n",
    "# adjust them to 0-52\n",
    "week_values = ts[stage].index.week - 1\n",
    "timestamp_week_year = np.array(week_values)\n",
    "\n",
    "# build arrays with positional encoding components and cast them to float32\n",
    "sin_hour_day = np.sin(2*np.pi*timestamp_hour_day/hours_in_day).astype(np.float32)\n",
    "cos_hour_day = np.cos(2*np.pi*timestamp_hour_day/hours_in_day).astype(np.float32)\n",
    "\n",
    "sin_day_week = np.sin(2*np.pi*timestamp_day_week/days_in_week).astype(np.float32)\n",
    "cos_day_week = np.cos(2*np.pi*timestamp_day_week/days_in_week).astype(np.float32)\n",
    "\n",
    "sin_week_year = np.sin(2*np.pi*timestamp_week_year/weeks_in_year).astype(np.float32)\n",
    "cos_week_year = np.cos(2*np.pi*timestamp_week_year/weeks_in_year).astype(np.float32)\n",
    "\n",
    "# now expand the time series dataframe with positional encoding components\n",
    "# pass the pos encoding arrays to dataframe as lists\n",
    "ts[stage]['sin_hour_day'] = list(sin_hour_day)\n",
    "ts[stage]['cos_hour_day'] = list(cos_hour_day)\n",
    "ts[stage]['sin_day_week'] = list(sin_day_week)\n",
    "ts[stage]['cos_day_week'] = list(cos_day_week)\n",
    "ts[stage]['sin_week_year'] = list(sin_week_year)\n",
    "ts[stage]['cos_week_year'] = list(cos_week_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "      <th>sin_hour_day</th>\n",
       "      <th>cos_hour_day</th>\n",
       "      <th>sin_day_week</th>\n",
       "      <th>cos_day_week</th>\n",
       "      <th>sin_week_year</th>\n",
       "      <th>cos_week_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-01 00:00:00</th>\n",
       "      <td>0.277562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.902798</td>\n",
       "      <td>-0.430065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 01:00:00</th>\n",
       "      <td>0.174138</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.902798</td>\n",
       "      <td>-0.430065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 02:00:00</th>\n",
       "      <td>0.114769</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.902798</td>\n",
       "      <td>-0.430065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 03:00:00</th>\n",
       "      <td>0.099625</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.902798</td>\n",
       "      <td>-0.430065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01 04:00:00</th>\n",
       "      <td>0.080639</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.902798</td>\n",
       "      <td>-0.430065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 19:00:00</th>\n",
       "      <td>0.651798</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-0.403123</td>\n",
       "      <td>-0.915146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 20:00:00</th>\n",
       "      <td>0.656658</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-0.403123</td>\n",
       "      <td>-0.915146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 21:00:00</th>\n",
       "      <td>0.691807</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-0.403123</td>\n",
       "      <td>-0.915146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 22:00:00</th>\n",
       "      <td>0.606559</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-0.403123</td>\n",
       "      <td>-0.915146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31 23:00:00</th>\n",
       "      <td>0.477601</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-0.403123</td>\n",
       "      <td>-0.915146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2208 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled  sin_hour_day  cos_hour_day  sin_day_week  \\\n",
       "timestamp                                                                  \n",
       "2018-05-01 00:00:00   0.277562      0.000000      1.000000      0.781832   \n",
       "2018-05-01 01:00:00   0.174138      0.258819      0.965926      0.781832   \n",
       "2018-05-01 02:00:00   0.114769      0.500000      0.866025      0.781832   \n",
       "2018-05-01 03:00:00   0.099625      0.707107      0.707107      0.781832   \n",
       "2018-05-01 04:00:00   0.080639      0.866025      0.500000      0.781832   \n",
       "...                        ...           ...           ...           ...   \n",
       "2018-07-31 19:00:00   0.651798     -0.965926      0.258819      0.781832   \n",
       "2018-07-31 20:00:00   0.656658     -0.866025      0.500000      0.781832   \n",
       "2018-07-31 21:00:00   0.691807     -0.707107      0.707107      0.781832   \n",
       "2018-07-31 22:00:00   0.606559     -0.500000      0.866025      0.781832   \n",
       "2018-07-31 23:00:00   0.477601     -0.258819      0.965926      0.781832   \n",
       "\n",
       "                     cos_day_week  sin_week_year  cos_week_year  \n",
       "timestamp                                                        \n",
       "2018-05-01 00:00:00       0.62349       0.902798      -0.430065  \n",
       "2018-05-01 01:00:00       0.62349       0.902798      -0.430065  \n",
       "2018-05-01 02:00:00       0.62349       0.902798      -0.430065  \n",
       "2018-05-01 03:00:00       0.62349       0.902798      -0.430065  \n",
       "2018-05-01 04:00:00       0.62349       0.902798      -0.430065  \n",
       "...                           ...            ...            ...  \n",
       "2018-07-31 19:00:00       0.62349      -0.403123      -0.915146  \n",
       "2018-07-31 20:00:00       0.62349      -0.403123      -0.915146  \n",
       "2018-07-31 21:00:00       0.62349      -0.403123      -0.915146  \n",
       "2018-07-31 22:00:00       0.62349      -0.403123      -0.915146  \n",
       "2018-07-31 23:00:00       0.62349      -0.403123      -0.915146  \n",
       "\n",
       "[2208 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts[stage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on, all data required for inference will be extracted from the time series\n",
    "# therefore, remove all references to SLDB datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the model identifier is obtained via Abseil Flags\n",
    "# remember this notebook is based on local execution,\n",
    "# therefore model directory must be downloaded from GS before running the notebook\n",
    "model_id = 'BSCTRFM_TPU_009'\n",
    "\n",
    "# during batch prediction, the SLDB identifier is obtained via Abseil Flags\n",
    "# THE SLDB FOR INFERENCE MUST BE THE SAME USED FOR TRAINING! (THE ONE SETUP IN THE CONFIGURATION FILE)\n",
    "sldb_id = 'CPE04115_H_kw_20210526212214_BSCTRFM_168_168'\n",
    "\n",
    "# during batch prediction, the dataset name is obtained via Abseil Flags\n",
    "dataset = 'test'\n",
    "\n",
    "# define a forecast window to guide the iterative prediction process\n",
    "# start with a hourly, day-ahead process\n",
    "forecast_window = 24\n",
    "\n",
    "# ADD AN INFERENCE IDENTIFIER, BECAUSE FOR TRANSFORMER-BASED MODELS, DIFFERENT INFERENCES\n",
    "# CAN BE PRODUCED FROM A SINGLE SAVED MODEL (USUALLY TO PRODUCE DIFFERENT FORECAST WINDOWS)\n",
    "# during batch prediction, the inference identifier should be obtained via Abseil Flags\n",
    "inference = '{:03d}'.format(forecast_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the SLDB json file\n",
    "data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)\n",
    "\n",
    "# then get the ts_identifier from the json file in the sldb directory\n",
    "sldb_json_file = '{}/sldb.json'.format(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the json file\n",
    "with open(sldb_json_file, 'r') as inputfile:\n",
    "    sldb_dict = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04115_H_kw_20210526212214'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and get the time series identifier\n",
    "ts_identifier = sldb_dict['ts']\n",
    "ts_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler on test dataset loaded for time series CPE04115_H_kw_20210526212214\n"
     ]
    }
   ],
   "source": [
    "# use the time series identifier to obtain the SK-Learn scaler used on it\n",
    "# get the scaler used to normalize the test dataset (unseen)\n",
    "scaler_test = joblib.load('{}/{}/{}/scaler_test.save'.format(PROJECT_ROOT,\n",
    "                                                             'timeseries',\n",
    "                                                             ts_identifier))\n",
    "\n",
    "print('Scaler on test dataset loaded for time series {}'.format(ts_identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SLDB parameters for the forecasting model\n",
    "config_json_file = '{}/{}/{}.json'.format(PROJECT_ROOT,\n",
    "                                          'parameters',\n",
    "                                          model_id)\n",
    "\n",
    "# recover the sldb dictionary from the json file in parameters/\n",
    "with open(config_json_file, 'r') as inputfile:\n",
    "    configuration = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jump to global predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the prediction function and test it with the adequate tensor-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_TPU_008_07'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use model identifier and execution number to build the model directory string\n",
    "execution = 7\n",
    "model_dir = '{}_{:02d}'.format(model_id, execution)\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_008_07/export/exporter'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the path to the saved model main directory\n",
    "saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                     'models',\n",
    "                                                     model_dir)\n",
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1627485676'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the files in the saved model path, to find the most recent one\n",
    "all_files = os.listdir(saved_model_path)\n",
    "# get the path to the most recent saved model\n",
    "latest_saved_model_id = sorted(all_files)[-1]\n",
    "latest_saved_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_008_07/export/exporter/1627485676\n"
     ]
    }
   ],
   "source": [
    "# build the full path for the latest saved model dir\n",
    "export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "print ('Exported model path is {}'.format(export_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction pruned(example_bytes) at 0x7F199C38F290>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the saved model and the prediction function\n",
    "imported = load(export_dir=export_dir, tags='serve')\n",
    "predict_fn = imported.signatures[\"serving_default\"]\n",
    "predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'CPE04115_H_kw_20210526212214',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'stats': {'train': {'n_rows': 17207},\n",
       "  'eval': {'n_rows': 2544},\n",
       "  'test': {'n_rows': 1873}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sldb_dict['embedding']['hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sldb_dict['no_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BSCTRFM_TPU_009', 'test', '024')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the values of the variables for batch inference\n",
    "model_id, dataset, inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run inference process and build databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_00/export/exporter/1627917517\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_00_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_00_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_01/export/exporter/1627917726\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_01_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_01_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_02/export/exporter/1627917922\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_02_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_02_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_03/export/exporter/1627918120\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_03_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_03_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_04/export/exporter/1627918318\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_04_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_04_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_05/export/exporter/1627918513\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_05_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_05_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_06/export/exporter/1627918715\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_06_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_06_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_07/export/exporter/1627918934\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_07_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_07_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_08/export/exporter/1627919135\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_08_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_08_test_024\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/BSCTRFM_TPU_009_09/export/exporter/1627919334\n",
      "Persisted Pandas dataframe for predictions detail of BSCTRFM_TPU_009_09_test_024\n",
      "Persisted Pandas dataframe for predictions summary of BSCTRFM_TPU_009_09_test_024\n"
     ]
    }
   ],
   "source": [
    "# during batch prediction, the execution identifier is obtained via Abseil Flags\n",
    "for execution in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    # a columns list for the predictions dataframe\n",
    "    pred_df_columns = ['model_id',\n",
    "                       'execution',\n",
    "                       'dataset',\n",
    "                       'inference',\n",
    "                       'string_timestamps',\n",
    "                       'predictions',\n",
    "                       'targets']\n",
    "    \n",
    "    # build the predictions dataframe\n",
    "    predictions_detail_df = pd.DataFrame(columns=pred_df_columns)\n",
    "\n",
    "    # use model identifier and execution number to build the model directory string\n",
    "    model_dir = '{}_{:02d}'.format(model_id, execution)\n",
    "\n",
    "    # get the path to the saved model main directory\n",
    "    saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                         'models',\n",
    "                                                         model_dir)\n",
    "\n",
    "    # get all the files in the saved model path, to find the most recent one\n",
    "    all_files = os.listdir(saved_model_path)\n",
    "    # get the path to the most recent saved model\n",
    "    latest_saved_model_id = sorted(all_files)[-1]\n",
    "\n",
    "    # build the full path for the latest saved model dir\n",
    "    export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "    print ('Exported model path is {}'.format(export_dir))\n",
    "\n",
    "    # load the saved model and the prediction function\n",
    "    imported = load(export_dir=export_dir, tags='serve')\n",
    "    predict_fn = imported.signatures[\"serving_default\"]\n",
    "    \n",
    "    # iterate on a set of valid rows of the test dataset\n",
    "    starting_point = 0 # based on the inference dataset\n",
    "    span = 8*7*24 # number of weeks expressed in hours\n",
    "    dataset_row_indexes_list = starting_point + np.arange(span)\n",
    "    \n",
    "    for start_index in dataset_row_indexes_list:\n",
    "\n",
    "        # define first prediction interval with start- and end-index\n",
    "        # given the interval time_series[start_index:end_index]\n",
    "        # the conditioning range is the union of the encoder-input and the decoder-input\n",
    "        # and the prediction range is only the last lecture in the interval,\n",
    "        # by means of a recursive inference process\n",
    "        # on each step the last prediction is added to the decoder input\n",
    "        # and the prediction range grows one step into the future\n",
    "\n",
    "        # get the end-index of this recursive inference interval\n",
    "        end_index = start_index + (m + t)\n",
    "\n",
    "        # initialize a list to store recurrent predictions for this interval\n",
    "        predictions_list = list()\n",
    "\n",
    "        for i in np.arange(forecast_window):\n",
    "\n",
    "            # build the inference interval as a sub-series of the dataset\n",
    "            sub_series = ts[stage][start_index + i : end_index + i]\n",
    "\n",
    "            # important: build sources as copies of the sub-series (and therefore of the global time series)\n",
    "            # to avoid overwriting the original dataset\n",
    "\n",
    "            # the encoder input source\n",
    "            encoder_input = sub_series[:m].copy()\n",
    "\n",
    "            # the decoder input source\n",
    "            decoder_input = sub_series[m-1:-1].copy()\n",
    "\n",
    "            # on first step (i=0), the decoder input carries only true values\n",
    "            # and the predictions list is empty\n",
    "            # on subsequent steps, the decoder input includes all previous predictions\n",
    "            # (stored in the predictions list)\n",
    "            if i > 0:\n",
    "                decoder_input['kw_scaled'][-i:] = predictions_list\n",
    "\n",
    "            # the target source, for metrics calculation (first pass)\n",
    "            target = sub_series[m:].copy()\n",
    "\n",
    "            # build source tensors from the sub-series    \n",
    "            encoder_input_tensor = tf.expand_dims(encoder_input, axis=0)\n",
    "            decoder_input_tensor = tf.expand_dims(decoder_input, axis=0)\n",
    "\n",
    "            # make input example for the prediction function\n",
    "            input_example = input_tensors_to_serialized_example(encoder_input_tensor,\n",
    "                                                                decoder_input_tensor)\n",
    "\n",
    "            # get the output of the prediction function as a dictionary\n",
    "            predict_output_dict = predict_fn(input_example)\n",
    "\n",
    "            # get the prediction output tensor\n",
    "            predict_output_tensor = predict_output_dict['forecast']\n",
    "\n",
    "            # get the most recent prediction\n",
    "            most_recent_prediction = predict_output_tensor[0, :, 0].numpy()[-1]\n",
    "\n",
    "            # append the most recent prediction timestep to the predictions list\n",
    "            predictions_list.append(most_recent_prediction)\n",
    "\n",
    "            # pass the predictions list to an array\n",
    "            # current_predictions_array = np.array(predictions_list).reshape(-1, 1)\n",
    "\n",
    "            # get the targets vector to be compared with the current predictions array\n",
    "            # current_targets = np.array(target['kw_scaled'][-i-1:]).reshape(-1, 1)\n",
    "\n",
    "            # calculate SMAPE on the rescaled variable\n",
    "            # rescaled_predictions = scaler_test.inverse_transform(current_predictions_array)\n",
    "            # rescaled_targets = scaler_test.inverse_transform(current_targets)\n",
    "            # current_smape = smape(rescaled_targets, rescaled_predictions)\n",
    "            # print('SMAPE for the first {} rescaled prediction(s) is {}'.format(i + 1, current_smape))\n",
    "        \n",
    "\n",
    "        # iterative predictions over the forecast window reside in predictions_list\n",
    "        # convert list to array, then expand feature dimension with value 1\n",
    "        predicted_values = np.array(predictions_list).reshape(-1, 1)\n",
    "\n",
    "        # inverse-scale predictions\n",
    "        rescaled_predicted_values = scaler_test.inverse_transform(predicted_values)\n",
    "\n",
    "        # and the true values remain in the prediction tensor, pass them to a NumPy array\n",
    "        # for the true values array, expand feature dimension with value 1\n",
    "        true_values = np.array(target['kw_scaled'][-i-1:]).reshape(-1, 1)\n",
    "\n",
    "        # inverse-scale true values\n",
    "        rescaled_true_values = scaler_test.inverse_transform(true_values)\n",
    "\n",
    "        # a temporary dataframe built from the data in the current row\n",
    "        df = pd.DataFrame(columns=pred_df_columns)\n",
    "        df['model_id'] = [model_id]\n",
    "        df['execution'] = [execution]\n",
    "        df['dataset'] = [dataset]\n",
    "        df['inference'] = [inference]\n",
    "        df['string_timestamps']= [pd.to_datetime(target.index[-i-1:]).astype(str).tolist()]\n",
    "        df['predictions'] = [np.squeeze(rescaled_predicted_values).tolist()]\n",
    "        df['targets'] = [np.squeeze(rescaled_true_values).tolist()]\n",
    "        df['mae'] = mean_absolute_error(rescaled_true_values, rescaled_predicted_values)\n",
    "        df['rmse'] = sqrt(mean_squared_error(rescaled_true_values, rescaled_predicted_values))\n",
    "        df['smape'] = smape(rescaled_true_values, rescaled_predicted_values)\n",
    "\n",
    "        # append the temporary dataframe to the predictions detail dataframe\n",
    "        predictions_detail_df = pd.concat([predictions_detail_df, df])\n",
    "\n",
    "    \n",
    "    # reset the index of final dataframe, once all of its rows (dataset) have been processed\n",
    "    predictions_detail_df = predictions_detail_df.reset_index(drop=True)\n",
    "\n",
    "    # build a predictions summary dataframe, reset index to avoid making a multi-column index when grouping by\n",
    "    predictions_summary_df = predictions_detail_df.groupby(['model_id',\n",
    "                                                            'execution',\n",
    "                                                            'dataset',\n",
    "                                                            'inference']).mean().reset_index()\n",
    "\n",
    "    # a range to iterate on prediction timesteps\n",
    "    targets_range = np.arange(forecast_window)\n",
    "\n",
    "    # vector metric (vector component to vector component)\n",
    "    # an array forecast_window-d: metric for 1, 2,..., no_targets step-ahead\n",
    "    # (target versus prediction for rows in dataset)\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    mae_vector = [\n",
    "        mean_absolute_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['mae_vector'] = [mae_vector]\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    rmse_vector = [\n",
    "        sqrt(mean_squared_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        )) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['rmse_vector'] = [rmse_vector]\n",
    "    \n",
    "    # for index, row in dataframe.iterrows()\n",
    "    smape_vector = [\n",
    "        smape(\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['smape_vector'] = [smape_vector]\n",
    "\n",
    "    # insert count of rows as a column value\n",
    "    predictions_summary_df.insert(4, 'count', len(dataset_row_indexes_list))\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_detail/\n",
    "    detail_pickle_path = '{}/{}/{}/{}_{:02d}_{}_{}.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_detail',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset,\n",
    "        inference)\n",
    "    \n",
    "    # persist the Pandas dataframe to database/predictions_detail/\n",
    "    predictions_detail_df.to_pickle(detail_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions detail of {}_{:02d}_{}_{}'.format(model_id,\n",
    "                                                                                        execution,\n",
    "                                                                                        dataset,\n",
    "                                                                                        inference))\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_summary/\n",
    "    summary_pickle_path = '{}/{}/{}/{}_{:02d}_{}_{}.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_summary',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset,\n",
    "        inference)\n",
    "\n",
    "    # persist the Pandas dataframe to database/predictions_summary/\n",
    "    predictions_summary_df.to_pickle(summary_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions summary of {}_{:02d}_{}_{}'.format(model_id,\n",
    "                                                                                         execution,\n",
    "                                                                                         dataset,\n",
    "                                                                                         inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'BSCTRFM_TPU_009'\n",
    "execution = 0\n",
    "dataset = 'test'\n",
    "forecast_window = 24\n",
    "inference = '{:03d}'.format(forecast_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/database/predictions_detail/BSCTRFM_TPU_009_00_test_024.pkl'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a path to access the predictions detail dataframe\n",
    "detail_pickle_path = '{}/{}/{}/{}_{:02d}_{}_{}.pkl'.format(\n",
    "    PROJECT_ROOT,\n",
    "    'database',\n",
    "    'predictions_detail',\n",
    "    model_id,\n",
    "    execution,\n",
    "    dataset,\n",
    "    inference)\n",
    "detail_pickle_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_detail_df = pd.read_pickle(detail_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results for a given row\n",
    "row = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = predictions_detail_df.loc[row]['string_timestamps'][0]\n",
    "end_timestamp = predictions_detail_df.loc[row]['string_timestamps'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the plot with the predictions start timestamp\n",
    "label = start_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"61af1f8e-8c2b-4b80-8b45-438a784d7225\" data-root-id=\"1002\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"61717e4f-35f9-4326-96cf-e33e92da1913\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"DatetimeAxis\"}],\"center\":[{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1022\",\"type\":\"Grid\"},{\"id\":\"1059\",\"type\":\"Legend\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":960,\"renderers\":[{\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"id\":\"1064\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1003\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1029\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1005\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{\"days\":[1,8,15,22]},\"id\":\"1051\",\"type\":\"DaysTicker\"},{\"attributes\":{\"base\":24,\"mantissas\":[1,2,4,6,8,12],\"max_interval\":43200000.0,\"min_interval\":3600000.0,\"num_minor_ticks\":0},\"id\":\"1048\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"dimension\":1,\"grid_line_alpha\":0.5,\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1042\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"base\":60,\"mantissas\":[1,2,5,10,15,20,30],\"max_interval\":1800000.0,\"min_interval\":1000.0,\"num_minor_ticks\":0},\"id\":\"1047\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"axis_label\":\"Active power [KW]\",\"formatter\":{\"id\":\"1042\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{\"label\":{\"value\":\"predicted\"},\"renderers\":[{\"id\":\"1064\",\"type\":\"GlyphRenderer\"}]},\"id\":\"1086\",\"type\":\"LegendItem\"},{\"attributes\":{\"days\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},\"id\":\"1049\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"HelpTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1058\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1094\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":{\"__ndarray__\":\"AACYHA42dkIAAICLETZ2QgAAaPoUNnZCAABQaRg2dkIAADjYGzZ2QgAAIEcfNnZCAAAItiI2dkIAAPAkJjZ2QgAA2JMpNnZCAADAAi02dkIAAKhxMDZ2QgAAkOAzNnZCAAB4Tzc2dkIAAGC+OjZ2QgAASC0+NnZCAAAwnEE2dkIAABgLRTZ2QgAAAHpINnZCAADo6Es2dkIAANBXTzZ2QgAAuMZSNnZCAACgNVY2dkIAAIikWTZ2QgAAcBNdNnZC\",\"dtype\":\"float64\",\"shape\":[24]},\"y\":[2399.04248046875,2159.216796875,1973.577392578125,1875.57080078125,1788.22509765625,1754.3798828125,1770.549560546875,1987.1558837890625,2107.11474609375,2182.489013671875,2335.122802734375,2568.9658203125,2721.298583984375,2861.849853515625,2957.750244140625,2966.237548828125,2943.69677734375,2890.994140625,2861.4423828125,2836.3134765625,2792.591064453125,2889.121337890625,2945.3427734375,2718.617431640625]},\"selected\":{\"id\":\"1095\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1094\",\"type\":\"UnionRenderers\"}},\"id\":\"1061\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"1061\",\"type\":\"ColumnDataSource\"}},\"id\":\"1065\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"days\":[1,4,7,10,13,16,19,22,25,28]},\"id\":\"1050\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"Selection\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"SaveTool\"},{\"id\":\"1027\",\"type\":\"ResetTool\"},{\"id\":\"1028\",\"type\":\"HelpTool\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1095\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"1061\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1062\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1063\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"1065\",\"type\":\"CDSView\"}},\"id\":\"1064\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1038\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{\"months\":[0,2,4,6,8,10]},\"id\":\"1054\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"ResetTool\"},{\"attributes\":{\"label\":{\"value\":\"real\"},\"renderers\":[{\"id\":\"1039\",\"type\":\"GlyphRenderer\"}]},\"id\":\"1060\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"mantissas\":[1,2,5],\"max_interval\":500.0,\"num_minor_ticks\":0},\"id\":\"1046\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"months\":[0,1,2,3,4,5,6,7,8,9,10,11]},\"id\":\"1053\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"months\":[0,4,8]},\"id\":\"1055\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"text\":\"24-step-ahead active power predictions starting on 2018-05-14 23:00:00 with MAE=109.123205, RMSE=146.486437, SMAPE=0.038758\"},\"id\":\"1003\",\"type\":\"Title\"},{\"attributes\":{\"days\":[1,15]},\"id\":\"1052\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1037\",\"type\":\"Line\"},{\"attributes\":{\"items\":[{\"id\":\"1060\",\"type\":\"LegendItem\"},{\"id\":\"1086\",\"type\":\"LegendItem\"}],\"location\":\"bottom_left\"},\"id\":\"1059\",\"type\":\"Legend\"},{\"attributes\":{\"callback\":null},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{\"months\":[0,6]},\"id\":\"1056\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1058\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1044\",\"type\":\"DatetimeTickFormatter\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1057\",\"type\":\"YearsTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1037\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1038\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\",\"type\":\"CDSView\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":{\"__ndarray__\":\"AACYHA42dkIAAICLETZ2QgAAaPoUNnZCAABQaRg2dkIAADjYGzZ2QgAAIEcfNnZCAAAItiI2dkIAAPAkJjZ2QgAA2JMpNnZCAADAAi02dkIAAKhxMDZ2QgAAkOAzNnZCAAB4Tzc2dkIAAGC+OjZ2QgAASC0+NnZCAAAwnEE2dkIAABgLRTZ2QgAAAHpINnZCAADo6Es2dkIAANBXTzZ2QgAAuMZSNnZCAACgNVY2dkIAAIikWTZ2QgAAcBNdNnZC\",\"dtype\":\"float64\",\"shape\":[24]},\"y\":[2451.085,2157.115,1955.9133333333332,1843.0166666666667,1821.4583333333333,1782.45,1804.0933333333335,1990.0666666666668,2105.6749999999997,2245.0483333333336,2407.685,2621.2916666666665,2783.2816666666663,2943.241666666667,3107.74,3164.7966666666666,3190.125,3182.22,3194.318333333333,3050.028333333333,3011.1266666666666,3110.056666666667,3016.4833333333336,2859.786666666667]},\"selected\":{\"id\":\"1072\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1071\",\"type\":\"UnionRenderers\"}},\"id\":\"1036\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1063\",\"type\":\"Line\"},{\"attributes\":{\"grid_line_alpha\":0.5,\"ticker\":{\"id\":\"1014\",\"type\":\"DatetimeTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{\"line_color\":\"red\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1062\",\"type\":\"Line\"},{\"attributes\":{\"num_minor_ticks\":5,\"tickers\":[{\"id\":\"1046\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1047\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1048\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1049\",\"type\":\"DaysTicker\"},{\"id\":\"1050\",\"type\":\"DaysTicker\"},{\"id\":\"1051\",\"type\":\"DaysTicker\"},{\"id\":\"1052\",\"type\":\"DaysTicker\"},{\"id\":\"1053\",\"type\":\"MonthsTicker\"},{\"id\":\"1054\",\"type\":\"MonthsTicker\"},{\"id\":\"1055\",\"type\":\"MonthsTicker\"},{\"id\":\"1056\",\"type\":\"MonthsTicker\"},{\"id\":\"1057\",\"type\":\"YearsTicker\"}]},\"id\":\"1014\",\"type\":\"DatetimeTicker\"},{\"attributes\":{\"axis_label\":\"Timestamp\",\"formatter\":{\"id\":\"1044\",\"type\":\"DatetimeTickFormatter\"},\"ticker\":{\"id\":\"1014\",\"type\":\"DatetimeTicker\"}},\"id\":\"1013\",\"type\":\"DatetimeAxis\"},{\"attributes\":{\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"}},\"id\":\"1040\",\"type\":\"CDSView\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
       "  var render_items = [{\"docid\":\"61717e4f-35f9-4326-96cf-e33e92da1913\",\"roots\":{\"1002\":\"61af1f8e-8c2b-4b80-8b45-438a784d7225\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots[label] = figure(\n",
    "    x_axis_type='datetime',\n",
    "    plot_width=960,\n",
    "    plot_height=400,\n",
    "    title='{}-step-ahead active power predictions starting on {} with MAE={}, RMSE={}, SMAPE={}'\\\n",
    "    .format(forecast_window,\n",
    "            label,\n",
    "            round(predictions_detail_df.loc[row]['mae'], 6),\n",
    "            round(predictions_detail_df.loc[row]['rmse'], 6),\n",
    "            round(predictions_detail_df.loc[row]['smape'], 6)\n",
    "           )\n",
    ")\n",
    "\n",
    "plots[label].grid.grid_line_alpha=0.5\n",
    "\n",
    "plots[label].xaxis.axis_label = 'Timestamp'\n",
    "plots[label].yaxis.axis_label = 'Active power [KW]'\n",
    "\n",
    "plots[label].line(\n",
    "    pd.to_datetime(predictions_detail_df.loc[row]['string_timestamps']),\n",
    "    predictions_detail_df.loc[row]['targets'],\n",
    "    color='green',\n",
    "    legend_label='real')\n",
    "\n",
    "plots[label].line(\n",
    "    pd.to_datetime(predictions_detail_df.loc[row]['string_timestamps']),\n",
    "    predictions_detail_df.loc[row]['predictions'],\n",
    "    color='red',\n",
    "    legend_label='predicted')\n",
    "\n",
    "plots[label].legend.location = 'bottom_left'\n",
    "\n",
    "# uncomment the following two lines to save plot\n",
    "# output_file('/home/developer/gcp/cbidmltsf/datasets/cfe/{}_H_kw.html'.format(device))\n",
    "# save(plots[label])\n",
    "\n",
    "# uncomment the following line to display plot\n",
    "show(plots[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
