{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: change core library for compat v1\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BUCKET_NAME = 'cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.get_bucket(_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '007_CPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/007_CPU/export/exporter'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models path has changed from dplstm to models\n",
    "saved_model_path = 'models/{0}/export/exporter'.format(model_dir)\n",
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hackersandslackers.com/manage-files-in-google-cloud-storage-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(bucketFolder):\n",
    "    \"\"\"List all files in GCP bucket.\"\"\"\n",
    "    files = bucket.list_blobs(prefix=bucketFolder)\n",
    "    fileList = [file.name for file in files if '.' in file.name]\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list_files(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/007_CPU/export/exporter/1594219862/saved_model.pb',\n",
       " 'models/007_CPU/export/exporter/1594219862/variables/variables.data-00000-of-00001',\n",
       " 'models/007_CPU/export/exporter/1594219862/variables/variables.index']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the names of the subdirectories in export/exporter (one for each training process)\n",
    "prefix_length = len(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_saved_model_id = sorted(list(set([file[prefix_length+1:prefix_length+11] for file in all_files])))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/models/007_CPU/export/exporter/1594219862'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LATEST_SAVED_MODEL_DIR = 'gs://{0}/{1}/{2}'.format(_BUCKET_NAME,\n",
    "                                                    saved_model_path,\n",
    "                                                    latest_saved_model_id)\n",
    "_LATEST_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from gs://cbidmltsf/models/007_CPU/export/exporter/1594219862/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# build a prediction function\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(_LATEST_SAVED_MODEL_DIR)\n",
    "# following the warning that results from executing the previous line\n",
    "# predict_fn = tf.compat.v1.saved_model.load(_LATEST_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SLDB_ID = 'CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D007024001_W004168001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-comment the following line to predict on test set\n",
    "# test_dataset_filename = 'gs://cbidmltsf/sldbs/{0}/test.tfrecord'.format(_SLDB_ID)\n",
    "# use evaluation dataset as testing dataset\n",
    "test_dataset_filename = 'gs://cbidmltsf/sldbs/{0}/test.tfrecord'.format(_SLDB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/sldbs/CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D007024001_W004168001/test.tfrecord'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(test_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV1 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test_dataset.map(lambda row: row)\n",
    "# ToDo: find out if there is a faster way to build the predictions list,\n",
    "#   with no iterator on the dataset\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xcf\\x02\\nm\\n\\x05oh_dh\\x12d\\x12b\\n`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x12\\n\\x06target\\x12\\x08\\x12\\x06\\n\\x04\\xce\\xfb\\xc5>\\n.\\n\\x06hourly\\x12$\\x12\"\\n \\xa7r\\xb7>\\xfck\\xdd>\\xc3\\xb8\\xe0>(\\xa4\\x03?_\\xfe\\x1b?\\x04\\xab#?QF\\xf7>9\\r\\x06?\\n\\x1e\\n\\x06weekly\\x12\\x14\\x12\\x12\\n\\x10\\xb6\\xfe\\xcd>\\xa1\\xa9\\x19?-y\\x9e>g\\x8e\\xa9>\\n)\\n\\x05oh_wd\\x12 \\x12\\x1e\\n\\x1c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n)\\n\\x05daily\\x12 \\x12\\x1e\\n\\x1cg\\x8e\\xa9>\\x7f\\xd4\\xc7>\\x10\\xb3\\x14?J\\xfc\\x0c?\\x81\\'\\x1b?)\\xbd\\x17?\\xc8\\x81\\x0e?\\n$\\n\\ttimestamp\\x12\\x17\\n\\x15\\n\\x132018-02-23 14:00:00'\n"
     ]
    }
   ],
   "source": [
    "# ToDo: load a single serialized example to test the prediction funcion on\n",
    "value = sess.run(next_element)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            example = sess.run(next_element)\n",
    "            predictions_list.append(predict_fn({'example_bytes': example}))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D007024001_W004168001'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_SLDB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'008'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 58\n",
    "_SLDB_ID[offset:offset+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files(bucketName):\n",
    "    \"\"\"Upload files to GCP bucket.\"\"\"\n",
    "    files = [f for f in listdir(localFolder) if isfile(join(localFolder, f))]\n",
    "    for file in files:\n",
    "        localFile = localFolder + file\n",
    "        blob = bucket.blob(bucketFolder + file)\n",
    "        blob.upload_from_filename(localFile)\n",
    "    return f'Uploaded {files} to \"{bucketName}\" bucket.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload a file to the model_dir in GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.storage.client.Client at 0x7f38641cd748>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Bucket: cbidmltsf>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Bucket: cbidmltsf>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = 'predictions.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dplstm/086403_073202_043201_CPU_07_00/predictions.html'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_blob_id = 'dplstm/{0}/{1}'.format(model_dir, local_file)\n",
    "plot_blob_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blob = bucket.blob(plot_blob_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sldb.json', 'r') as filename:\n",
    "    sldb = json.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_shapes = {\n",
    "    'hourly': [sldb['components']['hourly']['m'], 1],\n",
    "    'daily': [sldb['components']['daily']['m'], 1],\n",
    "    'weekly': [sldb['components']['weekly']['m'], 1],\n",
    "    'target': [sldb['components']['hourly']['no_targets'], 1],\n",
    "    'oh_wd': [7, 1],  # Monday to Sunday\n",
    "    'oh_dh': [24, 1],  # midnight to 23:00\n",
    "    'timestamp': [sldb['components']['hourly']['no_targets'], 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not pass objective shapes as a parameter, get them from an outer scope variable instead\n",
    "def _parse_dataset_function(example_proto):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shapes['daily'])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shapes['weekly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "\n",
    "    # important: this is a different parse function from the one in training\n",
    "    # it returns target as a feature to use target values and timestamps for plotting\n",
    "    return {'hourly': hourly,\n",
    "            'daily': daily,\n",
    "            'weekly': weekly,\n",
    "            'target': target,\n",
    "            'oh_wd': oh_wd,\n",
    "            'oh_dh': oh_dh,\n",
    "            'timestamp': timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test_dataset.map(lambda row: _parse_dataset_function(row))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_list = []\n",
    "targets_list = []\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            item = sess.run(next_element)\n",
    "            # get the timestamp, then the scalar value, then the string value, then convert it to datetime\n",
    "            timestamps_list.append(to_datetime(item['timestamp'][0][0].decode()))\n",
    "            # get the target, then the scalar value\n",
    "            targets_list.append(item['target'][0][0])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timestamps_list), len(targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
