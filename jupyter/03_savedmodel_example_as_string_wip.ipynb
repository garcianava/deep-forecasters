{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a temporary workaround\n",
    "# pass this code to the setup.py file of the final module\n",
    "import sys\n",
    "_ROOT_DIR = '{0}/gcp/cbidmltsf'.format(os.getenv(\"HOME\"))\n",
    "sys.path.append(_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dplstm.data import make_input_fn\n",
    "from dplstm.model import DPLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build parameters dictionary for interactive execution\n",
    "# in batch execution, this dictionary comes from parsed cli arguments\n",
    "\n",
    "parameters = {\n",
    "    'model_dir': 'lstm_48',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 20,  # NOT REQUIRED IN DISTRIBUTED MODE, IT IS OVERRIDDEN BY MAX_TRAIN_STEPS\n",
    "    'max_train_steps': 100,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_interval': 300,\n",
    "    'keep_checkpoint_max': 3,\n",
    "    'eval_steps': None,\n",
    "    'eval_batch_size': 2**16,\n",
    "    'start_delay_secs': 600,\n",
    "    'throttle_secs': 600,\n",
    "    'train_data_path': '{0}/data/tfrecord/train.tfrecord'.format(_ROOT_DIR),\n",
    "    'eval_data_path': '{0}/data/tfrecord/val.tfrecord'.format(_ROOT_DIR),\n",
    "    'test_data_path': '{0}/data/tfrecord/test.tfrecord'.format(_ROOT_DIR),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for log files\n",
    "import logging\n",
    "\n",
    "# advanced numerical operations\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))\n",
    "\n",
    "# ToDo: pass a complex dictionary as a starting point for the LSTM network chromosome\n",
    "'''\n",
    "    Deep/Parallel LSTM network chromosome includes:\n",
    "    m_hour: [24, 12, 8, etc], tau=1, implement this variable later and work now with m_hour=24\n",
    "    m_day: [7, 14, etc], tau=7, implement this variable later and work now with m_day=7\n",
    "    m_week: [4, 8, 12, etc], tau=168, implement this variable later and work now with m_week=4\n",
    "    hidden_hour:\n",
    "    hidden_day:\n",
    "    hidden_week:\n",
    "    levels_hour:\n",
    "    levels_day:\n",
    "    levels_week:\n",
    "'''\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove main function and transfer everything to first-level code cells\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_forecaster(features, labels, mode):\n",
    "    # instantiate network topology from the corresponding class\n",
    "    forecaster_topology = DPLSTM()\n",
    "\n",
    "    # ToDo: global_step might be moved to TRAIN scope\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # call operator to forecaster_topology, over features\n",
    "    forecast = forecaster_topology(features)\n",
    "\n",
    "    # predictions are stored in a dictionary for further use at inference stage\n",
    "    predictions = {\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "\n",
    "    # CHANGE MODEL FUNCTION STRUCTURE ACCORDING TO GILLARD'S ARCHITECTURE\n",
    "\n",
    "    # Estimator in TRAIN or EVAL mode\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # use labels and predictions to define training loss and evaluation loss\n",
    "        # generate summaries for TensorBoard\n",
    "        with tf.name_scope('loss'):\n",
    "            mean_squared_error = tf.losses.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, scope='loss')\n",
    "            tf.summary.scalar('loss', mean_squared_error)\n",
    "\n",
    "        with tf.name_scope('val_loss'):\n",
    "            val_loss = tf.metrics.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, name='mse')\n",
    "            tf.summary.scalar('val_loss', val_loss[1])\n",
    "\n",
    "        # this is only required for PREDICT mode\n",
    "        prediction_hooks = None\n",
    "\n",
    "        # Estimator in TRAIN mode ONLY\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss=mean_squared_error,\n",
    "                    global_step=global_step,\n",
    "                    learning_rate=parameters['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "            # Create a hook to print acc, loss & global step every 100 iter\n",
    "            train_hook_list = []\n",
    "            train_tensors_log = {'val_loss': val_loss[1],\n",
    "                                 'loss': mean_squared_error,\n",
    "                                 'global_step': global_step}\n",
    "            train_hook_list.append(tf.train.LoggingTensorHook(\n",
    "                tensors=train_tensors_log, every_n_iter=100))\n",
    "\n",
    "            predictions = None  # this is not required in TRAIN mode\n",
    "            loss = mean_squared_error\n",
    "            eval_metric_ops = None\n",
    "            training_hooks = train_hook_list\n",
    "            evaluation_hooks = None\n",
    "\n",
    "        else:  # Estimator in EVAL mode ONLY\n",
    "            loss = mean_squared_error\n",
    "            train_op = None\n",
    "            training_hooks = None\n",
    "            eval_metric_ops = {'val_loss': val_loss}\n",
    "            evaluation_hooks = None\n",
    "\n",
    "    # Estimator in PREDICT mode ONLY\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    "        training_hooks = None\n",
    "        evaluation_hooks = None\n",
    "        prediction_hooks = None  # this might change as we are in PREDICT mode\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        # export_outputs=not_used_yet (for TensorFlow Serving, redirected from predictions if omitted)\n",
    "        # training_chief_hooks=not_used_yet\n",
    "        # ToDo: verify use of training_hooks\n",
    "        # temporarily disable training hooks\n",
    "        # training_hooks=training_hooks,\n",
    "        training_hooks=training_hooks,\n",
    "        # scaffold=not_used_yet\n",
    "        evaluation_hooks=evaluation_hooks,\n",
    "        prediction_hooks=prediction_hooks\n",
    "    )\n",
    "\n",
    "# ToDo: evaluate the convenience of packaging execution in a tf.app\n",
    "# in the meantime, run the estimator outside tf.app package\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure file writer cache is clear for TensorBoard events file\n",
    "tf.summary.FileWriterCache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters required for RunConfig()\n",
    "# _EVAL_INTERVAL = 300  # how often checkpoints are written out, given in seconds\n",
    "# _KEEP_CHECKPOINT_MAX = 3  # how many checkpoints to keep\n",
    "# _MAX_TRAIN_STEPS = 16000\n",
    "# _TRAIN_BATCH_SIZE = 2**5\n",
    "# _EVAL_STEPS = None  # if None, evaluate on entire dataset\n",
    "# _EVAL_BATCH_SIZE = 2**16\n",
    "# _START_DELAY_SECONDS = 600\n",
    "# _THROTTLE_SECONDS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_master': '', '_keep_checkpoint_max': 3, '_session_creation_timeout_secs': 7200, '_service': None, '_global_id_in_cluster': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fef6640a278>, '_num_worker_replicas': 1, '_save_summary_steps': 100, '_tf_random_seed': None, '_save_checkpoints_secs': 300, '_keep_checkpoint_every_n_hours': 10000, '_experimental_distribute': None, '_task_id': 0, '_protocol': None, '_train_distribute': None, '_num_ps_replicas': 0, '_experimental_max_worker_delay_secs': None, '_is_chief': True, '_task_type': 'worker', '_device_fn': None, '_model_dir': '/home/jupyter/gcp/cbidmltsf/dplstm/lstm_48', '_evaluation_master': '', '_save_checkpoints_steps': None, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# instantiate base estimator class for custom model function\n",
    "tsf_estimator = tf.estimator.Estimator(\n",
    "    model_fn=time_series_forecaster,\n",
    "    # no parameters passed at this point\n",
    "    # params=hparams,\n",
    "    # parameters passed in original model include: train_data_path, batch_size, augment, train_steps\n",
    "    config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=parameters['eval_interval'],\n",
    "        keep_checkpoint_max=parameters['keep_checkpoint_max']\n",
    "    ),\n",
    "    model_dir='{0}/dplstm/{1}'.format(_ROOT_DIR, parameters['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's train_spec to use train_input_fn and train for so many steps\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['train_data_path'],\n",
    "        batch_size=parameters['train_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.TRAIN\n",
    "    ),\n",
    "    max_steps=parameters['max_train_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: activate TensorFlow Serving for distributed inference\n",
    "# Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "# exporter = tf.estimator.LatestExporter(\n",
    "#     name=\"exporter\",\n",
    "#     serving_input_receiver_fn=serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT IS IMPORTANT TO FEED THE SERVING INPUT FUNCTION WITH A PROTOCOL BUFFER EXAMPLE, TO SPEED UP LOADING THE DATA\n",
    "# LET'S TRY THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create exporter that uses serving_input_fn to create saved_model for serving\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "    name = \"exporter\", \n",
    "    serving_input_receiver_fn = serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's eval_spec to use eval_input_fn and export saved_model\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['eval_data_path'],\n",
    "        batch_size=parameters['eval_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.EVAL\n",
    "    ),\n",
    "    steps=parameters['eval_steps'],  # use None to evaluate on the entire dataset\n",
    "    exporters=exporter,\n",
    "    start_delay_secs=parameters['start_delay_secs'],  # delay first evaluation\n",
    "    throttle_secs=parameters['throttle_secs']  # evaluate at a different rate (usually longer) than checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.028384779, step = 1\n",
      "INFO:tensorflow:global_step = 1, loss = 0.028384779, val_loss = 0.028384779\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T18:42:22Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-18:42:25\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 0.0067395815, val_loss = 0.0067395815\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/model.ckpt-100\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/model.ckpt-100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/export/exporter/temp-b'1576089745'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.007965051.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 100, 'loss': 0.0067395815, 'val_loss': 0.0067395815},\n",
       " [b'/home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/export/exporter/1576089745'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run train_and_evaluate loop\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=tsf_estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the directory of the latest saved model\n",
    "_SAVED_MODEL_DIR = '{0}/dplstm/{1}/export/exporter'.format(_ROOT_DIR, parameters['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = [x for x in Path(_SAVED_MODEL_DIR).iterdir()\n",
    "           if x.is_dir() and 'temp' not in str(x)]\n",
    "_LATEST_SAVED_MODEL_DIR = str(sorted(subdirs)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/export/exporter/1576089745'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LATEST_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_48/export/exporter/1576089745/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# build a prediction function\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(_LATEST_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test with a serialized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xcf\\x01\\n\\x1e\\n\\x06weekly\\x12\\x14\\x12\\x12\\n\\x10\\x15(\\xda>\\xd2y\\xf4>\\x1b\\x11\\x03?7\\xa7\\x0b?\\n\\x12\\n\\x06target\\x12\\x08\\x12\\x06\\n\\x04W\\xcb\\xfc>\\n)\\n\\x05daily\\x12 \\x12\\x1e\\n\\x1c7\\xa7\\x0b?\\x85\\x9a\\x0c?\\x81\\xf5\\x05?sO\\xd6>\\x9d\\xaa\\xcd>\\x81y\\xc6>\\x01\\x92\\x15?\\nn\\n\\x06hourly\\x12d\\x12b\\n`\\x01\\x92\\x15?\\xcbo\\x1f?\\xe4A/?\\xc4\\xcc(?\\xe1\\x961?\\xce\\x00=?\\xee\\xbcJ?d\\xc1L?\\xea\\xe2+?\\xff+\"?D\\x93\\xed>G\\xc4\\x98>\\xc4d\\xa5>\\xbc\\xd4\\x85>\\\\Og>\\x80\\xf9\\x8f>\\x9d\\x8ep>\\x9f\\xeb\\x8d>I\\xc2\\x88>[b^>\\xb8\\xf8)>N\\xff\\xbb>6\\xb0\\xd4>\\x91H\\xf0>'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forecast': array([[0.46904117]], dtype=float32)}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fn({'example_bytes': single_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soy la ostia :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test the prediction function over an artificial dictionary with np arrays as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with a custom-generated dictionary filled with NumPy arrays\n",
    "fake_parsed_row = {\n",
    "    'hourly': np.zeros([1, 24]),\n",
    "    'daily': np.zeros([1, 7]),\n",
    "    'weekly': np.zeros([1, 4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forecast': array([[0.11004811]], dtype=float32)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in its current state, saved model is able to predict when is fed with NumPy arrays\n",
    "predict_fn(fake_parsed_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why did it worked? why the serving input function serves a tensor that is compatible with the original model?\n",
    "fake_parsed_row['hourly'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_8:0' shape=(1, 24, 1) dtype=float64>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the shape required by the model\n",
    "tf.expand_dims(fake_parsed_row['hourly'], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(24, 1) dtype=float32>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and this is the shape served by the current parsing function\n",
    "features['hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_9:0' shape=(1, 24, 1) dtype=float32>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it have to be changed like this\n",
    "tf.expand_dims(features['hourly'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ORGANIZE ALL SHAPES TO AVOID UNNECESSARY RE-SHAPING OPERATIONS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to define a serving input function which requires example protocol buffers as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/data/tfrecord/test.tfrecord'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now locate TFRecord test dataset and load it, need to parse the examples to a dictionary\n",
    "test_dataset_filename = '{0}/data/tfrecord/test.tfrecord'.format(_ROOT_DIR)\n",
    "test_dataset_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV1 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now read the dataset from TFRecord file using non-deprecated methods from tf.data module\n",
    "test_raw_dataset = tf.data.TFRecordDataset(test_dataset_filename)\n",
    "test_raw_dataset        tf.squeeze(feature_placeholders['examples']))\n",
    "    features = {\n",
    "      'image': tf.expand_dims(image, 0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.string"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can I access to the binary, string-based, raw dataset using a one-shot iterator?\n",
    "iterator = test_raw_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# there is only one row in the raw dataset\n",
    "single_example = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xcf\\x01\\n\\x1e\\n\\x06weekly\\x12\\x14\\x12\\x12\\n\\x10\\x15(\\xda>\\xd2y\\xf4>\\x1b\\x11\\x03?7\\xa7\\x0b?\\n\\x12\\n\\x06target\\x12\\x08\\x12\\x06\\n\\x04W\\xcb\\xfc>\\n)\\n\\x05daily\\x12 \\x12\\x1e\\n\\x1c7\\xa7\\x0b?\\x85\\x9a\\x0c?\\x81\\xf5\\x05?sO\\xd6>\\x9d\\xaa\\xcd>\\x81y\\xc6>\\x01\\x92\\x15?\\nn\\n\\x06hourly\\x12d\\x12b\\n`\\x01\\x92\\x15?\\xcbo\\x1f?\\xe4A/?\\xc4\\xcc(?\\xe1\\x961?\\xce\\x00=?\\xee\\xbcJ?d\\xc1L?\\xea\\xe2+?\\xff+\"?D\\x93\\xed>G\\xc4\\x98>\\xc4d\\xa5>\\xbc\\xd4\\x85>\\\\Og>\\x80\\xf9\\x8f>\\x9d\\x8ep>\\x9f\\xeb\\x8d>I\\xc2\\x88>[b^>\\xb8\\xf8)>N\\xff\\xbb>6\\xb0\\xd4>\\x91H\\xf0>'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features dictionary for parsing TFrecord files\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.FixedLenFeature([], dtype=tf.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dataset_function(example_proto, objective_shape):\n",
    "    # parse the input tf.Example proto using the dictionary called read_features (above defined)\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shape[0])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shape[1])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shape[2])\n",
    "    target = tf.reshape(row['target'], [1, ])\n",
    "    return {'hourly': hourly, 'daily': daily, 'weekly': weekly}, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify objective shapes for serving, then avoid reshaping tensors while serving\n",
    "_SERVING_OBJECTIVE_SHAPES = [[1, 24, 1], [1, 7, 1], [1, 4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'daily': <tf.Tensor 'Reshape_11:0' shape=(1, 7, 1) dtype=float32>,\n",
       "  'hourly': <tf.Tensor 'Reshape_10:0' shape=(1, 24, 1) dtype=float32>,\n",
       "  'weekly': <tf.Tensor 'Reshape_12:0' shape=(1, 4, 1) dtype=float32>},\n",
       " <tf.Tensor 'Reshape_13:0' shape=(1,) dtype=float32>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_dataset_function(single_example, _SERVING_OBJECTIVE_SHAPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the serving input function does not require the label, so\n",
    "features, _ = _parse_dataset_function(single_example, _SERVING_OBJECTIVE_SHAPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': <tf.Tensor 'Reshape_19:0' shape=(1, 7, 1) dtype=float32>,\n",
       " 'hourly': <tf.Tensor 'Reshape_18:0' shape=(1, 24, 1) dtype=float32>,\n",
       " 'weekly': <tf.Tensor 'Reshape_20:0' shape=(1, 4, 1) dtype=float32>}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not required anymore\n",
    "# for value in features.values():\n",
    "#    print(tf.expand_dims(value, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to create a new serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    # it handles only one example at a time\n",
    "    # TPU are not optimized for serving, so it is assumed the predictions server is CPU or GPU-based\n",
    "    # inputs is equivalent to example protocol buffers\n",
    "    feature_placeholders = {'example_bytes': tf.placeholder(tf.string, shape=())}\n",
    "    _SERVING_OBJECTIVE_SHAPES = [[1, 24, 1], [1, 7, 1], [1, 4, 1]]\n",
    "    # the serving input function does not require the label\n",
    "    features, _ = _parse_dataset_function(feature_placeholders['example_bytes'], _SERVING_OBJECTIVE_SHAPES)\n",
    "    # re-shape to original model spec\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict based on checkpoints and plot results with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anaconda Interactive Visualization\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import output_file, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# persistence for the scaler located in $DATA/scalers\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# reload scaler fitted model here\n",
    "scaler = joblib.load('{0}/data/scalers/ci_LSTM_scaler.save'.format(_ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# build predictions using the trained model\n",
    "# ToDo: use TensorFlow Serving instead of the custom estimator\n",
    "# pred = list(tsf_estimator.predict(input_fn=test_input_fn))\n",
    "pred = list(\n",
    "    tsf_estimator.predict(\n",
    "        input_fn=make_input_fn(\n",
    "            tfrecord_path=parameters['test_data_path'],\n",
    "            batch_size=32,\n",
    "            mode=tf.estimator.ModeKeys.PREDICT\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'forecast': array([0.4926325], dtype=float32)},\n",
       " {'forecast': array([0.54299], dtype=float32)},\n",
       " {'forecast': array([0.5770303], dtype=float32)},\n",
       " {'forecast': array([0.58423465], dtype=float32)},\n",
       " {'forecast': array([0.6234549], dtype=float32)},\n",
       " {'forecast': array([0.6371721], dtype=float32)},\n",
       " {'forecast': array([0.6415772], dtype=float32)},\n",
       " {'forecast': array([0.6419458], dtype=float32)},\n",
       " {'forecast': array([0.6172749], dtype=float32)},\n",
       " {'forecast': array([0.56820893], dtype=float32)}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at a predictions subset\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to array and re-scale\n",
    "pred = [p['forecast'][0] for p in pred]\n",
    "pred = np.asarray(pred)\n",
    "pred_ci = scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "pred_ci = np.squeeze(pred_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.056008 , 4.470618 , 4.7508826, 4.810199 , 5.1331124, 5.2460504,\n",
       "       5.2823186, 5.2853537, 5.0822296, 4.6782537], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ci[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: get ytarget_test array from test.tfrecord dataset to perform the following operation\n",
    "# temporarily get the array from disk\n",
    "y_test = np.load('{0}/data/arrays/y_test.npy'.format(_ROOT_DIR))\n",
    "n = 0  # first step ahead\n",
    "ytarget_test = y_test[:, n]\n",
    "ytarget_test = ytarget_test.reshape(ytarget_test.shape[0], 1)\n",
    "\n",
    "actual_ci = scaler.inverse_transform(ytarget_test)\n",
    "actual_ci = np.squeeze(actual_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EQUIPMENT = 'CPE04105'fake_parsed_row['hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/plots/lstm_46.html'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_predictions_fig = figure(title='Predicted Current Imbalance for ' + _EQUIPMENT,\n",
    "                            background_fill_color='#E8DDCB',\n",
    "                            plot_width=1800, plot_height=450, x_axis_type='datetime')\n",
    "\n",
    "# ToDo: yts_test array is required to get timestamps for plot, then wire it now and get it from TFRecord later...\n",
    "yts_test = np.load('{0}/data/arrays/yts_test.npy'.format(_ROOT_DIR), allow_pickle=True)\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        actual_ci, line_color='red',\n",
    "                        line_width=1, alpha=0.7, legend='Actual')\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        pred_ci, line_color='blue',\n",
    "                        line_width=1, alpha=0.7, legend='Predicted')\n",
    "\n",
    "ci_predictions_fig.legend.location = \"top_right\"\n",
    "ci_predictions_fig.legend.background_fill_color = \"darkgrey\"\n",
    "\n",
    "ci_predictions_fig.xaxis.axis_label = 'Timestamp'\n",
    "ci_predictions_fig.yaxis.axis_label = 'Current Imbalance [%]'\n",
    "\n",
    "# output_file('{0}/plots/'.format(_ROOT_DIR) + '{:03d}'.format(n) + '.html', title=_EQUIPMENT)\n",
    "output_file('{0}/plots/'.format(_ROOT_DIR) + '{}'.format(parameters['model_dir']) + '.html', title=_EQUIPMENT)\n",
    "save(ci_predictions_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here, just open the test dataset and use it for prediction\n",
    "# also, verify the cli tool for predictions, as described in Zhang, 2019\n",
    "# saved_model_cli show --dir export/1524906774 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n$ saved_model_cli show --dir 1575917715 --tag_set serve --signature_def serving_default\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['daily'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 7)\\n      name: Placeholder_1:0\\n  inputs['hourly'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 24)\\n      name: Placeholder:0\\n  inputs['weekly'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 4)\\n      name: Placeholder_2:0\\nThe given SavedModel SignatureDef contains the following output(s):\\n  outputs['forecast'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 1)\\n      name: output_4_1/Sigmoid:0\\nMethod name is: tensorflow/serving/predict\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the saved model, on the terminal\n",
    "'''\n",
    "$ saved_model_cli show --dir 1575917715 --tag_set serve --signature_def serving_default\n",
    "The given SavedModel SignatureDef contains the following input(s):\n",
    "  inputs['daily'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 7)\n",
    "      name: Placeholder_1:0\n",
    "  inputs['hourly'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 24)\n",
    "      name: Placeholder:0\n",
    "  inputs['weekly'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 4)\n",
    "      name: Placeholder_2:0\n",
    "The given SavedModel SignatureDef contains the following output(s):\n",
    "  outputs['forecast'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: output_4_1/Sigmoid:0\n",
    "Method name is: tensorflow/serving/predict\n",
    "'''\n",
    "\n",
    "# It works! Now it's time to test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
