{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code to build SLDBs for the DMSLSTM architecture.\n",
    "## It superseedes the make_sldb.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the variable 'labels' with 'targets', as the latter is more adequate for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale datasets to improve neural networks performance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the time series directory\n",
    "# scaler.save\n",
    "# ts.json\n",
    "# ts.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the SLDB directory:\n",
    "# train.tfrecord\n",
    "# eval.tfrecord\n",
    "# test.tfrecord\n",
    "# sldb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to configure the SLDB\n",
    "# ToDo: transfer this dictionary to dplstm/configs/sldb_config.py\n",
    "\n",
    "# modify the dictionary structure:\n",
    "# no_targets must be the same for all components, then move it to an upper level\n",
    "# remove components and use the same structure as in architecture_parameters\n",
    "\n",
    "# ToDo: build all sldb dictionaries on the basis of list-type parameters,\n",
    "#  by iterating on them to avoid comments on the non-used resolutions, like\n",
    "#  m = [8, 8, 8], tau = [1, 24, 168], no_targets = [24] or\n",
    "#  m = [256], tau = [1], no_targets = [24]\n",
    "sldb = {\n",
    "    'ts': 'CPE04115_H_kw_20201021084001',\n",
    "    'embedding': {\n",
    "        'hourly': 256,\n",
    "        # 'daily': 8,\n",
    "        # 'weekly': 8\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1,\n",
    "        # 'daily': 24,\n",
    "        # 'weekly': 168        \n",
    "    },\n",
    "    'no_targets': 24\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series was built and persisted in a different code\n",
    "# SLDB constructions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required time series\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(sldb['ts'])\n",
    "pickle_filename = '{}/ts.pkl'.format(time_series_folder)\n",
    "ts_df = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation stage is not used for TPU-based training,\n",
    "# however, evaluation dataset might be useful to get stats from CPU-based training\n",
    "stages = ['train', 'eval', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into train/eval/test at time series level\n",
    "# to avoid data overlapping at SLDB level\n",
    "split = np.array([0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of the scaled time series for train, validation, and test thresholds\n",
    "train_eval_limit = np.int(ts_df.count()*split[0])\n",
    "eval_test_limit = np.int(ts_df.count()*split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage the time series for the different model stages\n",
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18103 lectures in train time series from 2016-01-01 00:00:00 to 2018-01-24 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for train set\n",
    "ts['train'] = ts_df[:train_eval_limit]\n",
    "print('{0} lectures in train time series from {1} to {2}'.format(ts['train'].count()[0],\n",
    "                                                                 ts['train'].index[0],\n",
    "                                                                 ts['train'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in eval time series from 2018-01-24 09:00:00 to 2018-04-28 16:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for eval set\n",
    "ts['eval'] = ts_df[train_eval_limit:eval_test_limit]\n",
    "print('{0} lectures in eval time series from {1} to {2}'.format(ts['eval'].count()[0],\n",
    "                                                                ts['eval'].index[0],\n",
    "                                                                ts['eval'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in test time series from 2018-04-28 17:00:00 to 2018-07-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for test set\n",
    "ts['test'] = ts_df[eval_test_limit:]\n",
    "print('{} lectures in test time series from {} to {}'.format(ts['test'].count()[0],\n",
    "                                                             ts['test'].index[0],\n",
    "                                                             ts['test'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to one-hot encode a timestamp\n",
    "def one_hot_encode(timestamp):\n",
    "    # input: a timestamp\n",
    "    # output: a 7-bit list encoding the week-day, and a 24-bit list encoding the day-hour\n",
    "    fv_weekday = np.zeros(7)\n",
    "    fv_hour = np.zeros(24)\n",
    "    fv_weekday[timestamp.weekday()] = 1.\n",
    "    fv_hour[timestamp.hour] = 1.\n",
    "    return list(fv_weekday), list(fv_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_targets_timestamps_ohvs(time_series, m, tau, n_targets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "           time series: original time series\n",
    "           m: embedding dimension\n",
    "           tau: lag\n",
    "           n_targets: number of targets to predict\n",
    "    Output:\n",
    "           features: list of features\n",
    "           targets: list of targets\n",
    "           timestamps: list of target (target) timestamps\n",
    "           oh_wds: list of one-hot vectors describing weekday of timestamp\n",
    "           oh_dhs: list of one-hot vectors describing hour of the day of timestamp\n",
    "    \"\"\"\n",
    "    # a couple of empty lists to store feature vectors and targets\n",
    "    features = []\n",
    "    targets = []\n",
    "    timestamps = []\n",
    "    oh_wds = []\n",
    "    oh_dhs = []\n",
    "    sequence = range(m * tau, time_series.shape[0] - n_targets + 1)\n",
    "    for i in sequence:\n",
    "        # uncomment the following line to preview features sequence timestamps (to verify the functionality)\n",
    "        # features.append(list(time_series.iloc[(i - m * tau):i:tau].index))\n",
    "        features.append(list(time_series.iloc[(i - m * tau):i:tau]))\n",
    "        # uncomment the following line to preview targets sequence timestamps (to verify the functionality)\n",
    "        # targets.append(list(time_series.iloc[i:(i + n_targets):1].index))\n",
    "        targets.append(list(time_series.iloc[i:(i + n_targets):1]))\n",
    "        # get the timestamps for the target values (just one for the first experiment)\n",
    "        targets_timestamps_list = list(time_series.index[i:(i + n_targets):1])\n",
    "        # EXTRACT TIMESTAMPS AS BYTES FOR TFRECORD PERSISTENCE\n",
    "        targets_timestamps_list_as_bytes = [timestamp.strftime(\"%Y-%m-%d %H:%M:%S\").encode() for timestamp in\n",
    "                                           targets_timestamps_list]\n",
    "        timestamps.append(targets_timestamps_list_as_bytes)\n",
    "        # build one-hot vectors for week-day and day-hour\n",
    "        # pass the timestamp(s) in the list, not the list!\n",
    "        oh_wd_vectors, oh_dh_vectors = one_hot_encode(targets_timestamps_list[0])\n",
    "        # the one-hot-encode function already returns lists, then,\n",
    "        oh_wds.append(oh_wd_vectors)\n",
    "        oh_dhs.append(oh_dh_vectors)\n",
    "\n",
    "    # uncomment the following line to return NumPy arrays instead of Python lists\n",
    "    # features, targets, timestamps = np.array(features), np.array(targets), np.array(timestamps)\n",
    "\n",
    "    return features, targets, timestamps, oh_wds, oh_dhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to temporarily store the following SLDBs:\n",
    "# train (hourly, daily, weekly, targets, timestamps)\n",
    "# test (hourly, daily, weekly, targets, timestamps)\n",
    "# no eval(uation) dataset as the model will be trained on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_full = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {}\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {}\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw_scaled'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the variable to build the forecast over\n",
    "# get it from the extracted time series dataframe\n",
    "variable = ts_df.columns[0]\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on data resolutions\n",
    "resolutions = [\n",
    "    'hourly',\n",
    "    # 'daily',\n",
    "    # 'weekly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD ALL THE SLDBs!!!\n",
    "for stage in stages:\n",
    "    # train, eval, test\n",
    "    # for component_key in sldb['components'].keys():\n",
    "    for resolution in resolutions:\n",
    "        # hourly, daily, weekly\n",
    "        sldb_full[stage][resolution]['features'], \\\n",
    "        sldb_full[stage][resolution]['targets'], \\\n",
    "        sldb_full[stage][resolution]['timestamps'], \\\n",
    "        sldb_full[stage][resolution]['oh_wds'], \\\n",
    "        sldb_full[stage][resolution]['oh_dhs'] = \\\n",
    "        make_features_targets_timestamps_ohvs(\n",
    "            ts[stage][variable],\n",
    "            sldb['embedding'][resolution],\n",
    "            sldb['tau'][resolution],\n",
    "            sldb['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the target is stored as a no_targets-element list\n",
    "len(sldb_full['test']['hourly']['targets'][0]) == sldb['no_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on the sldb items\n",
    "items = ['features', 'targets', 'timestamps', 'oh_wds', 'oh_dhs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to collect statistics\n",
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {},\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {},\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        # 'daily': {},\n",
    "        # 'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17824 features / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 targets / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 timestamps / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 oh_wds / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 oh_dhs / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "1984 features / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 targets / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 timestamps / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 oh_wds / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 oh_dhs / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 features / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 targets / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 timestamps / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 oh_wds / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 oh_dhs / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# report statistics on stages and resolutions of SLDBs\n",
    "# and persist them to the sldb['stats'] level\n",
    "for stage in stages:\n",
    "    for resolution in resolutions:\n",
    "        for item in items:\n",
    "            # fill the values in the stats sub-dictionary\n",
    "            sldb['stats'][stage][resolution][item] = len(sldb_full[stage][resolution][item])\n",
    "            # timestamps are persisted as bytes, as in b'YYYY-MM-DD HH:MM;SS'\n",
    "            # but are required as strings, as in 'YYYY-MM-DD HH:MM;SS'\n",
    "            from_timestamp_str = sldb_full[stage][resolution]['timestamps'][0][0].decode()\n",
    "            sldb['stats'][stage][resolution]['from'] = from_timestamp_str\n",
    "            to_timestamp_str = sldb_full[stage][resolution]['timestamps'][-1][0].decode()\n",
    "            sldb['stats'][stage][resolution]['to'] = to_timestamp_str\n",
    "            # and log them\n",
    "            print('{0} {3} / {1} / {2} from {4} to {5}'.format(len(sldb_full[stage][resolution][item]),\n",
    "                                                               stage,\n",
    "                                                               resolution,\n",
    "                                                               item,\n",
    "                                                               from_timestamp_str,\n",
    "                                                               to_timestamp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in train set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['train']['hourly']['to'] == sldb['stats']['train']['daily']['to'] == sldb['stats']['train']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in eval set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['eval']['hourly']['to'] == sldb['stats']['eval']['daily']['to'] == sldb['stats']['eval']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in test set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['test']['hourly']['to'] == sldb['stats']['test']['daily']['to'] == sldb['stats']['test']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset on train stage was trimmed to 17824 rows.\n",
      "Dataset on eval stage was trimmed to 1984 rows.\n",
      "Dataset on test stage was trimmed to 1984 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows in the smaller resolution-based dataset, for alignment purposes\n",
    "for stage in stages:\n",
    "    sldb['stats'][stage]['trimmed_to_count'] = min([sldb['stats'][stage][resolution]['features'] for resolution in resolutions])\n",
    "    print('Dataset on {} stage was trimmed to {} rows.'.format(stage, sldb['stats'][stage]['trimmed_to_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary with final, trimmed data\n",
    "tfrecords = {\n",
    "    'train': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'eval': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'test': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # isolate this value, just for readability\n",
    "    value_to_trim = sldb['stats'][stage]['trimmed_to_count']\n",
    "    tfrecords[stage]['hourly'] = sldb_full[stage]['hourly']['features'][-value_to_trim:]\n",
    "    # tfrecords[stage]['daily'] = sldb_full[stage]['daily']['features'][-value_to_trim:]\n",
    "    # tfrecords[stage]['weekly'] = sldb_full[stage]['weekly']['features'][-value_to_trim:]\n",
    "    # targets and timestamps can be acquired from any resolution-based, temporary dataset (hourly, daily, weekly)\n",
    "    tfrecords[stage]['targets'] = sldb_full[stage]['hourly']['targets'][-value_to_trim:]\n",
    "    # find out the adequate way to persist timestamps (string?, bytes?)\n",
    "    # in the meantime, do not persist them to tfrecord files\n",
    "    tfrecords[stage]['timestamps'] = sldb_full[stage]['hourly']['timestamps'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_wds'] = sldb_full[stage]['hourly']['oh_wds'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_dhs'] = sldb_full[stage]['hourly']['oh_dhs'][-value_to_trim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4169542211238255,\n",
       " 0.46936634721910186,\n",
       " 0.5373257822849928,\n",
       " 0.5879978617414416,\n",
       " 0.6495006856372554,\n",
       " 0.656068083392084,\n",
       " 0.5982212167930769,\n",
       " 0.6293971815273058,\n",
       " 0.6244791868419628,\n",
       " 0.5759515637952541,\n",
       " 0.5945040557186796,\n",
       " 0.5904979198462939,\n",
       " 0.5677866699721876,\n",
       " 0.49785631832162225,\n",
       " 0.36367672009730656,\n",
       " 0.26492093866451816,\n",
       " 0.17351968205024915,\n",
       " 0.12694360730416743,\n",
       " 0.11225160175709092,\n",
       " 0.1099111383127902,\n",
       " 0.11352371065333111,\n",
       " 0.20790690827955427,\n",
       " 0.2614323233418814,\n",
       " 0.3075946915407082]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify again specs for the contents in tfrecords dictionary\n",
    "tfrecords['test']['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode byte values for serialized examples\n",
    "def _bytes_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a bytes_list from a list of strings / bytes.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: pass one-hot vectors as _int_features and decode when reading dataset???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-42-5bb59698d096>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-5bb59698d096>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    '{:03d}'.format(sldb['embedding']['hourly'],\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = '{:03d}{:03d}_' \\\n",
    "             # '{:03d}{:03d}_' \\\n",
    "             # '{:03d}{:03d}_' \\\n",
    "             '{:03d}'.format(sldb['embedding']['hourly'],\n",
    "                             sldb['tau']['hourly'],\n",
    "                             # sldb['embedding']['daily'],\n",
    "                             # sldb['tau']['daily'],\n",
    "                             # sldb['embedding']['weekly'],\n",
    "                             # sldb['tau']['weekly'],\n",
    "                             sldb['no_targets']\n",
    "                            )\n",
    "\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04115_H_kw_20201021084001_008001_008024_008168_024'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a time-based identifer for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001_008024_008168_024 was created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now persist SLDBs as TFRecords\n",
    "for stage in stages:\n",
    "    N_ROWS = sldb['stats'][stage]['trimmed_to_count']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        # get an iterable with the indexes of the NumPy array to be stored in the TFRecord file\n",
    "        for row in np.arange(N_ROWS):\n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    # [lecture[0] for lecture in Xadj_train[row]] flattens the adjacent hours array\n",
    "                    feature={'hourly': _float_feature_from_list_of_values(tfrecords[stage]['hourly'][row]),\n",
    "                             'daily': _float_feature_from_list_of_values(tfrecords[stage]['daily'][row]),\n",
    "                             'weekly': _float_feature_from_list_of_values(tfrecords[stage]['weekly'][row]),\n",
    "                             'target': _float_feature_from_list_of_values(tfrecords[stage]['targets'][row]),\n",
    "                             'oh_wd': _float_feature_from_list_of_values(tfrecords[stage]['oh_wds'][row]),\n",
    "                             'oh_dh': _float_feature_from_list_of_values(tfrecords[stage]['oh_dhs'][row]),\n",
    "                             # timestamps to be incorporated later as _byte_feature???\n",
    "                             'timestamp': _bytes_feature_from_list_of_values(tfrecords[stage]['timestamps'][row])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path for the json file\n",
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the final, compact dictionary to JSON\n",
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001_008024_008168_024/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001_008024_008168_024/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001_008024_008168_024/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001_008024_008168_024/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "\\ [4 files][ 16.8 MiB/ 16.8 MiB]                                                \n",
      "Operation completed over 4 objects/16.8 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
