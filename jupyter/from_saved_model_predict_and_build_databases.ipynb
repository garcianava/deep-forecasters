{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "# therefore, get and test the appropriate function\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via Abseil Flags\n",
    "model_id = 'DMSLSTM_TPU_000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via Abseil Flags\n",
    "execution = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '{}_{:02d}'.format(model_id, execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a given last saved model dir\n",
    "# it will be automatically found on production version\n",
    "last_saved_model_suffix = 'export/exporter/1603508757'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = '{}/{}/{}/{}'.format(PROJECT_ROOT, 'models', model_dir, last_saved_model_suffix)\n",
    "tags = 'serve'\n",
    "imported = load(export_dir=export_dir, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = imported.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via Abseil Flags\n",
    "sldb_id = 'CPE04115_H_kw_20201021084001_008001001_008024001_004168001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ts_identifier from the json file in the sldb directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_json_file = '{}/sldb.json'.format(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sldb_json_file, 'r') as inputfile:\n",
    "    sldb_dict = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_identifier = sldb_dict['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via Abseil Flags\n",
    "dataset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '{}/{}.tfrecord'.format(data_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20201021084001_008001001_008024001_004168001/test.tfrecord'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in tfrecord_dataset:\n",
    "    predictions_list.append(predict_fn(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scalar values from predictions list\n",
    "predictions = [p['forecast'][0][0] for p in predictions_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass predictions to an array\n",
    "predictions_array = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65809375, 0.66343164, 0.6697949 , 0.7323097 , 0.7499685 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load('/{}/{}/{}/scaler.save'.format(PROJECT_ROOT,\n",
    "                                                    'timeseries',\n",
    "                                                    ts_identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse-scale predictions\n",
    "predictions = scaler.inverse_transform(predictions_array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all dimensions equal to 1 in the predictions array\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass final prediction values to list for json serialization\n",
    "prediction_values_list = predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2914.691162109375,\n",
       " 2926.17431640625,\n",
       " 2939.863525390625,\n",
       " 3074.3505859375,\n",
       " 3112.33984375,\n",
       " 3034.739990234375,\n",
       " 2734.530517578125,\n",
       " 2470.22509765625,\n",
       " 2239.215576171875,\n",
       " 2104.2314453125]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_values_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_json_file = '{}/{}/{}/sldb_parameters.json'.format(PROJECT_ROOT,\n",
    "                                                              'parameters',\n",
    "                                                              model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the sldb dictionary from the json file in parameters/\n",
    "with open(json_file, 'r') as inputfile:\n",
    "    sldb_parameters = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the objective shapes for reshaping tensors in a dictionary\n",
    "_EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "    'hourly': [sldb_parameters['embedding']['hourly'], 1],\n",
    "    'daily': [sldb_parameters['embedding']['daily'], 1],\n",
    "    'weekly': [sldb_parameters['embedding']['weekly'], 1],\n",
    "    # number of targets is included in hourly, daily, and weekly features, take anyone of them\n",
    "    # ToDo: un-wire this!\n",
    "    # 'target': [sldb['components']['hourly']['no_targets'], 1],\n",
    "    'target': [1, 1],\n",
    "    'oh_wd': [7, 1],  # Monday to Sunday\n",
    "    'oh_dh': [24, 1],  # midnight to 23:00\n",
    "    # number of targets is included in hourly, daily, and weekly features, take anyone of them\n",
    "    # ToDo: un-wire this!\n",
    "    # 'timestamp': [sldb['components']['hourly']['no_targets'], 1]\n",
    "    'timestamp': [1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will be imported as:\n",
    "# from dplstm.data import _parse_dataset_function\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shapes['daily'])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shapes['weekly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    # do not parse the timestamp to TPUEstimator, as it does not support string types!\n",
    "    # ToDo: code timestamps into features, as numbers\n",
    "    #  so they can be parsed to training\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'hourly': hourly,\n",
    "        'daily': daily,\n",
    "        'weekly': weekly,\n",
    "        'oh_wd': oh_wd,\n",
    "        'oh_dh': oh_dh,\n",
    "    }\n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    return feature_dict, target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset was previously acquired from tfrecord file\n",
    "# use it again to build arrays for targets and timestamps\n",
    "parsed_dataset = tfrecord_dataset.map(lambda row: _parse_dataset_function(example_proto=row,\n",
    "                                                                          objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "                                                                          parse_timestamp=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_timestamps_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parsed_example in parsed_dataset:\n",
    "    string_timestamp = str(np.asarray(parsed_example[0]['timestamp'][0][0]).astype(str))\n",
    "    string_timestamps_list.append(string_timestamp)\n",
    "    target = parsed_example[1][0]\n",
    "    targets_list.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-05-26 17:00:00',\n",
       " '2018-05-26 18:00:00',\n",
       " '2018-05-26 19:00:00',\n",
       " '2018-05-26 20:00:00',\n",
       " '2018-05-26 21:00:00']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_timestamps_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.65253454>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6731292>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.69301194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.73025095>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.78971857>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array = np.asarray(targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array = scaler.inverse_transform(targets_array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_array = np.squeeze(targets_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values_list = targets_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2902.731689453125,\n",
       " 2947.03662109375,\n",
       " 2989.81005859375,\n",
       " 3069.921630859375,\n",
       " 3197.853515625]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_values_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage prediction results\n",
    "prediction_results = {\n",
    "    'string_timestamps': string_timestamps_list,\n",
    "    'predictions': prediction_values_list,\n",
    "    'targets': target_values_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = '{}/{}/{}/{}_on_{}_tfrecord.json'.format(PROJECT_ROOT,\n",
    "                                                           'stats',\n",
    "                                                           'predictions',\n",
    "                                                           model_dir,\n",
    "                                                           dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the results dictionary to json\n",
    "with open(output_filename, 'w') as outfile:\n",
    "    json.dump(prediction_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far the json file for stats/predictions/ is ready..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the data in memory to produce the pickle file for database/predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a columns list for the predictions dataframe\n",
    "pred_df_columns = ['model_id', 'execution', 'dataset', 'string_timestamp', 'prediction', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataframe\n",
    "# how many predictions in the dataset?\n",
    "length = len(prediction_results['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list with model_id repeated length times\n",
    "model_id_repeat_list = [model_id]*length\n",
    "# same for execution\n",
    "execution_repeat_list = [execution]*length\n",
    "# same for dataset\n",
    "dataset_repeat_list = [dataset]*length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions dataframe\n",
    "predictions_df = pd.DataFrame(list(zip(model_id_repeat_list,\n",
    "                                       execution_repeat_list,\n",
    "                                       dataset_repeat_list,\n",
    "                                       prediction_results['string_timestamps'],\n",
    "                                       prediction_results['predictions'],\n",
    "                                       prediction_results['targets'])), columns=pred_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>execution</th>\n",
       "      <th>dataset</th>\n",
       "      <th>string_timestamp</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-05-26 17:00:00</td>\n",
       "      <td>2914.691162</td>\n",
       "      <td>2902.731689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-05-26 18:00:00</td>\n",
       "      <td>2926.174316</td>\n",
       "      <td>2947.036621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-05-26 19:00:00</td>\n",
       "      <td>2939.863525</td>\n",
       "      <td>2989.810059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-05-26 20:00:00</td>\n",
       "      <td>3074.350586</td>\n",
       "      <td>3069.921631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-05-26 21:00:00</td>\n",
       "      <td>3112.339844</td>\n",
       "      <td>3197.853516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-07-31 19:00:00</td>\n",
       "      <td>2929.029297</td>\n",
       "      <td>2902.200195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-07-31 20:00:00</td>\n",
       "      <td>3022.406006</td>\n",
       "      <td>2912.063232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-07-31 21:00:00</td>\n",
       "      <td>2983.091064</td>\n",
       "      <td>2983.391602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-07-31 22:00:00</td>\n",
       "      <td>2779.133301</td>\n",
       "      <td>2810.393311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>DMSLSTM_TPU_000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2018-07-31 23:00:00</td>\n",
       "      <td>2434.371582</td>\n",
       "      <td>2548.693359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1591 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_id  execution dataset     string_timestamp   prediction  \\\n",
       "0     DMSLSTM_TPU_000          0    test  2018-05-26 17:00:00  2914.691162   \n",
       "1     DMSLSTM_TPU_000          0    test  2018-05-26 18:00:00  2926.174316   \n",
       "2     DMSLSTM_TPU_000          0    test  2018-05-26 19:00:00  2939.863525   \n",
       "3     DMSLSTM_TPU_000          0    test  2018-05-26 20:00:00  3074.350586   \n",
       "4     DMSLSTM_TPU_000          0    test  2018-05-26 21:00:00  3112.339844   \n",
       "...               ...        ...     ...                  ...          ...   \n",
       "1586  DMSLSTM_TPU_000          0    test  2018-07-31 19:00:00  2929.029297   \n",
       "1587  DMSLSTM_TPU_000          0    test  2018-07-31 20:00:00  3022.406006   \n",
       "1588  DMSLSTM_TPU_000          0    test  2018-07-31 21:00:00  2983.091064   \n",
       "1589  DMSLSTM_TPU_000          0    test  2018-07-31 22:00:00  2779.133301   \n",
       "1590  DMSLSTM_TPU_000          0    test  2018-07-31 23:00:00  2434.371582   \n",
       "\n",
       "           target  \n",
       "0     2902.731689  \n",
       "1     2947.036621  \n",
       "2     2989.810059  \n",
       "3     3069.921631  \n",
       "4     3197.853516  \n",
       "...           ...  \n",
       "1586  2902.200195  \n",
       "1587  2912.063232  \n",
       "1588  2983.391602  \n",
       "1589  2810.393311  \n",
       "1590  2548.693359  \n",
       "\n",
       "[1591 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to persist the dataframe to database/predictions/\n",
    "pickle_path = '{}/{}/{}/{}_{:02d}_on_{}_tfrecord.pkl'.format(PROJECT_ROOT,\n",
    "                                                             'database',\n",
    "                                                             'predictions',\n",
    "                                                             model_id,\n",
    "                                                             execution,\n",
    "                                                             dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted Pandas dataframe for predictions of DMSLSTM_TPU_000_00 on test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# persist the Pandas dataframe to database/predictions/\n",
    "predictions_df.to_pickle(pickle_path)\n",
    "print('Persisted Pandas dataframe for predictions of {}_{:02d} on {}.tfrecord'.format(model_id,\n",
    "                                                                                      execution,\n",
    "                                                                                      dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get wall times from TensorBoard summaries for model training,\n",
    "# and persist them to json files in stats/training_wall_times,\n",
    "# and to pickle files in database/training_wall_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wall_time(path_to_logdir):\n",
    "    '''\n",
    "    receives a UNIX path to the TensorBoard logdir of a model training,\n",
    "    returns the wall time for the model training process\n",
    "    '''\n",
    "    # an event accumulator to the logdir\n",
    "    ea = event_accumulator.EventAccumulator(path_to_logdir,\n",
    "                                            size_guidance={ # see below regarding this argument\n",
    "                                                # event_accumulator.COMPRESSED_HISTOGRAMS: 500, # not used\n",
    "                                                # event_accumulator.IMAGES: 4, # not used\n",
    "                                                # event_accumulator.AUDIO: 4, # not used\n",
    "                                                event_accumulator.SCALARS: 0, # retrieve all\n",
    "                                                event_accumulator.TENSORS: 0, # retrieve all\n",
    "                                                # event_accumulator.HISTOGRAMS: 1 # not used\n",
    "                                            }\n",
    "                                           )\n",
    "    # loads events from file\n",
    "    ea.Reload()\n",
    "    \n",
    "    # wall time is end time - start time\n",
    "    wall_time = ea.Tensors('loss')[-1][0] - ea.Tensors('loss')[0][0]\n",
    "    print(\"Wall time for model in '{}' is {} seconds.\".format(path_to_logdir,\n",
    "                                                            wall_time))\n",
    "    return wall_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'models', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time for model in '/home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_000_00' is 23.646011114120483 seconds.\n"
     ]
    }
   ],
   "source": [
    "wall_time = get_wall_time(tensorboard_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python dictionary to store the wall_time value\n",
    "wt_dictionary = {\n",
    "    'wall_time': wall_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to stats/training_wall_times/ to persist the json file\n",
    "output_filename = '{}/{}/{}/{}.json'.format(PROJECT_ROOT, 'stats', 'training_wall_times', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/stats/training_wall_times/DMSLSTM_TPU_000_00.json'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and persist the dictionary as json file\n",
    "with open(output_filename, 'w') as outfile:\n",
    "    json.dump(wt_dictionary, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a columns list for the wall times dataframe\n",
    "wt_df_columns = ['model_id', 'execution', 'wall_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataframe\n",
    "wall_times_df = pd.DataFrame([[model_id,\n",
    "                               execution,\n",
    "                               wt_dictionary['wall_time']]], columns=wt_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to persist the dataframe to database/predictions/\n",
    "pickle_path = '{}/{}/{}/{}_{:02d}.pkl'.format(PROJECT_ROOT,\n",
    "                                                             'database',\n",
    "                                                             'training_wall_times',\n",
    "                                                             model_id,\n",
    "                                                             execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted Pandas dataframe for wall time of DMSLSTM_TPU_000_00\n"
     ]
    }
   ],
   "source": [
    "# persist the Pandas dataframe to database/\n",
    "wall_times_df.to_pickle(pickle_path)\n",
    "print('Persisted Pandas dataframe for wall time of {}_{:02d}'.format(model_id,\n",
    "                                                                     execution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
