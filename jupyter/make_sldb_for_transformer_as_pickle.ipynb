{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the variable 'labels' with 'targets', as the latter is more adequate for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale datasets to improve neural networks performance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the time series directory\n",
    "# scaler.save\n",
    "# ts.json\n",
    "# ts.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in the SLDB directory:\n",
    "# train.tfrecord\n",
    "# eval.tfrecord\n",
    "# test.tfrecord\n",
    "# sldb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to configure the SLDB\n",
    "# ToDo: transfer this dictionary to dplstm/configs/sldb_config.py\n",
    "\n",
    "# modify the dictionary structure:\n",
    "# no_targets must be the same for all components, then move it to an upper level\n",
    "# remove components and use the same structure as in architecture_parameters\n",
    "\n",
    "# ToDo: build all sldb dictionaries on the basis of list-type parameters,\n",
    "#  by iterating on them to avoid comments on the non-used resolutions, like\n",
    "#  m = [8, 8, 8], tau = [1, 24, 168], no_targets = [24] or\n",
    "#  m = [256], tau = [1], no_targets = [24]\n",
    "sldb = {\n",
    "    'ts': 'CPE04115_H_kw_20201021084001',\n",
    "    'embedding': {\n",
    "        'hourly': 256\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 24\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series was built and persisted in a different code\n",
    "# SLDB constructions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required time series\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(sldb['ts'])\n",
    "pickle_filename = '{}/ts.pkl'.format(time_series_folder)\n",
    "ts_df = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation stage is not used for TPU-based training,\n",
    "# however, evaluation dataset might be useful to get stats from CPU-based training\n",
    "stages = ['train', 'eval', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into train/eval/test at time series level\n",
    "# to avoid data overlapping at SLDB level\n",
    "split = np.array([0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of the scaled time series for train, validation, and test thresholds\n",
    "train_eval_limit = np.int(ts_df.count()*split[0])\n",
    "eval_test_limit = np.int(ts_df.count()*split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage the time series for the different model stages\n",
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18103 lectures in train time series from 2016-01-01 00:00:00 to 2018-01-24 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for train set\n",
    "ts['train'] = ts_df[:train_eval_limit]\n",
    "print('{0} lectures in train time series from {1} to {2}'.format(ts['train'].count()[0],\n",
    "                                                                 ts['train'].index[0],\n",
    "                                                                 ts['train'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in eval time series from 2018-01-24 09:00:00 to 2018-04-28 16:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for eval set\n",
    "ts['eval'] = ts_df[train_eval_limit:eval_test_limit]\n",
    "print('{0} lectures in eval time series from {1} to {2}'.format(ts['eval'].count()[0],\n",
    "                                                                ts['eval'].index[0],\n",
    "                                                                ts['eval'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 lectures in test time series from 2018-04-28 17:00:00 to 2018-07-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# get the time series portion for test set\n",
    "ts['test'] = ts_df[eval_test_limit:]\n",
    "print('{} lectures in test time series from {} to {}'.format(ts['test'].count()[0],\n",
    "                                                             ts['test'].index[0],\n",
    "                                                             ts['test'].index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to one-hot encode a timestamp\n",
    "def one_hot_encode(timestamp):\n",
    "    # input: a timestamp\n",
    "    # output: a 7-bit list encoding the week-day, and a 24-bit list encoding the day-hour\n",
    "    fv_weekday = np.zeros(7)\n",
    "    fv_hour = np.zeros(24)\n",
    "    fv_weekday[timestamp.weekday()] = 1.\n",
    "    fv_hour[timestamp.hour] = 1.\n",
    "    return list(fv_weekday), list(fv_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_targets_timestamps_ohvs(time_series, m, tau, n_targets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "           time series: original time series\n",
    "           m: embedding dimension\n",
    "           tau: lag\n",
    "           n_targets: number of targets to predict\n",
    "    Output:\n",
    "           features: list of features\n",
    "           targets: list of targets\n",
    "           timestamps: list of target (target) timestamps\n",
    "           oh_wds: list of one-hot vectors describing weekday of timestamp\n",
    "           oh_dhs: list of one-hot vectors describing hour of the day of timestamp\n",
    "    \"\"\"\n",
    "    # a couple of empty lists to store feature vectors and targets\n",
    "    features = []\n",
    "    targets = []\n",
    "    timestamps = []\n",
    "    oh_wds = []\n",
    "    oh_dhs = []\n",
    "    sequence = range(m * tau, time_series.shape[0] - n_targets + 1)\n",
    "    for i in sequence:\n",
    "        # uncomment the following line to preview features sequence timestamps (to verify the functionality)\n",
    "        # features.append(list(time_series.iloc[(i - m * tau):i:tau].index))\n",
    "        features.append(list(time_series.iloc[(i - m * tau):i:tau]))\n",
    "        # uncomment the following line to preview targets sequence timestamps (to verify the functionality)\n",
    "        # targets.append(list(time_series.iloc[i:(i + n_targets):1].index))\n",
    "        targets.append(list(time_series.iloc[i:(i + n_targets):1]))\n",
    "        # get the timestamps for the target values (just one for the first experiment)\n",
    "        targets_timestamps_list = list(time_series.index[i:(i + n_targets):1])\n",
    "        # EXTRACT TIMESTAMPS AS BYTES FOR TFRECORD PERSISTENCE\n",
    "        targets_timestamps_list_as_bytes = [timestamp.strftime(\"%Y-%m-%d %H:%M:%S\").encode() for timestamp in\n",
    "                                           targets_timestamps_list]\n",
    "        timestamps.append(targets_timestamps_list_as_bytes)\n",
    "        # build one-hot vectors for week-day and day-hour\n",
    "        # pass the timestamp(s) in the list, not the list!\n",
    "        oh_wd_vectors, oh_dh_vectors = one_hot_encode(targets_timestamps_list[0])\n",
    "        # the one-hot-encode function already returns lists, then,\n",
    "        oh_wds.append(oh_wd_vectors)\n",
    "        oh_dhs.append(oh_dh_vectors)\n",
    "\n",
    "    # uncomment the following line to return NumPy arrays instead of Python lists\n",
    "    # features, targets, timestamps = np.array(features), np.array(targets), np.array(timestamps)\n",
    "\n",
    "    return features, targets, timestamps, oh_wds, oh_dhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to temporarily store the following SLDBs:\n",
    "# train (hourly, daily, weekly, targets, timestamps)\n",
    "# test (hourly, daily, weekly, targets, timestamps)\n",
    "# no eval(uation) dataset as the model will be trained on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_full = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw_scaled'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the variable to build the forecast over\n",
    "# get it from the extracted time series dataframe\n",
    "variable = ts_df.columns[0]\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on data resolutions\n",
    "resolutions = [\n",
    "    'hourly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD ALL THE SLDBs!!!\n",
    "for stage in stages:\n",
    "    # train, eval, test\n",
    "    # for component_key in sldb['components'].keys():\n",
    "    for resolution in resolutions:\n",
    "        # hourly, daily, weekly\n",
    "        sldb_full[stage][resolution]['features'], \\\n",
    "        sldb_full[stage][resolution]['targets'], \\\n",
    "        sldb_full[stage][resolution]['timestamps'], \\\n",
    "        sldb_full[stage][resolution]['oh_wds'], \\\n",
    "        sldb_full[stage][resolution]['oh_dhs'] = \\\n",
    "        make_features_targets_timestamps_ohvs(\n",
    "            ts[stage][variable],\n",
    "            sldb['embedding'][resolution],\n",
    "            sldb['tau'][resolution],\n",
    "            sldb['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the target is stored as a no_targets-element list\n",
    "len(sldb_full['test']['hourly']['targets'][0]) == sldb['no_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on the sldb items\n",
    "items = ['features', 'targets', 'timestamps', 'oh_wds', 'oh_dhs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to collect statistics\n",
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'hourly': {}\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {}\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17824 features / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 targets / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 timestamps / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 oh_wds / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "17824 oh_dhs / train / hourly from 2016-01-11 16:00:00 to 2018-01-23 09:00:00\n",
      "1984 features / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 targets / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 timestamps / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 oh_wds / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 oh_dhs / eval / hourly from 2018-02-04 01:00:00 to 2018-04-27 17:00:00\n",
      "1984 features / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 targets / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 timestamps / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 oh_wds / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n",
      "1984 oh_dhs / test / hourly from 2018-05-09 09:00:00 to 2018-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# report statistics on stages and resolutions of SLDBs\n",
    "# and persist them to the sldb['stats'] level\n",
    "for stage in stages:\n",
    "    for resolution in resolutions:\n",
    "        for item in items:\n",
    "            # fill the values in the stats sub-dictionary\n",
    "            sldb['stats'][stage][resolution][item] = len(sldb_full[stage][resolution][item])\n",
    "            # timestamps are persisted as bytes, as in b'YYYY-MM-DD HH:MM;SS'\n",
    "            # but are required as strings, as in 'YYYY-MM-DD HH:MM;SS'\n",
    "            from_timestamp_str = sldb_full[stage][resolution]['timestamps'][0][0].decode()\n",
    "            sldb['stats'][stage][resolution]['from'] = from_timestamp_str\n",
    "            to_timestamp_str = sldb_full[stage][resolution]['timestamps'][-1][0].decode()\n",
    "            sldb['stats'][stage][resolution]['to'] = to_timestamp_str\n",
    "            # and log them\n",
    "            print('{0} {3} / {1} / {2} from {4} to {5}'.format(len(sldb_full[stage][resolution][item]),\n",
    "                                                               stage,\n",
    "                                                               resolution,\n",
    "                                                               item,\n",
    "                                                               from_timestamp_str,\n",
    "                                                               to_timestamp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset on train stage was trimmed to 17824 rows.\n",
      "Dataset on eval stage was trimmed to 1984 rows.\n",
      "Dataset on test stage was trimmed to 1984 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows in the smaller resolution-based dataset, for alignment purposes\n",
    "for stage in stages:\n",
    "    sldb['stats'][stage]['trimmed_to_count'] = min([sldb['stats'][stage][resolution]['features'] for resolution in resolutions])\n",
    "    print('Dataset on {} stage was trimmed to {} rows.'.format(stage, sldb['stats'][stage]['trimmed_to_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary with final, trimmed data\n",
    "tfrecords = {\n",
    "    'train': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'eval': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'test': {}, # hourly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # isolate this value, just for readability\n",
    "    value_to_trim = sldb['stats'][stage]['trimmed_to_count']\n",
    "    tfrecords[stage]['hourly'] = sldb_full[stage]['hourly']['features'][-value_to_trim:]\n",
    "    # targets and timestamps can be acquired from any resolution-based, temporary dataset (hourly, daily, weekly)\n",
    "    tfrecords[stage]['targets'] = sldb_full[stage]['hourly']['targets'][-value_to_trim:]\n",
    "    # find out the adequate way to persist timestamps (string?, bytes?)\n",
    "    # in the meantime, do not persist them to tfrecord files\n",
    "    tfrecords[stage]['timestamps'] = sldb_full[stage]['hourly']['timestamps'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_wds'] = sldb_full[stage]['hourly']['oh_wds'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_dhs'] = sldb_full[stage]['hourly']['oh_dhs'][-value_to_trim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4169542211238255,\n",
       " 0.46936634721910186,\n",
       " 0.5373257822849928,\n",
       " 0.5879978617414416,\n",
       " 0.6495006856372554,\n",
       " 0.656068083392084,\n",
       " 0.5982212167930769,\n",
       " 0.6293971815273058,\n",
       " 0.6244791868419628,\n",
       " 0.5759515637952541,\n",
       " 0.5945040557186796,\n",
       " 0.5904979198462939,\n",
       " 0.5677866699721876,\n",
       " 0.49785631832162225,\n",
       " 0.36367672009730656,\n",
       " 0.26492093866451816,\n",
       " 0.17351968205024915,\n",
       " 0.12694360730416743,\n",
       " 0.11225160175709092,\n",
       " 0.1099111383127902,\n",
       " 0.11352371065333111,\n",
       " 0.20790690827955427,\n",
       " 0.2614323233418814,\n",
       " 0.3075946915407082]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify again specs for the contents in tfrecords dictionary\n",
    "tfrecords['test']['targets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets from tfrecords dictionary and persist them as pickles\n",
    "# to test transformer operations on a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hourly = np.array(tfrecords['train']['hourly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17824, 256)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hourly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_train_hourly.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, train_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat operation for all required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17824, 24)\n"
     ]
    }
   ],
   "source": [
    "train_targets = np.array(tfrecords['train']['targets'])\n",
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_to_24_train_targets.npy'\n",
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, train_targets)\n",
    "    \n",
    "print(train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 256)\n"
     ]
    }
   ],
   "source": [
    "eval_hourly = np.array(tfrecords['eval']['hourly'])\n",
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_to_24_eval_hourly.npy'\n",
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, eval_hourly)\n",
    "    \n",
    "print(eval_hourly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 24)\n"
     ]
    }
   ],
   "source": [
    "eval_targets = np.array(tfrecords['eval']['targets'])\n",
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_to_24_eval_targets.npy'\n",
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, eval_targets)\n",
    "    \n",
    "print(eval_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 256)\n"
     ]
    }
   ],
   "source": [
    "test_hourly = np.array(tfrecords['test']['hourly'])\n",
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_to_24_test_hourly.npy'\n",
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, test_hourly)\n",
    "    \n",
    "print(test_hourly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 24)\n"
     ]
    }
   ],
   "source": [
    "test_targets = np.array(tfrecords['test']['targets'])\n",
    "path = '/home/developer/gcp/cbidmltsf/sldbs/256_to_24_test_targets.npy'\n",
    "with open(path, 'wb') as filename:\n",
    "    np.save(filename, test_targets)\n",
    "    \n",
    "print(test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
