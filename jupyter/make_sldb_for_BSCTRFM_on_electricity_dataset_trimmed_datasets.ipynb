{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make SLDB datasets for BSCTRFM from individual time series\n",
    "\n",
    "# first, use this code for trimmed datasets only\n",
    "# later, generalize for all the time series in the electricity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import row, gridplot, layout\n",
    "from bokeh.palettes import d3\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main source is the electricity dataset LD2011-2014 from UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it resides in\n",
    "dataset_path = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD2011_2014.txt',\n",
       " 'separated_preprocessed',\n",
       " 'separated_raw',\n",
       " 'hourly_electricity_complete.pkl',\n",
       " 'hourly_electricity.csv',\n",
       " 'LD2011_2014.txt.zip',\n",
       " 'hourly_electricity_filtered_academic_papers.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LD2011_2014.txt'                                          source from UCI\n",
    "# 'LD2011_2014.txt.zip'                                      source from UCI, compressed\n",
    "# 'hourly_electricity.csv'                                   complete dataset in CSV\n",
    "# 'hourly_electricity_complete.pkl'                          complete dataset in Pandas\n",
    "# 'hourly_electricity_filtered_academic_papers.pkl'          filtered dataset for benchmarking\n",
    "# 'separated_raw/'                                           pickles per customer, raw data\n",
    "# 'separated_preprocessed/'                                  pickles per customer, outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a SLDB is produced from separated time series (raw or preprocessed)\n",
    "\n",
    "# SLDB contents are:\n",
    "# TFRecord files for training\n",
    "# TFRecord files for evaluation (if eval required)\n",
    "# time series pickles for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a constant to make sin/cos functions from hours_from_start (the 'age' covariate)\n",
    "total_hours = 32303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global dataset intervals (they might not be precise when missing values exist)\n",
    "\n",
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 16:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "dates = {\n",
    "    'train': {\n",
    "        'start': '2014-01-01 00:00:00',\n",
    "        'end': '2014-08-07 15:00:00',\n",
    "    },\n",
    "    'eval': {\n",
    "        'start': '2014-08-07 16:00:00',\n",
    "        'end': '2014-08-31 23:00:00',\n",
    "    },\n",
    "    'test': {\n",
    "        'start': '2014-09-01 00:00:00',\n",
    "        'end': '2014-09-07 23:00:00',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "\n",
    "# use 7D encoder (age, hour-day, day-week)\n",
    "\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hours_from_start',\n",
    "    'cos_hours_from_start',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    # 'sin_day_month',\n",
    "    # 'cos_day_month',\n",
    "    # 'sin_day_year',\n",
    "    # 'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_SEPARATED_FULL',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1,\n",
    "    'preprocessed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_SEPARATED_FULL',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'preprocessed': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "\n",
    "# add the suffix '11D' to differentiate this SLDB from the original one, which is 9D\n",
    "\n",
    "# add the suffix MMX to indicate the scaler used was MinMax\n",
    "# add the suffix STD to indicate the scaler used was Standard\n",
    "\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}_07DB_MMX'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX/scalers'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a path to the scalers sub-directory\n",
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CREATE SLDB FOLDERS, THEY WERE CREATED BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hours_from_start',\n",
    "    'cos_hours_from_start',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    # 'sin_day_month',\n",
    "    # 'cos_day_month',\n",
    "    # 'sin_day_year',\n",
    "    # 'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store the number of examples per customer_id, stage\n",
    "count = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of cores available for training in Cloud TPU\n",
    "num_cores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 21 datasets to be trimmed for SLDB production\n",
    "\n",
    "token_ids = [66,\n",
    "             106, 107, 108, 109, 110, 111, 112, 113, 115, 116,\n",
    "             117, 120, 121, 122, 133, 160, 178, 181, 337, 347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are we training over raw data or preprocessed data?\n",
    "state = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer_id in customer_ids:\n",
    "    customer_data_path = '{}/separated_{}/{}.pkl'.format(dataset_path, state, customer_id)\n",
    "    data[customer_id] = pd.read_pickle(customer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary for trimming dates per customer_id\n",
    "train_start_date = {\n",
    "    'MT_066': '2014-07-15 16:00:00',\n",
    "    'MT_106': '2014-01-14 00:00:00',\n",
    "    'MT_107': '2014-01-14 00:00:00',\n",
    "    'MT_108': '2014-01-14 00:00:00',\n",
    "    'MT_109': '2014-02-18 00:00:00',\n",
    "    'MT_110': '2014-01-14 00:00:00',\n",
    "    'MT_111': '2014-01-14 00:00:00',\n",
    "    'MT_112': '2014-02-12 00:00:00',\n",
    "    'MT_113': '2014-01-14 00:00:00',\n",
    "    'MT_115': '2014-01-14 00:00:00',\n",
    "    'MT_116': '2014-02-18 00:00:00',\n",
    "    'MT_117': '2014-01-14 00:00:00',\n",
    "    'MT_120': '2014-01-14 00:00:00',\n",
    "    'MT_121': '2014-01-14 00:00:00',\n",
    "    'MT_122': '2014-01-14 00:00:00',\n",
    "    'MT_133': '2014-03-13 16:00:00',\n",
    "    'MT_160': '2014-02-04 00:00:00',\n",
    "    'MT_178': '2014-07-18 00:00:00',\n",
    "    'MT_181': '2014-03-05 00:00:00',\n",
    "    'MT_337': '2014-01-17 00:00:00',\n",
    "    'MT_347': '2014-02-28 00:00:00',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added positional encodings to MT_066.\n",
      "MT_066 train interval: from 1648317 on 2014-07-15 16:00:00 to 1648868 on 2014-08-07 15:00:00, 552 lectures\n",
      "MT_066 eval interval: from 1648869 on 2014-08-07 16:00:00 to 1649452 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_066\n",
      "Scaler min_max persisted for MT_066\n",
      "MT_066 test interval: from 1649118 on 2014-08-18 01:00:00 to 1649620 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_066\n",
      "MT_066 processed. The number of examples in train dataset is 217\n",
      "MT_066 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_066 was adjusted to 216\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_066 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_066\n",
      "Persisted eval TFRecord file for MT_066\n",
      "Added positional encodings to MT_106.\n",
      "MT_106 train interval: from 2668763 on 2014-01-14 00:00:00 to 2673698 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_106 eval interval: from 2673699 on 2014-08-07 16:00:00 to 2674282 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_106\n",
      "Scaler min_max persisted for MT_106\n",
      "MT_106 test interval: from 2673948 on 2014-08-18 01:00:00 to 2674450 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_106\n",
      "MT_106 processed. The number of examples in train dataset is 4601\n",
      "MT_106 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_106 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_106 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_106\n",
      "Persisted eval TFRecord file for MT_106\n",
      "Added positional encodings to MT_107.\n",
      "MT_107 train interval: from 2677212 on 2014-01-14 00:00:00 to 2682147 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_107 eval interval: from 2682148 on 2014-08-07 16:00:00 to 2682731 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_107\n",
      "Scaler min_max persisted for MT_107\n",
      "MT_107 test interval: from 2682397 on 2014-08-18 01:00:00 to 2682899 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_107\n",
      "MT_107 processed. The number of examples in train dataset is 4601\n",
      "MT_107 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_107 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_107 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_107\n",
      "Persisted eval TFRecord file for MT_107\n",
      "Added positional encodings to MT_108.\n",
      "MT_108 train interval: from 2685661 on 2014-01-14 00:00:00 to 2690596 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_108 eval interval: from 2690597 on 2014-08-07 16:00:00 to 2691180 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_108\n",
      "Scaler min_max persisted for MT_108\n",
      "MT_108 test interval: from 2690846 on 2014-08-18 01:00:00 to 2691348 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_108\n",
      "MT_108 processed. The number of examples in train dataset is 4601\n",
      "MT_108 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_108 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_108 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_108\n",
      "Persisted eval TFRecord file for MT_108\n",
      "Added positional encodings to MT_109.\n",
      "MT_109 train interval: from 2694110 on 2014-02-18 00:00:00 to 2698205 on 2014-08-07 15:00:00, 4096 lectures\n",
      "MT_109 eval interval: from 2698206 on 2014-08-07 16:00:00 to 2698789 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_109\n",
      "Scaler min_max persisted for MT_109\n",
      "MT_109 test interval: from 2698455 on 2014-08-18 01:00:00 to 2698957 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_109\n",
      "MT_109 processed. The number of examples in train dataset is 3761\n",
      "MT_109 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_109 was adjusted to 3760\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_109 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_109\n",
      "Persisted eval TFRecord file for MT_109\n",
      "Added positional encodings to MT_110.\n",
      "MT_110 train interval: from 2701719 on 2014-01-14 00:00:00 to 2706654 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_110 eval interval: from 2706655 on 2014-08-07 16:00:00 to 2707238 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_110\n",
      "Scaler min_max persisted for MT_110\n",
      "MT_110 test interval: from 2706904 on 2014-08-18 01:00:00 to 2707406 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_110\n",
      "MT_110 processed. The number of examples in train dataset is 4601\n",
      "MT_110 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_110 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_110 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_110\n",
      "Persisted eval TFRecord file for MT_110\n",
      "Added positional encodings to MT_111.\n",
      "MT_111 train interval: from 2710168 on 2014-01-14 00:00:00 to 2715103 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_111 eval interval: from 2715104 on 2014-08-07 16:00:00 to 2715687 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_111\n",
      "Scaler min_max persisted for MT_111\n",
      "MT_111 test interval: from 2715353 on 2014-08-18 01:00:00 to 2715855 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_111\n",
      "MT_111 processed. The number of examples in train dataset is 4601\n",
      "MT_111 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_111 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_111 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_111\n",
      "Persisted eval TFRecord file for MT_111\n",
      "Added positional encodings to MT_112.\n",
      "MT_112 train interval: from 2718617 on 2014-02-12 00:00:00 to 2722856 on 2014-08-07 15:00:00, 4240 lectures\n",
      "MT_112 eval interval: from 2722857 on 2014-08-07 16:00:00 to 2723440 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_112\n",
      "Scaler min_max persisted for MT_112\n",
      "MT_112 test interval: from 2723106 on 2014-08-18 01:00:00 to 2723608 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_112\n",
      "MT_112 processed. The number of examples in train dataset is 3905\n",
      "MT_112 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_112 was adjusted to 3904\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_112 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_112\n",
      "Persisted eval TFRecord file for MT_112\n",
      "Added positional encodings to MT_113.\n",
      "MT_113 train interval: from 2726370 on 2014-01-14 00:00:00 to 2731305 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_113 eval interval: from 2731306 on 2014-08-07 16:00:00 to 2731889 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_113\n",
      "Scaler min_max persisted for MT_113\n",
      "MT_113 test interval: from 2731555 on 2014-08-18 01:00:00 to 2732057 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_113\n",
      "MT_113 processed. The number of examples in train dataset is 4601\n",
      "MT_113 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_113 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_113 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_113\n",
      "Persisted eval TFRecord file for MT_113\n",
      "Added positional encodings to MT_115.\n",
      "MT_115 train interval: from 2761124 on 2014-01-14 00:00:00 to 2766059 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_115 eval interval: from 2766060 on 2014-08-07 16:00:00 to 2766643 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_115\n",
      "Scaler min_max persisted for MT_115\n",
      "MT_115 test interval: from 2766309 on 2014-08-18 01:00:00 to 2766811 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_115 processed. The number of examples in train dataset is 4601\n",
      "MT_115 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_115 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_115 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_115\n",
      "Persisted eval TFRecord file for MT_115\n",
      "Added positional encodings to MT_116.\n",
      "MT_116 train interval: from 2769573 on 2014-02-18 00:00:00 to 2773668 on 2014-08-07 15:00:00, 4096 lectures\n",
      "MT_116 eval interval: from 2773669 on 2014-08-07 16:00:00 to 2774252 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_116\n",
      "Scaler min_max persisted for MT_116\n",
      "MT_116 test interval: from 2773918 on 2014-08-18 01:00:00 to 2774420 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_116\n",
      "MT_116 processed. The number of examples in train dataset is 3761\n",
      "MT_116 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_116 was adjusted to 3760\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_116 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_116\n",
      "Persisted eval TFRecord file for MT_116\n",
      "Added positional encodings to MT_117.\n",
      "MT_117 train interval: from 2777182 on 2014-01-14 00:00:00 to 2782117 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_117 eval interval: from 2782118 on 2014-08-07 16:00:00 to 2782701 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_117\n",
      "Scaler min_max persisted for MT_117\n",
      "MT_117 test interval: from 2782367 on 2014-08-18 01:00:00 to 2782869 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_117\n",
      "MT_117 processed. The number of examples in train dataset is 4601\n",
      "MT_117 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_117 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_117 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_117\n",
      "Persisted eval TFRecord file for MT_117\n",
      "Added positional encodings to MT_120.\n",
      "MT_120 train interval: from 2838241 on 2014-01-14 00:00:00 to 2843176 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_120 eval interval: from 2843177 on 2014-08-07 16:00:00 to 2843760 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_120\n",
      "Scaler min_max persisted for MT_120\n",
      "MT_120 test interval: from 2843426 on 2014-08-18 01:00:00 to 2843928 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_120\n",
      "MT_120 processed. The number of examples in train dataset is 4601\n",
      "MT_120 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_120 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_120 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_120\n",
      "Persisted eval TFRecord file for MT_120\n",
      "Added positional encodings to MT_121.\n",
      "MT_121 train interval: from 2846690 on 2014-01-14 00:00:00 to 2851625 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_121 eval interval: from 2851626 on 2014-08-07 16:00:00 to 2852209 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_121\n",
      "Scaler min_max persisted for MT_121\n",
      "MT_121 test interval: from 2851875 on 2014-08-18 01:00:00 to 2852377 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_121\n",
      "MT_121 processed. The number of examples in train dataset is 4601\n",
      "MT_121 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_121 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_121 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_121\n",
      "Persisted eval TFRecord file for MT_121\n",
      "Added positional encodings to MT_122.\n",
      "MT_122 train interval: from 2855139 on 2014-01-14 00:00:00 to 2860074 on 2014-08-07 15:00:00, 4936 lectures\n",
      "MT_122 eval interval: from 2860075 on 2014-08-07 16:00:00 to 2860658 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_122\n",
      "Scaler min_max persisted for MT_122\n",
      "MT_122 test interval: from 2860324 on 2014-08-18 01:00:00 to 2860826 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_122\n",
      "MT_122 processed. The number of examples in train dataset is 4601\n",
      "MT_122 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_122 was adjusted to 4600\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_122 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_122\n",
      "Persisted eval TFRecord file for MT_122\n",
      "Added positional encodings to MT_133.\n",
      "MT_133 train interval: from 3148550 on 2014-03-13 16:00:00 to 3152077 on 2014-08-07 15:00:00, 3528 lectures\n",
      "MT_133 eval interval: from 3152078 on 2014-08-07 16:00:00 to 3152661 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_133\n",
      "Scaler min_max persisted for MT_133\n",
      "MT_133 test interval: from 3152327 on 2014-08-18 01:00:00 to 3152829 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_133\n",
      "MT_133 processed. The number of examples in train dataset is 3193\n",
      "MT_133 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_133 was adjusted to 3192\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_133 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_133\n",
      "Persisted eval TFRecord file for MT_133\n",
      "Added positional encodings to MT_160.\n",
      "MT_160 train interval: from 3840858 on 2014-02-04 00:00:00 to 3845289 on 2014-08-07 15:00:00, 4432 lectures\n",
      "MT_160 eval interval: from 3845290 on 2014-08-07 16:00:00 to 3845873 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_160\n",
      "Scaler min_max persisted for MT_160\n",
      "MT_160 test interval: from 3845539 on 2014-08-18 01:00:00 to 3846041 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_160\n",
      "MT_160 processed. The number of examples in train dataset is 4097\n",
      "MT_160 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_160 was adjusted to 4096\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_160 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_160\n",
      "Persisted eval TFRecord file for MT_160\n",
      "Added positional encodings to MT_178.\n",
      "MT_178 train interval: from 4364151 on 2014-07-18 00:00:00 to 4364646 on 2014-08-07 15:00:00, 496 lectures\n",
      "MT_178 eval interval: from 4364647 on 2014-08-07 16:00:00 to 4365230 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_178\n",
      "Scaler min_max persisted for MT_178\n",
      "MT_178 test interval: from 4364896 on 2014-08-18 01:00:00 to 4365398 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_178\n",
      "MT_178 processed. The number of examples in train dataset is 161\n",
      "MT_178 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_178 was adjusted to 160\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_178 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_178\n",
      "Persisted eval TFRecord file for MT_178\n",
      "Added positional encodings to MT_181.\n",
      "MT_181 train interval: from 4412274 on 2014-03-05 00:00:00 to 4416009 on 2014-08-07 15:00:00, 3736 lectures\n",
      "MT_181 eval interval: from 4416010 on 2014-08-07 16:00:00 to 4416593 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_181\n",
      "Scaler min_max persisted for MT_181\n",
      "MT_181 test interval: from 4416259 on 2014-08-18 01:00:00 to 4416761 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_181\n",
      "MT_181 processed. The number of examples in train dataset is 3401\n",
      "MT_181 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_181 was adjusted to 3400\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_181 was adjusted to 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted train TFRecord file for MT_181\n",
      "Persisted eval TFRecord file for MT_181\n",
      "Added positional encodings to MT_337.\n",
      "MT_337 train interval: from 9596915 on 2014-01-17 00:00:00 to 9601778 on 2014-08-07 15:00:00, 4864 lectures\n",
      "MT_337 eval interval: from 9601779 on 2014-08-07 16:00:00 to 9602362 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_337\n",
      "Scaler min_max persisted for MT_337\n",
      "MT_337 test interval: from 9602028 on 2014-08-18 01:00:00 to 9602530 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_337\n",
      "MT_337 processed. The number of examples in train dataset is 4529\n",
      "MT_337 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_337 was adjusted to 4528\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_337 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_337\n",
      "Persisted eval TFRecord file for MT_337\n",
      "Added positional encodings to MT_347.\n",
      "MT_347 train interval: from 9860973 on 2014-02-28 00:00:00 to 9864828 on 2014-08-07 15:00:00, 3856 lectures\n",
      "MT_347 eval interval: from 9864829 on 2014-08-07 16:00:00 to 9865412 on 2014-08-31 23:00:00, 584 lectures\n",
      "Scaler min_max generated on training data for MT_347\n",
      "Scaler min_max persisted for MT_347\n",
      "MT_347 test interval: from 9865078 on 2014-08-18 01:00:00 to 9865580 on 2014-09-07 23:00:00, 503 lectures\n",
      "Test dataset persisted as a time series pickle for MT_347\n",
      "MT_347 processed. The number of examples in train dataset is 3521\n",
      "MT_347 processed. The number of examples in eval dataset is 249\n",
      "For 8 cores in Cloud TPU, the number of train examples for MT_347 was adjusted to 3520\n",
      "For 8 cores in Cloud TPU, the number of eval examples for MT_347 was adjusted to 248\n",
      "Persisted train TFRecord file for MT_347\n",
      "Persisted eval TFRecord file for MT_347\n"
     ]
    }
   ],
   "source": [
    "for customer_id in customer_ids:\n",
    "\n",
    "    # initialize the examples dictionary for each customer\n",
    "    examples = {\n",
    "        'train': [],\n",
    "        'eval': [],\n",
    "        # test dataset is not passed to SLDB\n",
    "        # 'test': []\n",
    "    }\n",
    "    \n",
    "    # use now a reference to the dataframe in the data dictionary \n",
    "    data_df = data[customer_id]\n",
    "    \n",
    "    # a sub-dictionary to keep the number of examples per customer_id, stage\n",
    "    count[customer_id] = dict()\n",
    "\n",
    "    # expand with positional encodings\n",
    "    data_df['sin_hours_from_start'] = np.sin(2*np.pi*data_df.hours_from_start/total_hours)\n",
    "    data_df['cos_hours_from_start'] = np.cos(2*np.pi*data_df.hours_from_start/total_hours)\n",
    "    data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    \n",
    "    print('Added positional encodings to {}.'.format(customer_id))\n",
    "\n",
    "    # get the time series indexes that delimit train, eval, and test intervals\n",
    "    \n",
    "    # train interval goes from the first available lecture (ideally '2014-01-01 00:00:00')\n",
    "    # to '2014-08-07 15:00:00' (ideally 5248 lectures)\n",
    "    train_start_index = data_df[data_df['date'] == pd.to_datetime(train_start_date[customer_id])].index[0]\n",
    "    train_end_index = data_df[data_df['date'] == pd.to_datetime(dates['train']['end'])].index[0]\n",
    "    print('{} train interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "         format(customer_id,\n",
    "                train_start_index, train_start_date[customer_id],\n",
    "                train_end_index, dates['train']['end'],\n",
    "                train_end_index - train_start_index + 1))\n",
    "\n",
    "    eval_start_index = data_df[data_df['date'] == pd.to_datetime(dates['eval']['start'])].index[0]\n",
    "    eval_end_index = data_df[data_df['date'] == pd.to_datetime(dates['eval']['end'])].index[0]\n",
    "    print('{} eval interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "         format(customer_id,\n",
    "                eval_start_index, dates['eval']['start'],\n",
    "                eval_end_index, dates['eval']['end'],\n",
    "                eval_end_index - eval_start_index + 1))\n",
    "\n",
    "    \n",
    "    # get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "    # set up the upper limit of this series based on a fixed date, not on a fixed value!!!\n",
    "    lectures_train_data = data_df['power_usage'].loc[train_start_index:train_end_index]\n",
    "\n",
    "    # fit a scaler only on train data\n",
    "    # it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "    lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)\n",
    "\n",
    "    # get MinMaxScaler on train data, store it in a dictionary\n",
    "    scaler_type = 'min_max'\n",
    "    scaler = MinMaxScaler()\n",
    "    fitted_scaler = scaler.fit(lectures_train_data_array)\n",
    "    print('Scaler {} generated on training data for {}'.format(scaler_type, customer_id))\n",
    "    \n",
    "    # persist the scaler\n",
    "    scaler_filename = '{}/{}_{}.save'.format(scalers_dir, scaler_type, customer_id)\n",
    "    joblib.dump(fitted_scaler, scaler_filename)\n",
    "    print('Scaler {} persisted for {}'.format(scaler_type, customer_id))\n",
    "\n",
    "    # get an array from the variable time series (seen and unseen)\n",
    "    all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)\n",
    "\n",
    "    # apply the scaler over all data (seen and unseen)\n",
    "    # rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "    data_df['kw_scaled'] = np.squeeze(fitted_scaler.transform(all_data_variable_array))\n",
    "\n",
    "    # at this moment, the individual time series are ready to be window-rolled to produce\n",
    "    # sub-series/examples to serialize\n",
    "\n",
    "    # BSCTRFM inference process is not direct, but iterative, therefore\n",
    "    # no TFRecord SLDB is required for test dataset,\n",
    "\n",
    "    test_start_index = data_df[data_df['date'] == pd.to_datetime(dates['test']['start'])].index[0]\n",
    "    test_end_index = data_df[data_df['date'] == pd.to_datetime(dates['test']['end'])].index[0]\n",
    "    \n",
    "    # the time series used to build the test dataset must go\n",
    "    # from '2014-08-18 01:00:00' to '2014-09-07 23:00:00'\n",
    "    # in order to extract 168 features with targets\n",
    "    # (the last element in the decoder output)\n",
    "    # ranging from '2014-09-01 00:00:00' to '2014-09-07 23:00:00'\n",
    "\n",
    "    # therefore\n",
    "    test_ts_start_index = data_df[data_df['date'] == pd.to_datetime('2014-08-18 01:00:00')].index[0]\n",
    "    \n",
    "    # persist only the time series corresponding to the inference interval as test dataset\n",
    "    test_time_series = data_df[sldb_columns].loc[test_ts_start_index:test_end_index]\n",
    "\n",
    "    print('{} test interval: from {} on {} to {} on {}, {} lectures'.\\\n",
    "         format(customer_id,\n",
    "                test_ts_start_index, '2014-08-18 01:00:00',\n",
    "                test_end_index, dates['test']['end'],\n",
    "                test_end_index - test_ts_start_index + 1))\n",
    "\n",
    "    # path to persist the time series dataframe corresponding to test dataset\n",
    "    path = '{}/test/{}.pkl'.format(sldb_dir, customer_id)\n",
    "\n",
    "    test_time_series.to_pickle(path)\n",
    "    print('Test dataset persisted as a time series pickle for {}'.format(customer_id))\n",
    "    \n",
    "    # make SLDB training dataset\n",
    "    # get an iterable with all the possible sub-series for training examples\n",
    "    train_starting_indexes = np.arange(train_start_index, train_end_index - (m + t) + 2)\n",
    "    \n",
    "    for train_starting_index in train_starting_indexes:\n",
    "        \n",
    "        # substract 1 at the end of the slice because loc works different from direct slicing!\n",
    "        sub_series_df = data_df[sldb_columns].loc[train_starting_index:train_starting_index + (m + t) - 1]\n",
    "        \n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "        \n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        \n",
    "        examples['train'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'train', len(examples['train'])))\n",
    "    \n",
    "    \n",
    "    # make SLDB evaluation dataset\n",
    "    # get an iterable with all the possible sub-series for evaluation examples\n",
    "    eval_starting_indexes = np.arange(eval_start_index, eval_end_index - (m + t) + 2)\n",
    "    \n",
    "    for eval_starting_index in eval_starting_indexes:\n",
    "        \n",
    "        # substract 1 at the end of the slice because loc works different from direct slicing!\n",
    "        sub_series_df = data_df[sldb_columns].loc[eval_starting_index:eval_starting_index + (m + t) - 1]\n",
    "        \n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "        \n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        \n",
    "        examples['eval'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'eval', len(examples['eval'])))\n",
    "   \n",
    "    # DO NOT PRODUCE A TEST DATASET FOR SLDB, AS INFERENCE PROCESS IS NOT DIRECT\n",
    "    # (IT IS ITERATIVE OVER UNSEEN DATA TIME SERIES, ALREADY PERSISTED AS A PICKLE FILE)\n",
    "\n",
    "    # on each customer dataset, adjust the number of examples to the number of training cores\n",
    "    for stage in ['train', 'eval']:\n",
    "        # how many examples/rows must be removed from examples[stage] to comply with the number of cores\n",
    "        examples_to_remove = len(examples[stage])%num_cores\n",
    "\n",
    "        # remove the last 'examples_to_remove' examples from the dataset\n",
    "        for _ in np.arange(examples_to_remove):\n",
    "            examples[stage].pop(-1)\n",
    "\n",
    "        \n",
    "        # keep a record of the number of training and evaluation examples\n",
    "        count[customer_id][stage] = len(examples[stage])\n",
    "        print('For {} cores in Cloud TPU, the number of {} examples for {} was adjusted to {}'.\\\n",
    "             format(num_cores, stage, customer_id, len(examples[stage])))\n",
    "\n",
    "        \n",
    "    # serialize the rows in examples['train'] and, if present, examples['eval']\n",
    "    # process each customer, then release data structures to avoid excesive memory consumption\n",
    "\n",
    "    # write a TFRecord file for each consumer_id/stage\n",
    "    for stage in ['train', 'eval']:\n",
    "        # N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "        N_ROWS = len(examples[stage])\n",
    "        filename = '{}/{}/{}.tfrecord'.format(sldb_dir, stage, customer_id)\n",
    "\n",
    "        with tf.io.TFRecordWriter(filename) as writer:\n",
    "            for row in np.arange(N_ROWS):\n",
    "\n",
    "                example = tf.train.Example(\n",
    "                    # features within the example\n",
    "                    features=tf.train.Features(\n",
    "                        # individual feature definition\n",
    "                        feature={'encoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['encoder_input']),\n",
    "                                 'decoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['decoder_input']),\n",
    "                                 'target':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['target']),\n",
    "                                 'id':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['id'])\n",
    "                                 }\n",
    "                    )\n",
    "                )\n",
    "                serialized_example = example.SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "\n",
    "            # report TFRecord file as completed\n",
    "            print('Persisted {} TFRecord file for {}'.format(stage, customer_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MT_106': {'train': 4600, 'eval': 248},\n",
       " 'MT_066': {'train': 216, 'eval': 248},\n",
       " 'MT_107': {'train': 4600, 'eval': 248},\n",
       " 'MT_108': {'train': 4600, 'eval': 248},\n",
       " 'MT_109': {'train': 3760, 'eval': 248},\n",
       " 'MT_110': {'train': 4600, 'eval': 248},\n",
       " 'MT_111': {'train': 4600, 'eval': 248},\n",
       " 'MT_112': {'train': 3904, 'eval': 248},\n",
       " 'MT_113': {'train': 4600, 'eval': 248},\n",
       " 'MT_115': {'train': 4600, 'eval': 248},\n",
       " 'MT_116': {'train': 3760, 'eval': 248},\n",
       " 'MT_117': {'train': 4600, 'eval': 248},\n",
       " 'MT_120': {'train': 4600, 'eval': 248},\n",
       " 'MT_121': {'train': 4600, 'eval': 248},\n",
       " 'MT_122': {'train': 4600, 'eval': 248},\n",
       " 'MT_133': {'train': 3192, 'eval': 248},\n",
       " 'MT_160': {'train': 4096, 'eval': 248},\n",
       " 'MT_178': {'train': 160, 'eval': 248},\n",
       " 'MT_181': {'train': 3400, 'eval': 248},\n",
       " 'MT_337': {'train': 4528, 'eval': 248},\n",
       " 'MT_347': {'train': 3520, 'eval': 248}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dataframe to keep track of training examples count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_list = list()\n",
    "\n",
    "for customer_id in count.keys():\n",
    "    buffer_list.append([customer_id, count[customer_id]['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df = pd.DataFrame(buffer_list, columns=['customer_id', 'train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df = trimmed_df.set_index('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df = trimmed_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_customer_ids = ['MT_{:03d}'.format(token_id) for token_id in np.arange(1, 370 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular_token_ids = [\n",
    "    66, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116,\n",
    "    117, 120, 121, 122, 133, 160, 178, 181, 223, 337, 347\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular_customer_ids = ['MT_{:03d}'.format(token_id) for token_id in irregular_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_customer_ids = list(set(all_customer_ids).difference(set(irregular_customer_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regular_customer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_list = list()\n",
    "\n",
    "for customer_id in regular_customer_ids:\n",
    "    buffer_list.append([customer_id, 4912])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_df = pd.DataFrame(buffer_list, columns=['customer_id', 'train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_df = regular_df.set_index('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_df = regular_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT_001</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_002</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_003</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_004</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_005</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_366</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_367</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_368</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_369</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_370</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             train\n",
       "customer_id       \n",
       "MT_001        4912\n",
       "MT_002        4912\n",
       "MT_003        4912\n",
       "MT_004        4912\n",
       "MT_005        4912\n",
       "...            ...\n",
       "MT_366        4912\n",
       "MT_367        4912\n",
       "MT_368        4912\n",
       "MT_369        4912\n",
       "MT_370        4912\n",
       "\n",
       "[348 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_df = pd.concat([trimmed_df, regular_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_df = trainable_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT_001</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_002</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_003</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_004</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_005</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_366</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_367</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_368</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_369</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_370</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             train\n",
       "customer_id       \n",
       "MT_001        4912\n",
       "MT_002        4912\n",
       "MT_003        4912\n",
       "MT_004        4912\n",
       "MT_005        4912\n",
       "...            ...\n",
       "MT_366        4912\n",
       "MT_367        4912\n",
       "MT_368        4912\n",
       "MT_369        4912\n",
       "MT_370        4912\n",
       "\n",
       "[369 rows x 1 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainable_df.loc['MT_001':'MT_370']['train']) == np.sum(regular_df['train']) + np.sum(trimmed_df['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the dataframe\n",
    "trainable_df.to_pickle('/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX/example_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_df = pd.read_pickle(\n",
    "    '/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_FULL_BSCTRFM_168_168_07DB_MMX/example_count.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MT_001</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_002</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_003</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_004</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_005</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_006</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_007</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_008</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_009</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_010</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_011</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_012</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_013</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_014</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_015</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_016</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_017</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_018</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_019</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_020</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_021</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_022</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_023</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_024</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_025</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_026</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_027</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_028</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_029</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_030</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_031</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_032</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_033</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_034</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_035</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_036</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_037</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_038</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_039</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_040</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_041</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_042</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_043</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_044</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_045</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_046</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_047</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_048</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_049</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_050</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_051</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_052</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_053</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_054</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_055</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_056</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_057</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_058</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_059</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_060</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_061</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_062</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_063</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_064</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_065</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_066</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_067</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_068</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_069</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_070</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_071</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_072</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_073</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_074</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_075</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_076</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_077</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_078</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_079</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_080</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_081</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_082</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_083</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_084</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_085</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_086</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_087</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_088</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_089</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_090</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_091</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_092</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_093</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_094</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_095</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_096</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_097</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_098</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_099</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_100</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_101</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_102</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_103</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_104</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_105</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_106</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_107</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_108</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_109</th>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_110</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_111</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_112</th>\n",
       "      <td>3904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_113</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_114</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_115</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_116</th>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_117</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_118</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_119</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_120</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_121</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_122</th>\n",
       "      <td>4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_123</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_124</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_125</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_126</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_127</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_128</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_129</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_130</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_131</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_132</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_133</th>\n",
       "      <td>3192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_134</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_135</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_136</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_137</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_138</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_139</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_140</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_141</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_142</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_143</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_144</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_145</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_146</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_147</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_148</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_149</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_150</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_151</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_152</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_153</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_154</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_155</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_156</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_157</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_158</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_159</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_160</th>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_161</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_162</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_163</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_164</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_165</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_166</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_167</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_168</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_169</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_170</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_171</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_172</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_173</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_174</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_175</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_176</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_177</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_178</th>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_179</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_180</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_181</th>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_182</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_183</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_184</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_185</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_186</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_187</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_188</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_189</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_190</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_191</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_192</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_193</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_194</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_195</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_196</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_197</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_198</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_199</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_200</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_201</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_202</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_203</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_204</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_205</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_206</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_207</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_208</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_209</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_210</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_211</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_212</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_213</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_214</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_215</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_216</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_217</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_218</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_219</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_220</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_221</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_222</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_224</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_225</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_226</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_227</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_228</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_229</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_230</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_231</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_232</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_233</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_234</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_235</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_236</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_237</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_238</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_239</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_240</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_241</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_242</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_243</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_244</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_245</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_246</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_247</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_248</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_249</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_250</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_251</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_252</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_253</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_254</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_255</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_256</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_257</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_258</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_259</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_260</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_261</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_262</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_263</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_264</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_265</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_266</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_267</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_268</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_269</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_270</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_271</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_272</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_273</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_274</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_275</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_276</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_277</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_278</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_279</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_280</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_281</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_282</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_283</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_284</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_285</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_286</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_287</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_288</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_289</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_290</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_291</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_292</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_293</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_294</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_295</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_296</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_297</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_298</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_299</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_300</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_301</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_302</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_303</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_304</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_305</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_306</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_307</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_308</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_309</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_310</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_311</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_312</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_313</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_314</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_315</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_316</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_317</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_318</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_319</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_320</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_321</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_322</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_323</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_324</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_325</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_326</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_327</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_328</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_329</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_330</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_331</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_332</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_333</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_334</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_335</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_336</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_337</th>\n",
       "      <td>4528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_338</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_339</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_340</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_341</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_342</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_343</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_344</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_345</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_346</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_347</th>\n",
       "      <td>3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_348</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_349</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_350</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_351</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_352</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_353</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_354</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_355</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_356</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_357</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_358</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_359</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_360</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_361</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_362</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_363</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_364</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_365</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_366</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_367</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_368</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_369</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT_370</th>\n",
       "      <td>4912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train\n",
       "customer_id       \n",
       "MT_001        4912\n",
       "MT_002        4912\n",
       "MT_003        4912\n",
       "MT_004        4912\n",
       "MT_005        4912\n",
       "MT_006        4912\n",
       "MT_007        4912\n",
       "MT_008        4912\n",
       "MT_009        4912\n",
       "MT_010        4912\n",
       "MT_011        4912\n",
       "MT_012        4912\n",
       "MT_013        4912\n",
       "MT_014        4912\n",
       "MT_015        4912\n",
       "MT_016        4912\n",
       "MT_017        4912\n",
       "MT_018        4912\n",
       "MT_019        4912\n",
       "MT_020        4912\n",
       "MT_021        4912\n",
       "MT_022        4912\n",
       "MT_023        4912\n",
       "MT_024        4912\n",
       "MT_025        4912\n",
       "MT_026        4912\n",
       "MT_027        4912\n",
       "MT_028        4912\n",
       "MT_029        4912\n",
       "MT_030        4912\n",
       "MT_031        4912\n",
       "MT_032        4912\n",
       "MT_033        4912\n",
       "MT_034        4912\n",
       "MT_035        4912\n",
       "MT_036        4912\n",
       "MT_037        4912\n",
       "MT_038        4912\n",
       "MT_039        4912\n",
       "MT_040        4912\n",
       "MT_041        4912\n",
       "MT_042        4912\n",
       "MT_043        4912\n",
       "MT_044        4912\n",
       "MT_045        4912\n",
       "MT_046        4912\n",
       "MT_047        4912\n",
       "MT_048        4912\n",
       "MT_049        4912\n",
       "MT_050        4912\n",
       "MT_051        4912\n",
       "MT_052        4912\n",
       "MT_053        4912\n",
       "MT_054        4912\n",
       "MT_055        4912\n",
       "MT_056        4912\n",
       "MT_057        4912\n",
       "MT_058        4912\n",
       "MT_059        4912\n",
       "MT_060        4912\n",
       "MT_061        4912\n",
       "MT_062        4912\n",
       "MT_063        4912\n",
       "MT_064        4912\n",
       "MT_065        4912\n",
       "MT_066         216\n",
       "MT_067        4912\n",
       "MT_068        4912\n",
       "MT_069        4912\n",
       "MT_070        4912\n",
       "MT_071        4912\n",
       "MT_072        4912\n",
       "MT_073        4912\n",
       "MT_074        4912\n",
       "MT_075        4912\n",
       "MT_076        4912\n",
       "MT_077        4912\n",
       "MT_078        4912\n",
       "MT_079        4912\n",
       "MT_080        4912\n",
       "MT_081        4912\n",
       "MT_082        4912\n",
       "MT_083        4912\n",
       "MT_084        4912\n",
       "MT_085        4912\n",
       "MT_086        4912\n",
       "MT_087        4912\n",
       "MT_088        4912\n",
       "MT_089        4912\n",
       "MT_090        4912\n",
       "MT_091        4912\n",
       "MT_092        4912\n",
       "MT_093        4912\n",
       "MT_094        4912\n",
       "MT_095        4912\n",
       "MT_096        4912\n",
       "MT_097        4912\n",
       "MT_098        4912\n",
       "MT_099        4912\n",
       "MT_100        4912\n",
       "MT_101        4912\n",
       "MT_102        4912\n",
       "MT_103        4912\n",
       "MT_104        4912\n",
       "MT_105        4912\n",
       "MT_106        4600\n",
       "MT_107        4600\n",
       "MT_108        4600\n",
       "MT_109        3760\n",
       "MT_110        4600\n",
       "MT_111        4600\n",
       "MT_112        3904\n",
       "MT_113        4600\n",
       "MT_114        4912\n",
       "MT_115        4600\n",
       "MT_116        3760\n",
       "MT_117        4600\n",
       "MT_118        4912\n",
       "MT_119        4912\n",
       "MT_120        4600\n",
       "MT_121        4600\n",
       "MT_122        4600\n",
       "MT_123        4912\n",
       "MT_124        4912\n",
       "MT_125        4912\n",
       "MT_126        4912\n",
       "MT_127        4912\n",
       "MT_128        4912\n",
       "MT_129        4912\n",
       "MT_130        4912\n",
       "MT_131        4912\n",
       "MT_132        4912\n",
       "MT_133        3192\n",
       "MT_134        4912\n",
       "MT_135        4912\n",
       "MT_136        4912\n",
       "MT_137        4912\n",
       "MT_138        4912\n",
       "MT_139        4912\n",
       "MT_140        4912\n",
       "MT_141        4912\n",
       "MT_142        4912\n",
       "MT_143        4912\n",
       "MT_144        4912\n",
       "MT_145        4912\n",
       "MT_146        4912\n",
       "MT_147        4912\n",
       "MT_148        4912\n",
       "MT_149        4912\n",
       "MT_150        4912\n",
       "MT_151        4912\n",
       "MT_152        4912\n",
       "MT_153        4912\n",
       "MT_154        4912\n",
       "MT_155        4912\n",
       "MT_156        4912\n",
       "MT_157        4912\n",
       "MT_158        4912\n",
       "MT_159        4912\n",
       "MT_160        4096\n",
       "MT_161        4912\n",
       "MT_162        4912\n",
       "MT_163        4912\n",
       "MT_164        4912\n",
       "MT_165        4912\n",
       "MT_166        4912\n",
       "MT_167        4912\n",
       "MT_168        4912\n",
       "MT_169        4912\n",
       "MT_170        4912\n",
       "MT_171        4912\n",
       "MT_172        4912\n",
       "MT_173        4912\n",
       "MT_174        4912\n",
       "MT_175        4912\n",
       "MT_176        4912\n",
       "MT_177        4912\n",
       "MT_178         160\n",
       "MT_179        4912\n",
       "MT_180        4912\n",
       "MT_181        3400\n",
       "MT_182        4912\n",
       "MT_183        4912\n",
       "MT_184        4912\n",
       "MT_185        4912\n",
       "MT_186        4912\n",
       "MT_187        4912\n",
       "MT_188        4912\n",
       "MT_189        4912\n",
       "MT_190        4912\n",
       "MT_191        4912\n",
       "MT_192        4912\n",
       "MT_193        4912\n",
       "MT_194        4912\n",
       "MT_195        4912\n",
       "MT_196        4912\n",
       "MT_197        4912\n",
       "MT_198        4912\n",
       "MT_199        4912\n",
       "MT_200        4912\n",
       "MT_201        4912\n",
       "MT_202        4912\n",
       "MT_203        4912\n",
       "MT_204        4912\n",
       "MT_205        4912\n",
       "MT_206        4912\n",
       "MT_207        4912\n",
       "MT_208        4912\n",
       "MT_209        4912\n",
       "MT_210        4912\n",
       "MT_211        4912\n",
       "MT_212        4912\n",
       "MT_213        4912\n",
       "MT_214        4912\n",
       "MT_215        4912\n",
       "MT_216        4912\n",
       "MT_217        4912\n",
       "MT_218        4912\n",
       "MT_219        4912\n",
       "MT_220        4912\n",
       "MT_221        4912\n",
       "MT_222        4912\n",
       "MT_224        4912\n",
       "MT_225        4912\n",
       "MT_226        4912\n",
       "MT_227        4912\n",
       "MT_228        4912\n",
       "MT_229        4912\n",
       "MT_230        4912\n",
       "MT_231        4912\n",
       "MT_232        4912\n",
       "MT_233        4912\n",
       "MT_234        4912\n",
       "MT_235        4912\n",
       "MT_236        4912\n",
       "MT_237        4912\n",
       "MT_238        4912\n",
       "MT_239        4912\n",
       "MT_240        4912\n",
       "MT_241        4912\n",
       "MT_242        4912\n",
       "MT_243        4912\n",
       "MT_244        4912\n",
       "MT_245        4912\n",
       "MT_246        4912\n",
       "MT_247        4912\n",
       "MT_248        4912\n",
       "MT_249        4912\n",
       "MT_250        4912\n",
       "MT_251        4912\n",
       "MT_252        4912\n",
       "MT_253        4912\n",
       "MT_254        4912\n",
       "MT_255        4912\n",
       "MT_256        4912\n",
       "MT_257        4912\n",
       "MT_258        4912\n",
       "MT_259        4912\n",
       "MT_260        4912\n",
       "MT_261        4912\n",
       "MT_262        4912\n",
       "MT_263        4912\n",
       "MT_264        4912\n",
       "MT_265        4912\n",
       "MT_266        4912\n",
       "MT_267        4912\n",
       "MT_268        4912\n",
       "MT_269        4912\n",
       "MT_270        4912\n",
       "MT_271        4912\n",
       "MT_272        4912\n",
       "MT_273        4912\n",
       "MT_274        4912\n",
       "MT_275        4912\n",
       "MT_276        4912\n",
       "MT_277        4912\n",
       "MT_278        4912\n",
       "MT_279        4912\n",
       "MT_280        4912\n",
       "MT_281        4912\n",
       "MT_282        4912\n",
       "MT_283        4912\n",
       "MT_284        4912\n",
       "MT_285        4912\n",
       "MT_286        4912\n",
       "MT_287        4912\n",
       "MT_288        4912\n",
       "MT_289        4912\n",
       "MT_290        4912\n",
       "MT_291        4912\n",
       "MT_292        4912\n",
       "MT_293        4912\n",
       "MT_294        4912\n",
       "MT_295        4912\n",
       "MT_296        4912\n",
       "MT_297        4912\n",
       "MT_298        4912\n",
       "MT_299        4912\n",
       "MT_300        4912\n",
       "MT_301        4912\n",
       "MT_302        4912\n",
       "MT_303        4912\n",
       "MT_304        4912\n",
       "MT_305        4912\n",
       "MT_306        4912\n",
       "MT_307        4912\n",
       "MT_308        4912\n",
       "MT_309        4912\n",
       "MT_310        4912\n",
       "MT_311        4912\n",
       "MT_312        4912\n",
       "MT_313        4912\n",
       "MT_314        4912\n",
       "MT_315        4912\n",
       "MT_316        4912\n",
       "MT_317        4912\n",
       "MT_318        4912\n",
       "MT_319        4912\n",
       "MT_320        4912\n",
       "MT_321        4912\n",
       "MT_322        4912\n",
       "MT_323        4912\n",
       "MT_324        4912\n",
       "MT_325        4912\n",
       "MT_326        4912\n",
       "MT_327        4912\n",
       "MT_328        4912\n",
       "MT_329        4912\n",
       "MT_330        4912\n",
       "MT_331        4912\n",
       "MT_332        4912\n",
       "MT_333        4912\n",
       "MT_334        4912\n",
       "MT_335        4912\n",
       "MT_336        4912\n",
       "MT_337        4528\n",
       "MT_338        4912\n",
       "MT_339        4912\n",
       "MT_340        4912\n",
       "MT_341        4912\n",
       "MT_342        4912\n",
       "MT_343        4912\n",
       "MT_344        4912\n",
       "MT_345        4912\n",
       "MT_346        4912\n",
       "MT_347        3520\n",
       "MT_348        4912\n",
       "MT_349        4912\n",
       "MT_350        4912\n",
       "MT_351        4912\n",
       "MT_352        4912\n",
       "MT_353        4912\n",
       "MT_354        4912\n",
       "MT_355        4912\n",
       "MT_356        4912\n",
       "MT_357        4912\n",
       "MT_358        4912\n",
       "MT_359        4912\n",
       "MT_360        4912\n",
       "MT_361        4912\n",
       "MT_362        4912\n",
       "MT_363        4912\n",
       "MT_364        4912\n",
       "MT_365        4912\n",
       "MT_366        4912\n",
       "MT_367        4912\n",
       "MT_368        4912\n",
       "MT_369        4912\n",
       "MT_370        4912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486504"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainable_df.loc['MT_001':'MT_100']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962160"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainable_df.loc['MT_001':'MT_200']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainable_df.loc['MT_001':'MT_370']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
