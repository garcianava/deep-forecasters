{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for prediction and evaluation of multi-step forecasting DMSLSTM models\n",
    "# for the most recent training system, based on a single JSON configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line for compatibility with TensorFlow 1.15 (on GCP)\n",
    "# import tensorflow.compat.v1 as tf\n",
    "\n",
    "# uncomment the following line for TensorFlow 2.X (local execution)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "# therefore, get and test the appropriate function\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical mean absolute percentage error\n",
    "def smape(targets, predictions):\n",
    "    '''\n",
    "    predictions: a list with the predicted values\n",
    "    targets: a list with the actual values\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # lists to NumPy arrays\n",
    "    targets, predictions = np.array(targets), np.array(predictions)\n",
    "    # verify predictions and targets have the same shape\n",
    "    if predictions.shape == targets.shape:\n",
    "            return(np.sum(2*np.abs(predictions - targets) /\n",
    "                          (np.abs(targets) + np.abs(predictions)))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference process is driven by the saved model, the corresponding SLDB and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will be imported as:\n",
    "# from dmslstm.data import _parse_dataset_function\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shapes['daily'])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shapes['weekly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    # do not parse the timestamp to TPUEstimator, as it does not support string types!\n",
    "    # ToDo: code timestamps into features, as numbers\n",
    "    #  so they can be parsed for training, if needed later\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'hourly': hourly,\n",
    "        'daily': daily,\n",
    "        'weekly': weekly,\n",
    "        'oh_wd': oh_wd,\n",
    "        'oh_dh': oh_dh,\n",
    "    }\n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    return feature_dict, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the model identifier is obtained via Abseil Flags\n",
    "# remember this notebook is based on local execution,\n",
    "# therefore model directory must be downloaded from GS before running the notebook\n",
    "model_id = 'DMSLSTM_TPU_007'\n",
    "\n",
    "# during batch prediction, the SLDB identifier is obtained via Abseil Flags\n",
    "# THE SLDB FOR INFERENCE MUST BE THE SAME USED FOR TRAINING! (THE ONE SETUP IN THE CONFIGURATION FILE)\n",
    "sldb_id = 'CPE04115_H_kw_20210526212214_008001_008024_008168_048'\n",
    "\n",
    "# during batch prediction, the dataset name is obtained via Abseil Flags\n",
    "dataset = 'test'\n",
    "\n",
    "# ADD AN INFERENCE IDENTIFIER, BECAUSE FOR ARTRFDC MODELS, DIFFERENT INFERENCES\n",
    "# CAN BE PRODUCED FROM A SINGLE SAVED MODEL (USUALLY DIFFERENT FORECAST WINDOWS)\n",
    "# during batch prediction, the inference identifier should be obtained via Abseil Flags\n",
    "inference = '048'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run inference on model_id and dataset for given executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_00/export/exporter/1623433248\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_00_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_00_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_01/export/exporter/1623433421\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_01_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_01_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_02/export/exporter/1623433603\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_02_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_02_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_03/export/exporter/1623433780\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_03_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_03_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_04/export/exporter/1623433968\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_04_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_04_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_05/export/exporter/1623434146\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_05_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_05_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_06/export/exporter/1623434322\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_06_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_06_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_07/export/exporter/1623434496\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_07_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_07_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_08/export/exporter/1623434672\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_08_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_08_test_048\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/DMSLSTM_TPU_007_09/export/exporter/1623434845\n",
      "Scaler loaded for time series CPE04115_H_kw_20210526212214 and test dataset\n",
      "Number of rows in the test dataset is 817.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of DMSLSTM_TPU_007_09_test_048\n",
      "Persisted Pandas dataframe for predictions summary of DMSLSTM_TPU_007_09_test_048\n"
     ]
    }
   ],
   "source": [
    "# during batch prediction, the execution identifier is obtained via Abseil Flags\n",
    "for execution in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    \n",
    "    # use model identifier and execution number to build the model directory string\n",
    "    model_dir = '{}_{:02d}'.format(model_id, execution)\n",
    "\n",
    "    # get the path to the saved model main directory\n",
    "    saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                         'models',\n",
    "                                                         model_dir)\n",
    "\n",
    "    # get all the files in the saved model path, to find the most recent one\n",
    "    all_files = os.listdir(saved_model_path)\n",
    "    # get the path to the most recent saved model\n",
    "    latest_saved_model_id = sorted(all_files)[-1]\n",
    "    # build the full path for the latest saved model dir\n",
    "    export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "    print ('Exported model path is {}'.format(export_dir))\n",
    "\n",
    "    # load the saved model and the prediction function\n",
    "    imported = load(export_dir=export_dir, tags='serve')\n",
    "    predict_fn = imported.signatures[\"serving_default\"]\n",
    "\n",
    "    # build a path to the sldb directory\n",
    "    data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)\n",
    "\n",
    "    # then get the ts_identifier from the json file in the sldb directory\n",
    "    sldb_json_file = '{}/sldb.json'.format(data_dir)\n",
    "\n",
    "    # open the json file\n",
    "    with open(sldb_json_file, 'r') as inputfile:\n",
    "        sldb_dict = json.load(inputfile)\n",
    "\n",
    "    # and get the time series identifier\n",
    "    ts_identifier = sldb_dict['ts']\n",
    "\n",
    "    # use the time series identifier to obtain the SK-Learn scaler used on it,\n",
    "    # remember this is usually the scaler for the test dataset (unseen data)\n",
    "    scaler = joblib.load('{}/{}/{}/scaler_{}.save'.format(PROJECT_ROOT,\n",
    "                                                          'timeseries',\n",
    "                                                          ts_identifier,\n",
    "                                                          dataset))\n",
    "\n",
    "    print('Scaler loaded for time series {} and {} dataset'.format(ts_identifier, dataset))\n",
    "\n",
    "    # build a path to the dataset for prediction\n",
    "    dataset_path = '{}/{}.tfrecord'.format(data_dir, dataset)\n",
    "    # load the dataset\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)\n",
    "\n",
    "    # a list to store prediction values\n",
    "    predictions_list = list()\n",
    "\n",
    "    # TensorFlow 2 eager execution allows to iterate over a dataset\n",
    "    for element in tfrecord_dataset:\n",
    "        predictions_list.append(predict_fn(element))\n",
    "\n",
    "    # get prediction values from predictions list\n",
    "    predictions = [p['forecast'][0] for p in predictions_list]\n",
    "\n",
    "    # pass predictions to a NumPy array\n",
    "    predictions_array = np.asarray(predictions)\n",
    "\n",
    "    # array from shape (rows, timesteps, 1) to (rows, timesteps)\n",
    "    predictions_array = np.squeeze(predictions_array)\n",
    "\n",
    "    # inverse-scale predictions\n",
    "    rescaled_predictions = scaler.inverse_transform(predictions_array)\n",
    "\n",
    "    # temporarily skip JSON serialization of predictions and targets for multistep forecasting\n",
    "\n",
    "    # get the SLDB parameters for the forecasting model\n",
    "    # parameters_json_file = '{}/{}/{}/sldb_parameters.json'.format(PROJECT_ROOT,\n",
    "    #                                                               'parameters',\n",
    "    #                                                               model_id)\n",
    "    \n",
    "    # get the SLDB parameters from the JSON configuration file\n",
    "    config_json_file = '{}/{}/{}.json'.format(PROJECT_ROOT,\n",
    "                                              'parameters',\n",
    "                                              model_id)    \n",
    "\n",
    "    # recover the sldb dictionary from the json file in parameters/\n",
    "    # with open(parameters_json_file, 'r') as inputfile:\n",
    "    #     sldb_parameters = json.load(inputfile)\n",
    "\n",
    "    # recover the sldb dictionary from the json file in parameters/\n",
    "    with open(config_json_file, 'r') as inputfile:\n",
    "        configuration = json.load(inputfile)\n",
    "    \n",
    "    # store the objective shapes for reshaping tensors in a dictionary\n",
    "    _EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "        'hourly': [configuration['embedding']['hourly'], 1],\n",
    "        'daily': [configuration['embedding']['daily'], 1],\n",
    "        'weekly': [configuration['embedding']['weekly'], 1],\n",
    "        'target': [configuration['no_targets'], 1],\n",
    "        'oh_wd': [7, 1],  # Monday to Sunday\n",
    "        'oh_dh': [24, 1],  # midnight to 23:00\n",
    "        'timestamp': [configuration['no_targets'], 1]\n",
    "    }\n",
    "\n",
    "    # test_dataset was previously acquired from tfrecord file\n",
    "    # use it again to build arrays for targets and timestamps\n",
    "    parsed_dataset = tfrecord_dataset.map(\n",
    "        lambda row: _parse_dataset_function(\n",
    "            example_proto=row,\n",
    "            objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "            parse_timestamp=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # a list to store the string_timestamps\n",
    "    string_timestamps_list = list()\n",
    "\n",
    "    # a list to store the targets\n",
    "    targets_list = list()\n",
    "\n",
    "    # get string_timestamps and targets associated to the predictions previously served\n",
    "    for parsed_example in parsed_dataset:\n",
    "        string_timestamps = np.squeeze(np.asarray(parsed_example[0]['timestamp']).astype(str))\n",
    "        string_timestamps_list.append(string_timestamps)\n",
    "        targets = np.squeeze(parsed_example[1])\n",
    "        targets_list.append(targets)\n",
    "\n",
    "\n",
    "    # get the number of rows in the dataset for prediction\n",
    "    length = configuration['total_{}_rows'.format(dataset)]\n",
    "    print('Number of rows in the {} dataset is {}.'.format(dataset, length))\n",
    "\n",
    "    # confirm all string_timestamps were loaded\n",
    "    print('Loaded all string timestamps: {}'.format(\n",
    "        len(string_timestamps_list) == length)\n",
    "    )\n",
    "\n",
    "    # confirm all targets were loaded\n",
    "    print('Loaded all targets: {}'.format(\n",
    "        len(targets_list) == length)\n",
    "    )\n",
    "\n",
    "    # targets to array\n",
    "    targets_array = np.asarray(targets_list)\n",
    "\n",
    "    # rescale the targets\n",
    "    rescaled_targets = scaler.inverse_transform(targets_array)\n",
    "\n",
    "    # a columns list for the predictions dataframe\n",
    "    pred_df_columns = ['model_id',\n",
    "                       'execution',\n",
    "                       'dataset',\n",
    "                       'inference',\n",
    "                       'string_timestamps',\n",
    "                       'predictions',\n",
    "                       'targets']\n",
    "\n",
    "    # a list with model_id repeated length times, to populate the predictions detail dataframe\n",
    "    model_id_repeat_list = [model_id]*length\n",
    "    # same for execution\n",
    "    execution_repeat_list = [execution]*length\n",
    "    # same for dataset\n",
    "    dataset_repeat_list = [dataset]*length\n",
    "    # same for the inference identifier\n",
    "    inference_repeat_list = [inference]*length\n",
    "\n",
    "    # predictions dataframe\n",
    "    predictions_detail_df = pd.DataFrame(list(zip(model_id_repeat_list,\n",
    "                                                  execution_repeat_list,\n",
    "                                                  dataset_repeat_list,\n",
    "                                                  inference_repeat_list,\n",
    "                                                  # from 2D NumPy array to list of 1D arrays\n",
    "                                                  string_timestamps_list,\n",
    "                                                  # from 2D NumPy array to list of 1D arrays\n",
    "                                                  rescaled_predictions.tolist(),\n",
    "                                                  rescaled_targets.tolist())), columns=pred_df_columns)\n",
    "\n",
    "    # complement the detailed predictions dataframe with mae, rmse, smape\n",
    "    # row by row, will be averaged at model-execution level, later...\n",
    "\n",
    "    # a list with MAE, evaluated row by row\n",
    "    predictions_detail_df['mae'] = [mean_absolute_error(row.targets, row.predictions) \\\n",
    "                                    for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # a list with RMSE, evaluated row by row\n",
    "    predictions_detail_df['rmse'] = [sqrt(mean_squared_error(row.targets, row.predictions)) \\\n",
    "                                     for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # a list with SMAPE, evaluated row by row\n",
    "    predictions_detail_df['smape'] = [smape(row.targets, row.predictions) \\\n",
    "                                      for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # build a predictions summary dataframe, reset index to avoid making a multi-column index when grouping by\n",
    "    predictions_summary_df = predictions_detail_df.groupby(['model_id',\n",
    "                                                            'execution',\n",
    "                                                            'dataset',\n",
    "                                                            'inference']).mean().reset_index()\n",
    "\n",
    "    # a range to iterate on prediction timesteps\n",
    "    targets_range = np.arange(configuration['no_targets'])\n",
    "\n",
    "    # vector metric (vector component to vector component)\n",
    "    # an array no_targets-d: metric for 1, 2,..., no_targets step-ahead (target versus prediction for rows in dataset)\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    mae_vector = [\n",
    "        mean_absolute_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['mae_vector'] = [mae_vector]\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    rmse_vector = [\n",
    "        sqrt(mean_squared_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        )) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['rmse_vector'] = [rmse_vector]\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    smape_vector = [\n",
    "        smape(\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['smape_vector'] = [smape_vector]\n",
    "\n",
    "    # insert count of rows as a column value\n",
    "    predictions_summary_df.insert(4, 'count', length)\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_detail/\n",
    "    detail_pickle_path = '{}/{}/{}/{}_{:02d}_{}_{}.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_detail',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset,\n",
    "        inference)\n",
    "\n",
    "    # persist the Pandas dataframe to database/predictions_detail/\n",
    "    predictions_detail_df.to_pickle(detail_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions detail of {}_{:02d}_{}_{}'.format(model_id,\n",
    "                                                                                        execution,\n",
    "                                                                                        dataset,\n",
    "                                                                                        inference))\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_summary/\n",
    "    summary_pickle_path = '{}/{}/{}/{}_{:02d}_{}_{}.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_summary',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset,\n",
    "        inference)\n",
    "\n",
    "    # persist the Pandas dataframe to database/predictions_summary/\n",
    "    predictions_summary_df.to_pickle(summary_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions summary of {}_{:02d}_{}_{}'.format(model_id,\n",
    "                                                                                         execution,\n",
    "                                                                                         dataset,\n",
    "                                                                                         inference))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
