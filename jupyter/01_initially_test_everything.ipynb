{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a temporary workaround\n",
    "# pass this code to the setup.py file of the final module\n",
    "import sys\n",
    "_ROOT_DIR = '{0}/gcp/cbidmltsf'.format(os.getenv(\"HOME\"))\n",
    "sys.path.append(_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dplstm.data import make_input_fn\n",
    "from dplstm.model import DPLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build parameters dictionary for interactive execution\n",
    "# in batch execution, this dictionary comes from parsed cli arguments\n",
    "\n",
    "parameters = {\n",
    "    'model_dir': 'lstm_46',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 20,  # NOT REQUIRED IN DISTRIBUTED MODE, IT IS OVERRIDDEN BY MAX_TRAIN_STEPS\n",
    "    'max_train_steps': 1000,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_interval': 300,\n",
    "    'keep_checkpoint_max': 3,\n",
    "    'eval_steps': None,\n",
    "    'eval_batch_size': 2**16,\n",
    "    'start_delay_secs': 600,\n",
    "    'throttle_secs': 600,\n",
    "    'train_data_path': '{0}/data/tfrecord/train.tfrecord'.format(_ROOT_DIR),\n",
    "    'eval_data_path': '{0}/data/tfrecord/val.tfrecord'.format(_ROOT_DIR),\n",
    "    'test_data_path': '{0}/data/tfrecord/test.tfrecord'.format(_ROOT_DIR),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for log files\n",
    "import logging\n",
    "\n",
    "# advanced numerical operations\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))\n",
    "\n",
    "# ToDo: pass a complex dictionary as a starting point for the LSTM network chromosome\n",
    "'''\n",
    "    Deep/Parallel LSTM network chromosome includes:\n",
    "    m_hour: [24, 12, 8, etc], tau=1, implement this variable later and work now with m_hour=24\n",
    "    m_day: [7, 14, etc], tau=7, implement this variable later and work now with m_day=7\n",
    "    m_week: [4, 8, 12, etc], tau=168, implement this variable later and work now with m_week=4\n",
    "    hidden_hour:\n",
    "    hidden_day:\n",
    "    hidden_week:\n",
    "    levels_hour:\n",
    "    levels_day:\n",
    "    levels_week:\n",
    "'''\n",
    "\n",
    "_LOG_PATH = '{0}/logs'.format(_ROOT_DIR)\n",
    "logging.basicConfig(filename='{}/test.log'.format(_LOG_PATH),\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(threadName)s -  %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info('Logging to {}/test.log.'.format(_LOG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove main function and transfer everything to first-level code cells\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_forecaster(features, labels, mode):\n",
    "    # instantiate network topology from the corresponding class\n",
    "    forecaster_topology = DPLSTM()\n",
    "\n",
    "    # ToDo: global_step might be moved to TRAIN scope\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # call operator to forecaster_topology, over features\n",
    "    forecast = forecaster_topology(features)\n",
    "\n",
    "    # predictions are stored in a dictionary for further use at inference stage\n",
    "    predictions = {\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "\n",
    "    # CHANGE MODEL FUNCTION STRUCTURE ACCORDING TO GILLARD'S ARCHITECTURE\n",
    "\n",
    "    # Estimator in TRAIN or EVAL mode\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # use labels and predictions to define training loss and evaluation loss\n",
    "        # generate summaries for TensorBoard\n",
    "        with tf.name_scope('loss'):\n",
    "            mean_squared_error = tf.losses.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, scope='loss')\n",
    "            tf.summary.scalar('loss', mean_squared_error)\n",
    "\n",
    "        with tf.name_scope('val_loss'):\n",
    "            val_loss = tf.metrics.mean_squared_error(\n",
    "                labels=labels, predictions=forecast, name='mse')\n",
    "            tf.summary.scalar('val_loss', val_loss[1])\n",
    "\n",
    "        # this is only required for PREDICT mode\n",
    "        prediction_hooks = None\n",
    "\n",
    "        # Estimator in TRAIN mode ONLY\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(key=tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(control_inputs=update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss=mean_squared_error,\n",
    "                    global_step=global_step,\n",
    "                    learning_rate=parameters['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "            # Create a hook to print acc, loss & global step every 100 iter\n",
    "            train_hook_list = []\n",
    "            train_tensors_log = {'val_loss': val_loss[1],\n",
    "                                 'loss': mean_squared_error,\n",
    "                                 'global_step': global_step}\n",
    "            train_hook_list.append(tf.train.LoggingTensorHook(\n",
    "                tensors=train_tensors_log, every_n_iter=100))\n",
    "\n",
    "            predictions = None  # this is not required in TRAIN mode\n",
    "            loss = mean_squared_error\n",
    "            eval_metric_ops = None\n",
    "            training_hooks = train_hook_list\n",
    "            evaluation_hooks = None\n",
    "\n",
    "        else:  # Estimator in EVAL mode ONLY\n",
    "            loss = mean_squared_error\n",
    "            train_op = None\n",
    "            training_hooks = None\n",
    "            eval_metric_ops = {'val_loss': val_loss}\n",
    "            evaluation_hooks = None\n",
    "\n",
    "    # Estimator in PREDICT mode ONLY\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    "        training_hooks = None\n",
    "        evaluation_hooks = None\n",
    "        prediction_hooks = None  # this might change as we are in PREDICT mode\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        # export_outputs=not_used_yet (for TensorFlow Serving, redirected from predictions if omitted)\n",
    "        # training_chief_hooks=not_used_yet\n",
    "        # ToDo: verify use of training_hooks\n",
    "        # temporarily disable training hooks\n",
    "        # training_hooks=training_hooks,\n",
    "        training_hooks=training_hooks,\n",
    "        # scaffold=not_used_yet\n",
    "        evaluation_hooks=evaluation_hooks,\n",
    "        prediction_hooks=prediction_hooks\n",
    "    )\n",
    "\n",
    "# ToDo: evaluate the convenience of packaging execution in a tf.app\n",
    "# in the meantime, run the estimator outside tf.app package\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure file writer cache is clear for TensorBoard events file\n",
    "tf.summary.FileWriterCache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters required for RunConfig()\n",
    "# _EVAL_INTERVAL = 300  # how often checkpoints are written out, given in seconds\n",
    "# _KEEP_CHECKPOINT_MAX = 3  # how many checkpoints to keep\n",
    "# _MAX_TRAIN_STEPS = 16000\n",
    "# _TRAIN_BATCH_SIZE = 2**5\n",
    "# _EVAL_STEPS = None  # if None, evaluate on entire dataset\n",
    "# _EVAL_BATCH_SIZE = 2**16\n",
    "# _START_DELAY_SECONDS = 600\n",
    "# _THROTTLE_SECONDS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_protocol': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_global_id_in_cluster': 0, '_save_checkpoints_steps': None, '_experimental_max_worker_delay_secs': None, '_train_distribute': None, '_task_id': 0, '_keep_checkpoint_max': 3, '_master': '', '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7effdd5f7ef0>, '_eval_distribute': None, '_experimental_distribute': None, '_model_dir': '/home/jupyter/gcp/cbidmltsf/dplstm/lstm_46', '_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_task_type': 'worker', '_session_creation_timeout_secs': 7200, '_save_summary_steps': 100, '_tf_random_seed': None, '_is_chief': True, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_device_fn': None}\n"
     ]
    }
   ],
   "source": [
    "# instantiate base estimator class for custom model function\n",
    "tsf_estimator = tf.estimator.Estimator(\n",
    "    model_fn=time_series_forecaster,\n",
    "    # no parameters passed at this point\n",
    "    # params=hparams,\n",
    "    # parameters passed in original model include: train_data_path, batch_size, augment, train_steps\n",
    "    config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=parameters['eval_interval'],\n",
    "        keep_checkpoint_max=parameters['keep_checkpoint_max']\n",
    "    ),\n",
    "    model_dir='{0}/dplstm/{1}'.format(_ROOT_DIR, parameters['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's train_spec to use train_input_fn and train for so many steps\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['train_data_path'],\n",
    "        batch_size=parameters['train_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.TRAIN\n",
    "    ),\n",
    "    max_steps=parameters['max_train_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: activate TensorFlow Serving for distributed inference\n",
    "# Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "# exporter = tf.estimator.LatestExporter(\n",
    "#     name=\"exporter\",\n",
    "#     serving_input_receiver_fn=serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set estimator's eval_spec to use eval_input_fn and export saved_model\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=make_input_fn(\n",
    "        tfrecord_path=parameters['eval_data_path'],\n",
    "        batch_size=parameters['eval_batch_size'],\n",
    "        mode=tf.estimator.ModeKeys.EVAL\n",
    "    ),\n",
    "    steps=parameters['eval_steps'],  # use None to evaluate on the entire dataset\n",
    "    exporters=exporter,\n",
    "    start_delay_secs=parameters['start_delay_secs'],  # delay first evaluation\n",
    "    throttle_secs=parameters['throttle_secs']  # evaluate at a different rate (usually longer) than checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.005017233, step = 901\n",
      "INFO:tensorflow:global_step = 901, loss = 0.005017233, val_loss = 0.005017233\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-09T18:55:12Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-09-18:55:15\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.00549519, val_loss = 0.00549519\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/export/exporter/temp-b'1575917715'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.0033044359.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 1000, 'loss': 0.00549519, 'val_loss': 0.00549519},\n",
       " [b'/home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/export/exporter/1575917715'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run train_and_evaluate loop\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=tsf_estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive design of serving_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_placeholders_demo = {\n",
    "    # this example from https://stackoverflow.com\n",
    "    # /questions/48510264/in-tensorflow-for-serving-a-model-what-does-the-serving-input-function-supposed\n",
    "    # receives 5 scalar values as features\n",
    "    # let's dive into types :-)\n",
    "    'var1': tf.placeholder(tf.float32, [None]),\n",
    "    'var2': tf.placeholder(tf.float32, [None]),\n",
    "    'var3': tf.placeholder(tf.float32, [None]),\n",
    "    'var4': tf.placeholder(tf.float32, [None]),\n",
    "    'var5': tf.placeholder(tf.float32, [None])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the newly created dictionary\n",
    "feature_placeholders_demo['var1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features are expanded-dimensions versions of placeholders from [None] to [None, 1]\n",
    "features_demo = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders_demo.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var1': <tf.Tensor 'ExpandDims_6:0' shape=(?, 1) dtype=float32>,\n",
       " 'var2': <tf.Tensor 'ExpandDims_5:0' shape=(?, 1) dtype=float32>,\n",
       " 'var3': <tf.Tensor 'ExpandDims_7:0' shape=(?, 1) dtype=float32>,\n",
       " 'var4': <tf.Tensor 'ExpandDims_9:0' shape=(?, 1) dtype=float32>,\n",
       " 'var5': <tf.Tensor 'ExpandDims_8:0' shape=(?, 1) dtype=float32>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the customized input function for serving predictions on dplstm\n",
    "# target dictionary is {'hourly': [?, 24, 1], 'daily': [?, 7, 1], 'weekly': [?, 4, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_placeholders = {\n",
    "    'hourly': tf.placeholder(tf.float32, [None, 24]),\n",
    "    'daily': tf.placeholder(tf.float32, [None, 7]),\n",
    "    'weekly': tf.placeholder(tf.float32, [None, 4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': <tf.Tensor 'ExpandDims_10:0' shape=(?, 7, 1) dtype=float32>,\n",
       " 'hourly': <tf.Tensor 'ExpandDims_11:0' shape=(?, 24, 1) dtype=float32>,\n",
       " 'weekly': <tf.Tensor 'ExpandDims_12:0' shape=(?, 4, 1) dtype=float32>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "\n",
    "    feature_placeholders = {\n",
    "    'hourly': tf.placeholder(tf.float32, [None, 24]),\n",
    "    'daily': tf.placeholder(tf.float32, [None, 7]),\n",
    "    'weekly': tf.placeholder(tf.float32, [None, 4])\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create exporter that uses serving_input_fn to create saved_model for serving\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "    name = \"exporter\", \n",
    "    serving_input_receiver_fn = serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict based on checkpoints and plot results with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anaconda Interactive Visualization\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import output_file, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# persistence for the scaler located in $DATA/scalers\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# reload scaler fitted model here\n",
    "scaler = joblib.load('{0}/data/scalers/ci_LSTM_scaler.save'.format(_ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/gcp/cbidmltsf/dplstm/lstm_46/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# build predictions using the trained model\n",
    "# ToDo: use TensorFlow Serving instead of the custom estimator\n",
    "# pred = list(tsf_estimator.predict(input_fn=test_input_fn))\n",
    "pred = list(\n",
    "    tsf_estimator.predict(\n",
    "        input_fn=make_input_fn(\n",
    "            tfrecord_path=parameters['test_data_path'],\n",
    "            batch_size=32,\n",
    "            mode=tf.estimator.ModeKeys.PREDICT\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'forecast': array([0.4926325], dtype=float32)},\n",
       " {'forecast': array([0.54299], dtype=float32)},\n",
       " {'forecast': array([0.5770303], dtype=float32)},\n",
       " {'forecast': array([0.58423465], dtype=float32)},\n",
       " {'forecast': array([0.6234549], dtype=float32)},\n",
       " {'forecast': array([0.6371721], dtype=float32)},\n",
       " {'forecast': array([0.6415772], dtype=float32)},\n",
       " {'forecast': array([0.6419458], dtype=float32)},\n",
       " {'forecast': array([0.6172749], dtype=float32)},\n",
       " {'forecast': array([0.56820893], dtype=float32)}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at a predictions subset\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to array and re-scale\n",
    "pred = [p['forecast'][0] for p in pred]\n",
    "pred = np.asarray(pred)\n",
    "pred_ci = scaler.inverse_transform(pred.reshape(-1, 1))\n",
    "pred_ci = np.squeeze(pred_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.056008 , 4.470618 , 4.7508826, 4.810199 , 5.1331124, 5.2460504,\n",
       "       5.2823186, 5.2853537, 5.0822296, 4.6782537], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ci[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: get ytarget_test array from test.tfrecord dataset to perform the following operation\n",
    "# temporarily get the array from disk\n",
    "y_test = np.load('{0}/data/arrays/y_test.npy'.format(_ROOT_DIR))\n",
    "n = 0  # first step ahead\n",
    "ytarget_test = y_test[:, n]\n",
    "ytarget_test = ytarget_test.reshape(ytarget_test.shape[0], 1)\n",
    "\n",
    "actual_ci = scaler.inverse_transform(ytarget_test)\n",
    "actual_ci = np.squeeze(actual_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EQUIPMENT = 'CPE04105'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf/plots/lstm_46.html'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_predictions_fig = figure(title='Predicted Current Imbalance for ' + _EQUIPMENT,\n",
    "                            background_fill_color='#E8DDCB',\n",
    "                            plot_width=1800, plot_height=450, x_axis_type='datetime')\n",
    "\n",
    "# ToDo: yts_test array is required to get timestamps for plot, then wire it now and get it from TFRecord later...\n",
    "yts_test = np.load('{0}/data/arrays/yts_test.npy'.format(_ROOT_DIR), allow_pickle=True)\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        actual_ci, line_color='red',\n",
    "                        line_width=1, alpha=0.7, legend='Actual')\n",
    "\n",
    "ci_predictions_fig.line(yts_test[:, n],\n",
    "                        pred_ci, line_color='blue',\n",
    "                        line_width=1, alpha=0.7, legend='Predicted')\n",
    "\n",
    "ci_predictions_fig.legend.location = \"top_right\"\n",
    "ci_predictions_fig.legend.background_fill_color = \"darkgrey\"\n",
    "\n",
    "ci_predictions_fig.xaxis.axis_label = 'Timestamp'\n",
    "ci_predictions_fig.yaxis.axis_label = 'Current Imbalance [%]'\n",
    "\n",
    "# output_file('{0}/plots/'.format(_ROOT_DIR) + '{:03d}'.format(n) + '.html', title=_EQUIPMENT)\n",
    "output_file('{0}/plots/'.format(_ROOT_DIR) + '{}'.format(parameters['model_dir']) + '.html', title=_EQUIPMENT)\n",
    "save(ci_predictions_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
