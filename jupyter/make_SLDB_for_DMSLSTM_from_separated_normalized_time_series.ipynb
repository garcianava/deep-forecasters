{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make similar SLDBs for DMSLSTM, EDSLSTM, and ARTRFDC architecture models\n",
    "# from the same time series in the MIRD Parquet archive\n",
    "\n",
    "# make sure the time series are normalized after being split for\n",
    "# seen data (training and evaluation) and unseen data (test)\n",
    "# example of similar SLDBs required:\n",
    "\n",
    "# hourly, day-ahead\n",
    "\n",
    "# DMSLSTM_TPU_006: balanced time-resolution [8, 8, 8] LSTM stacks,\n",
    "#                  [[64, 128], [64, 128], [64, 128]],\n",
    "#                  [512, 128, 24] dense layer (vector output)\n",
    "#                  direct, 24-step ahead\n",
    "\n",
    "# EDSLSTM_TPU_011: m=64, h=256, 'elu' encoder-3/decoder-2, batch_normalization, dense=[1]\n",
    "#                  direct, 24-step ahead\n",
    "\n",
    "# ARTRFDC_TPU_001: m=168, 2 decoder layers, 2 heads, ff=1024, dense=[1]\n",
    "#                  iterative, forecast window up to 168-step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import time\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once scaled time series for building train, eval, and test datasets have been persisted\n",
    "# SLDB files can be produced loading the corresponding pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that already exist in the time series directory\n",
    "# scaler_train_eval.save: scaler of the joint time series of data seen by the model\n",
    "# scaler_test.save: scaler of the time series of data unseen by the model\n",
    "# ts.json: description dictionary of the time series\n",
    "# ts_train.pkl\n",
    "# ts_eval.pkl\n",
    "# ts_test.pkl: pickle files of the time series, normalized after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time series for a given identifier\n",
    "identifier = 'CPE04115_H_kw_20210526212214'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, build the time series directory\n",
    "time_series_folder = '/home/developer/gcp/cbidmltsf/timeseries/{}'.format(identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a path to the JSON file that describes the time series\n",
    "json_filename = '{}/ts.json'.format(time_series_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the time series specs for further use\n",
    "with open(json_filename, 'r') as input_file:\n",
    "    ts_dict = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'CPE04115',\n",
       " 'resolution': 'hourly',\n",
       " 'variable': 'kw_scaled',\n",
       " 'train': {'start': '2016-01-01 00:00:00',\n",
       "  'end': '2017-12-31 23:00:00',\n",
       "  'num_lectures': 17542},\n",
       " 'eval': {'start': '2018-01-01 00:00:00',\n",
       "  'end': '2018-04-30 23:00:00',\n",
       "  'num_lectures': 2879},\n",
       " 'test': {'start': '2018-05-01 00:00:00',\n",
       "  'end': '2018-07-31 23:00:00',\n",
       "  'num_lectures': 2208},\n",
       " 'identifier': 'CPE04115_H_kw_20210526212214'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage the time series for the different model stages\n",
    "ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stages as string values\n",
    "stages = ['train', 'eval', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scaled time series for stages from pickle files\n",
    "for stage in stages:\n",
    "    ts[stage] = pd.read_pickle('{}/ts_{}.pkl'.format(time_series_folder, stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17542, 2879, 2208)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm time series are loaded to dictionary\n",
    "ts['train']['kw_scaled'].count(), ts['eval']['kw_scaled'].count(), ts['test']['kw_scaled'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized time series loaded, proceed with SLDB construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building an SLDB for DMSLSTM architecture based on the following configuration dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': identifier,\n",
    "    'embedding': {\n",
    "        'hourly': 8,\n",
    "        'daily': 8,\n",
    "        'weekly': 8\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1,\n",
    "        'daily': 24,\n",
    "        'weekly': 168        \n",
    "    },\n",
    "    'no_targets': 24\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to one-hot encode a timestamp,\n",
    "# used for single positional encoding in DMSLSTM\n",
    "def one_hot_encode(timestamp):\n",
    "    # input: a timestamp\n",
    "    # output: a 7-bit list encoding the week-day, and a 24-bit list encoding the day-hour\n",
    "    fv_weekday = np.zeros(7)\n",
    "    fv_hour = np.zeros(24)\n",
    "    fv_weekday[timestamp.weekday()] = 1.\n",
    "    fv_hour[timestamp.hour] = 1.\n",
    "    return list(fv_weekday), list(fv_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to build the features in the DMSLSTM SLDB\n",
    "def make_features_targets_timestamps_ohvs(time_series, m, tau, n_targets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "           time series: original time series\n",
    "           m: embedding dimension\n",
    "           tau: lag\n",
    "           n_targets: number of targets to predict\n",
    "    Output:\n",
    "           features: list of features\n",
    "           targets: list of targets\n",
    "           timestamps: list of target (target) timestamps\n",
    "           oh_wds: list of one-hot vectors describing weekday of timestamp\n",
    "           oh_dhs: list of one-hot vectors describing hour of the day of timestamp\n",
    "    \"\"\"\n",
    "    # a couple of empty lists to store feature vectors and targets\n",
    "    features = []\n",
    "    targets = []\n",
    "    timestamps = []\n",
    "    oh_wds = []\n",
    "    oh_dhs = []\n",
    "    sequence = range(m * tau, time_series.shape[0] - n_targets + 1)\n",
    "    for i in sequence:\n",
    "        # uncomment the following line to preview features sequence timestamps (to verify the functionality)\n",
    "        # features.append(list(time_series.iloc[(i - m * tau):i:tau].index))\n",
    "        features.append(list(time_series.iloc[(i - m * tau):i:tau]))\n",
    "        # uncomment the following line to preview targets sequence timestamps (to verify the functionality)\n",
    "        # targets.append(list(time_series.iloc[i:(i + n_targets):1].index))\n",
    "        targets.append(list(time_series.iloc[i:(i + n_targets):1]))\n",
    "        # get the timestamps for the target values (just one for the first experiment)\n",
    "        targets_timestamps_list = list(time_series.index[i:(i + n_targets):1])\n",
    "        # EXTRACT TIMESTAMPS AS BYTES FOR TFRECORD PERSISTENCE\n",
    "        targets_timestamps_list_as_bytes = [timestamp.strftime(\"%Y-%m-%d %H:%M:%S\").encode() for timestamp in\n",
    "                                           targets_timestamps_list]\n",
    "        timestamps.append(targets_timestamps_list_as_bytes)\n",
    "        # build one-hot vectors for week-day and day-hour\n",
    "        # pass the timestamp(s) in the list, not the list!\n",
    "        # important, only the first timestamp in the targets list is used\n",
    "        # to build one-hot-encoded vectors for positional encoding\n",
    "        oh_wd_vectors, oh_dh_vectors = one_hot_encode(targets_timestamps_list[0])\n",
    "        # the one-hot-encode function already returns lists, then,\n",
    "        oh_wds.append(oh_wd_vectors)\n",
    "        oh_dhs.append(oh_dh_vectors)\n",
    "\n",
    "    # uncomment the following line to return NumPy arrays instead of Python lists\n",
    "    # features, targets, timestamps = np.array(features), np.array(targets), np.array(timestamps)\n",
    "\n",
    "    return features, targets, timestamps, oh_wds, oh_dhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to temporarily store the following SLDBs:\n",
    "# train (hourly, daily, weekly, targets, timestamps)\n",
    "# eval (hourly, daily, weekly, targets, timestamps)\n",
    "# test (hourly, daily, weekly, targets, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_full = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw_scaled'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the variable to build the forecast over\n",
    "# get it from the time series descriptor dictionary\n",
    "variable = ts_dict['variable']\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on data resolutions\n",
    "resolutions = [\n",
    "    'hourly',\n",
    "    'daily',\n",
    "    'weekly'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the SLDB for DMSLSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # train, eval, test\n",
    "    # for component_key in sldb['components'].keys():\n",
    "    for resolution in resolutions:\n",
    "        # hourly, daily, weekly\n",
    "        sldb_full[stage][resolution]['features'], \\\n",
    "        sldb_full[stage][resolution]['targets'], \\\n",
    "        sldb_full[stage][resolution]['timestamps'], \\\n",
    "        sldb_full[stage][resolution]['oh_wds'], \\\n",
    "        sldb_full[stage][resolution]['oh_dhs'] = \\\n",
    "        make_features_targets_timestamps_ohvs(\n",
    "            ts[stage][variable],\n",
    "            sldb['embedding'][resolution],\n",
    "            sldb['tau'][resolution],\n",
    "            sldb['no_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the target is stored as a list with number of elements equal to no_targets\n",
    "len(sldb_full['test']['hourly']['targets'][0]) == sldb['no_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to iterate on the sldb items\n",
    "items = ['features', 'targets', 'timestamps', 'oh_wds', 'oh_dhs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to collect statistics\n",
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {},\n",
    "    },\n",
    "    'eval': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {},\n",
    "    },\n",
    "    'test': {\n",
    "        'hourly': {},\n",
    "        'daily': {},\n",
    "        'weekly': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17487 features / train / hourly from 2016-01-01 08:00:00 to 2017-12-30 00:00:00\n",
      "17487 targets / train / hourly from 2016-01-01 08:00:00 to 2017-12-30 00:00:00\n",
      "17487 timestamps / train / hourly from 2016-01-01 08:00:00 to 2017-12-30 00:00:00\n",
      "17487 oh_wds / train / hourly from 2016-01-01 08:00:00 to 2017-12-30 00:00:00\n",
      "17487 oh_dhs / train / hourly from 2016-01-01 08:00:00 to 2017-12-30 00:00:00\n",
      "17303 features / train / daily from 2016-01-09 00:00:00 to 2017-12-30 00:00:00\n",
      "17303 targets / train / daily from 2016-01-09 00:00:00 to 2017-12-30 00:00:00\n",
      "17303 timestamps / train / daily from 2016-01-09 00:00:00 to 2017-12-30 00:00:00\n",
      "17303 oh_wds / train / daily from 2016-01-09 00:00:00 to 2017-12-30 00:00:00\n",
      "17303 oh_dhs / train / daily from 2016-01-09 00:00:00 to 2017-12-30 00:00:00\n",
      "16151 features / train / weekly from 2016-02-26 00:00:00 to 2017-12-30 00:00:00\n",
      "16151 targets / train / weekly from 2016-02-26 00:00:00 to 2017-12-30 00:00:00\n",
      "16151 timestamps / train / weekly from 2016-02-26 00:00:00 to 2017-12-30 00:00:00\n",
      "16151 oh_wds / train / weekly from 2016-02-26 00:00:00 to 2017-12-30 00:00:00\n",
      "16151 oh_dhs / train / weekly from 2016-02-26 00:00:00 to 2017-12-30 00:00:00\n",
      "2824 features / eval / hourly from 2018-01-01 08:00:00 to 2018-04-29 00:00:00\n",
      "2824 targets / eval / hourly from 2018-01-01 08:00:00 to 2018-04-29 00:00:00\n",
      "2824 timestamps / eval / hourly from 2018-01-01 08:00:00 to 2018-04-29 00:00:00\n",
      "2824 oh_wds / eval / hourly from 2018-01-01 08:00:00 to 2018-04-29 00:00:00\n",
      "2824 oh_dhs / eval / hourly from 2018-01-01 08:00:00 to 2018-04-29 00:00:00\n",
      "2640 features / eval / daily from 2018-01-09 00:00:00 to 2018-04-29 00:00:00\n",
      "2640 targets / eval / daily from 2018-01-09 00:00:00 to 2018-04-29 00:00:00\n",
      "2640 timestamps / eval / daily from 2018-01-09 00:00:00 to 2018-04-29 00:00:00\n",
      "2640 oh_wds / eval / daily from 2018-01-09 00:00:00 to 2018-04-29 00:00:00\n",
      "2640 oh_dhs / eval / daily from 2018-01-09 00:00:00 to 2018-04-29 00:00:00\n",
      "1488 features / eval / weekly from 2018-02-26 00:00:00 to 2018-04-29 00:00:00\n",
      "1488 targets / eval / weekly from 2018-02-26 00:00:00 to 2018-04-29 00:00:00\n",
      "1488 timestamps / eval / weekly from 2018-02-26 00:00:00 to 2018-04-29 00:00:00\n",
      "1488 oh_wds / eval / weekly from 2018-02-26 00:00:00 to 2018-04-29 00:00:00\n",
      "1488 oh_dhs / eval / weekly from 2018-02-26 00:00:00 to 2018-04-29 00:00:00\n",
      "2153 features / test / hourly from 2018-05-01 08:00:00 to 2018-07-30 00:00:00\n",
      "2153 targets / test / hourly from 2018-05-01 08:00:00 to 2018-07-30 00:00:00\n",
      "2153 timestamps / test / hourly from 2018-05-01 08:00:00 to 2018-07-30 00:00:00\n",
      "2153 oh_wds / test / hourly from 2018-05-01 08:00:00 to 2018-07-30 00:00:00\n",
      "2153 oh_dhs / test / hourly from 2018-05-01 08:00:00 to 2018-07-30 00:00:00\n",
      "1969 features / test / daily from 2018-05-09 00:00:00 to 2018-07-30 00:00:00\n",
      "1969 targets / test / daily from 2018-05-09 00:00:00 to 2018-07-30 00:00:00\n",
      "1969 timestamps / test / daily from 2018-05-09 00:00:00 to 2018-07-30 00:00:00\n",
      "1969 oh_wds / test / daily from 2018-05-09 00:00:00 to 2018-07-30 00:00:00\n",
      "1969 oh_dhs / test / daily from 2018-05-09 00:00:00 to 2018-07-30 00:00:00\n",
      "817 features / test / weekly from 2018-06-26 00:00:00 to 2018-07-30 00:00:00\n",
      "817 targets / test / weekly from 2018-06-26 00:00:00 to 2018-07-30 00:00:00\n",
      "817 timestamps / test / weekly from 2018-06-26 00:00:00 to 2018-07-30 00:00:00\n",
      "817 oh_wds / test / weekly from 2018-06-26 00:00:00 to 2018-07-30 00:00:00\n",
      "817 oh_dhs / test / weekly from 2018-06-26 00:00:00 to 2018-07-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# report statistics on stages and resolutions of SLDBs\n",
    "# and persist them to the sldb['stats'] level\n",
    "for stage in stages:\n",
    "    for resolution in resolutions:\n",
    "        for item in items:\n",
    "            # fill the values in the stats sub-dictionary\n",
    "            sldb['stats'][stage][resolution][item] = len(sldb_full[stage][resolution][item])\n",
    "            # timestamps are persisted as bytes, as in b'YYYY-MM-DD HH:MM;SS'\n",
    "            # but are required as strings, as in 'YYYY-MM-DD HH:MM;SS'\n",
    "            from_timestamp_str = sldb_full[stage][resolution]['timestamps'][0][0].decode()\n",
    "            sldb['stats'][stage][resolution]['from'] = from_timestamp_str\n",
    "            to_timestamp_str = sldb_full[stage][resolution]['timestamps'][-1][0].decode()\n",
    "            sldb['stats'][stage][resolution]['to'] = to_timestamp_str\n",
    "            # and log them\n",
    "            print('{0} {3} / {1} / {2} from {4} to {5}'.format(len(sldb_full[stage][resolution][item]),\n",
    "                                                               stage,\n",
    "                                                               resolution,\n",
    "                                                               item,\n",
    "                                                               from_timestamp_str,\n",
    "                                                               to_timestamp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in train set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['train']['hourly']['to'] == sldb['stats']['train']['daily']['to'] == sldb['stats']['train']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in eval set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['eval']['hourly']['to'] == sldb['stats']['eval']['daily']['to'] == sldb['stats']['eval']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in test set, verify resolution-based datasets end in the same timestamp\n",
    "print(sldb['stats']['test']['hourly']['to'] == sldb['stats']['test']['daily']['to'] == sldb['stats']['test']['weekly']['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset on train stage will be trimmed to 16151 rows.\n",
      "Dataset on eval stage will be trimmed to 1488 rows.\n",
      "Dataset on test stage will be trimmed to 817 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows in the smaller resolution-based dataset, for alignment purposes\n",
    "for stage in stages:\n",
    "    sldb['stats'][stage]['trimmed_to_count'] = min([sldb['stats'][stage][resolution]['features'] for resolution in resolutions])\n",
    "    print('Dataset on {} stage will be trimmed to {} rows.'.format(stage, sldb['stats'][stage]['trimmed_to_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary with final, trimmed data\n",
    "tfrecords = {\n",
    "    'train': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'eval': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "    'test': {}, # hourly, daily, weekly, targets, timestamps, oh_wds, oh_dhs to be added\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    # isolate this value, just for readability\n",
    "    value_to_trim = sldb['stats'][stage]['trimmed_to_count']\n",
    "    tfrecords[stage]['hourly'] = sldb_full[stage]['hourly']['features'][-value_to_trim:]\n",
    "    tfrecords[stage]['daily'] = sldb_full[stage]['daily']['features'][-value_to_trim:]\n",
    "    tfrecords[stage]['weekly'] = sldb_full[stage]['weekly']['features'][-value_to_trim:]\n",
    "    # targets, target timestamps, and first-target-timestamp-one-hot vectors\n",
    "    # can be acquired from any resolution-based, temporary dataset (hourly, daily, weekly)\n",
    "    tfrecords[stage]['targets'] = sldb_full[stage]['hourly']['targets'][-value_to_trim:]\n",
    "    tfrecords[stage]['timestamps'] = sldb_full[stage]['hourly']['timestamps'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_wds'] = sldb_full[stage]['hourly']['oh_wds'][-value_to_trim:]\n",
    "    tfrecords[stage]['oh_dhs'] = sldb_full[stage]['hourly']['oh_dhs'][-value_to_trim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode byte values for serialized examples\n",
    "def _bytes_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a bytes_list from a list of strings / bytes.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = '{:03d}{:03d}_' \\\n",
    "             '{:03d}{:03d}_' \\\n",
    "             '{:03d}{:03d}_' \\\n",
    "             '{:03d}'.format(sldb['embedding']['hourly'],\n",
    "                             sldb['tau']['hourly'],\n",
    "                             sldb['embedding']['daily'],\n",
    "                             sldb['tau']['daily'],\n",
    "                             sldb['embedding']['weekly'],\n",
    "                             sldb['tau']['weekly'],\n",
    "                             sldb['no_targets']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'008001_008024_008168_048'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04115_H_kw_20210526212214_008001_008024_008168_048'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the identifer for the SLDB folder, with the specs dictionary and the time series identifier\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20210526212214_008001_008024_008168_048 was created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now persist SLDBs as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in stages:\n",
    "    N_ROWS = sldb['stats'][stage]['trimmed_to_count']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        # get an iterable with the indexes of the NumPy array to be stored in the TFRecord file\n",
    "        for row in np.arange(N_ROWS):\n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    # [lecture[0] for lecture in Xadj_train[row]] flattens the adjacent hours array\n",
    "                    feature={\n",
    "                        # the m-hourly vector of hourly lectures previous to the target\n",
    "                        'hourly': _float_feature_from_list_of_values(tfrecords[stage]['hourly'][row]),\n",
    "                        # the m-daily vector of daily lectures previous to the target\n",
    "                        'daily': _float_feature_from_list_of_values(tfrecords[stage]['daily'][row]),\n",
    "                        # the m-weekly vector of weekly lectures previous to the target\n",
    "                        'weekly': _float_feature_from_list_of_values(tfrecords[stage]['weekly'][row]),\n",
    "                        # the no_targets vector of target lectures (no_target-steps-ahead)\n",
    "                        'target': _float_feature_from_list_of_values(tfrecords[stage]['targets'][row]),\n",
    "                        # the 7d one hot vector of the weekday (from the first target lecture)\n",
    "                        'oh_wd': _float_feature_from_list_of_values(tfrecords[stage]['oh_wds'][row]),\n",
    "                        # the 24d one hot vector of the dayhour (from the first target lecture)\n",
    "                        'oh_dh': _float_feature_from_list_of_values(tfrecords[stage]['oh_dhs'][row]),\n",
    "                        # the no_targets vector of target timestamps (no_target-steps-ahead)\n",
    "                        'timestamp': _bytes_feature_from_list_of_values(tfrecords[stage]['timestamps'][row])\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path for the json file\n",
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the final, compact dictionary to JSON\n",
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do not forget to sync sldbs/ from local to Google Storage after the previous operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20210526212214_008001_008024_008168_048/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20210526212214_008001_008024_008168_048/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20210526212214_008001_008024_008168_048/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/CPE04115_H_kw_20210526212214_008001_008024_008168_048/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "| [4 files][ 27.4 MiB/ 27.4 MiB]                                                \n",
      "Operation completed over 4 objects/27.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
