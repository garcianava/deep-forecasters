{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for prediction and evaluation of multi-step forecasting ARTRFDC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# uncomment the following line for compatibility with TensorFlow 1.15 (on GCP)\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# uncomment the following line for TensorFlow 2.X (local execution)\n",
    "import tensorflow as tf\n",
    "\n",
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "# select a palette\n",
    "from bokeh.palettes import d3\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical mean absolute percentage error\n",
    "def smape(targets, predictions):\n",
    "    '''\n",
    "    predictions: a list with the predicted values\n",
    "    targets: a list with the actual values\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # lists to NumPy arrays\n",
    "    targets, predictions = np.array(targets), np.array(predictions)\n",
    "    # verify predictions and targets have the same shape\n",
    "    if predictions.shape == targets.shape:\n",
    "            return(np.sum(2*np.abs(predictions - targets) /\n",
    "                          (np.abs(targets) + np.abs(predictions)))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    \n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    source = tf.reshape(row['source'].values, objective_shapes['source'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'source': source\n",
    "    }\n",
    "    \n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    # _parse_dataset_function returns:\n",
    "    # features as a dictionary, and\n",
    "    # target as a float vector\n",
    "    return feature_dict, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_tensor_example(float_tensor):\n",
    "    # first, pass the float tensor to NumPy array, then flatten it\n",
    "    flat_array = float_tensor.numpy().flatten()\n",
    "    # second, build the protobuffer example\n",
    "    example = tf.train.Example(\n",
    "        # features within the example\n",
    "        features=tf.train.Features(\n",
    "            # individual feature definition\n",
    "            feature={'source': _float_feature_from_list_of_values(flat_array)}\n",
    "        )\n",
    "    )    \n",
    "    # third, serialize the example dictionary to a string\n",
    "    serialized_example = example.SerializeToString()\n",
    "    # fourth, wrap the serialized example as a NumPy-string array\n",
    "    numpy_example = np.array(serialized_example, dtype='S')\n",
    "    # fifth, wrap the NumPy-string array as a string tensor\n",
    "    tensor_example = tf.convert_to_tensor(numpy_example)\n",
    "\n",
    "    return tensor_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_features = {\n",
    "    'source': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'\n",
    "\n",
    "# during batch prediction, the SLDB identifier is obtained via Abseil Flags\n",
    "sldb_id = 'CPE04115_H_kw_20201021084001_ARTRFDC_168'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the SLDB json file\n",
    "data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)\n",
    "\n",
    "# then get the ts_identifier from the json file in the sldb directory\n",
    "sldb_json_file = '{}/sldb.json'.format(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the json file\n",
    "with open(sldb_json_file, 'r') as inputfile:\n",
    "    sldb_dict = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and get the time series identifier\n",
    "ts_identifier = sldb_dict['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# use the time series identifier to obtain the SK-Learn scaler used on it\n",
    "scaler = joblib.load('{}/{}/{}/scaler.save'.format(PROJECT_ROOT,\n",
    "                                                    'timeseries',\n",
    "                                                    ts_identifier))\n",
    "\n",
    "print('Scaler loaded for time series {}'.format(ts_identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all the code to a single notebook cell, then to a function, later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the model identifier is obtained via Abseil Flags\n",
    "model_id = 'ARTRFDC_TPU_001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the dataset name is obtained via Abseil Flags\n",
    "dataset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during batch prediction, the execution identifier is obtained via Abseil Flags\n",
    "execution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model identifier and execution number to build the model directory string\n",
    "model_dir = '{}_{:02d}'.format(model_id, execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to the saved model main directory\n",
    "saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                     'models',\n",
    "                                                     model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in the saved model path, to find the most recent one\n",
    "all_files = os.listdir(saved_model_path)\n",
    "# get the path to the most recent saved model\n",
    "latest_saved_model_id = sorted(all_files)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/ARTRFDC_TPU_001_02/export/exporter/1621561318\n"
     ]
    }
   ],
   "source": [
    "# build the full path for the latest saved model dir\n",
    "export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "print ('Exported model path is {}'.format(export_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model and the prediction function\n",
    "imported = load(export_dir=export_dir, tags='serve')\n",
    "predict_fn = imported.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the dataset for prediction\n",
    "dataset_path = '{}/{}.tfrecord'.format(data_dir, dataset)\n",
    "\n",
    "# load the dataset\n",
    "tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SLDB parameters for the forecasting model\n",
    "config_json_file = '{}/{}/{}.json'.format(PROJECT_ROOT,\n",
    "                                          'parameters',\n",
    "                                          model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the sldb dictionary from the json file in parameters/\n",
    "with open(config_json_file, 'r') as inputfile:\n",
    "    configuration = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': [168, 7], 'target': [168, 7]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the objective shapes for reshaping tensors in a dictionary\n",
    "_EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "    'source': [configuration['num_timesteps'], configuration['model_dimension']],\n",
    "    'target': [configuration['num_timesteps'], configuration['model_dimension']]\n",
    "}\n",
    "\n",
    "_EXTRACTING_OBJECTIVE_SHAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the test dataset from the TFRecord file\n",
    "# and use its features to produce predictions in a different way\n",
    "\n",
    "parsed_dataset = tfrecord_dataset.map(\n",
    "    lambda row: _parse_dataset_function(\n",
    "        example_proto=row,\n",
    "        objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "        # ToDo: parse the timestamps for plotting or additional positional encoding, later...\n",
    "        parse_timestamp=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on, inferences for ARTRFDC are produced in a very different way\n",
    "# from the used for DMSLSTM or EDALSTM models (prediction process has to be iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each predicted row is a sequence of n_timesteps values,\n",
    "# but only the first element in this sequence is used, as the first prediction,\n",
    "# then it is added (along with its positional encodings) to the end of the input sequence\n",
    "# (first entry of the input sequence is discarded to keep tensor shape)\n",
    "# to get the second prediction, and so on up to the n_timesteps-th prediction,\n",
    "# which completes the n_timesteps prediction sequence (the forecast window)\n",
    "# that starts immediately after the source input sequence ends (in time dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the iterative process for inference over the ARTRFDC saved model can be initiated now:\n",
    "# source feature (?, 168, 7) (unseen data) is on parsed_dataset[0]['source']\n",
    "# target feature (?, 168, 7) (unseen data) is on parsed_dataset[1]\n",
    "\n",
    "# uncomment and run the following two cells to confirm that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is not possible to iterate over a segment of a dataset, as required by iterative inference\n",
    "# then the complete test dataset will be passed to two NumPy arrays:\n",
    "\n",
    "# source_array, with shape (n_rows, n_timesteps, n_features), in this example (2095, 168, 7), and\n",
    "# target_array, with shape (n_rows, n_timesteps, n_features), in this example (2095, 168, 7)\n",
    "\n",
    "# remember source_array[1:, :, :] = target_array[:-2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build temporary lists to store source (features) and target (labels) tensors\n",
    "source_list = list()\n",
    "target_list = list()\n",
    "\n",
    "# fill in the lists from the parsed dataset\n",
    "for element in parsed_dataset:\n",
    "    source_list.append(element[0]['source'])\n",
    "    target_list.append(element[1])\n",
    "\n",
    "# from the temporary lists, build NumPy arrays to feed the model\n",
    "source_array = np.array(source_list)\n",
    "target_array = np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2095, 168, 7), (2095, 168, 7))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify shape of resulting arrays\n",
    "source_array.shape, target_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now follow the inference process detailed in Klingenbrunn to:\n",
    "# predict over the forecast window,\n",
    "# calculate prediction error metrics, and\n",
    "# plot prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a forecast window to guide the iterative prediction process\n",
    "# start with a hourly, day-ahead process\n",
    "forecast_window = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first source or input to the model is the first source row\n",
    "# that means, the true variable value, plus the six positional encodings for the timestamp\n",
    "# in the first row of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to get the source as a tensor with TensorShape([1, 168, 7])\n",
    "# source_tensor = tf.expand_dims(source_array[0, :, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: include timestamps in train, eval, and test datasets to easily keep tracking of prediction dates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now please code all the complicated previous stuff into an easy Python function, would you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important, the inference cycle was coded for tensors, not for NumPy arrays\n",
    "# then use source and prediction tensors and translate to tensor examples from float tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which row of the test dataset will be used for inference?\n",
    "row = 702\n",
    "\n",
    "# verify the dataset is long enough to iteratively predict from that row\n",
    "max_row_index = len(source_list) - configuration['num_timesteps'] - 1\n",
    "if row > max_row_index:\n",
    "    # clear row value to raise error\n",
    "    row = None\n",
    "    print('The source row index for iterative inference cannot be greater than {}.'.format(max_row_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then again, build the initial source tensor\n",
    "source_tensor = tf.expand_dims(source_array[row, :, :], axis=0)\n",
    "\n",
    "# and build the initial prediction tensor\n",
    "# a forecast-window-sized tensor (1, forecast_window, 7)\n",
    "# formed with the forecast_window true values, starting at the end of the source tensor\n",
    "# that means\n",
    "\n",
    "prediction_tensor = tf.expand_dims(\n",
    "    source_array[row + configuration['num_timesteps'], :forecast_window, :],\n",
    "    axis=0)\n",
    "\n",
    "next_input_model = source_tensor\n",
    "\n",
    "# re-initialize the prediction list previously used for prediction over TFRecords\n",
    "predictions_list = list()\n",
    "\n",
    "# fill the predictions list over the forecast window\n",
    "for i in range(forecast_window):\n",
    "    \n",
    "    # from the current next_input_model tensor (1, 168, 7)\n",
    "    # get a prediction as NumPy array (1, 168, 1)\n",
    "    prediction = predict_fn(tensor_to_tensor_example(next_input_model))['forecast'].numpy()\n",
    "    \n",
    "    # get the value of the most recent prediction (last timestep) into the predictions list\n",
    "    predictions_list.append(prediction[:, -1, :][0][0])\n",
    "    \n",
    "    # from the source tensor, get the positional encodings for ti+1 to t167 (that is 168-i-1 values)\n",
    "    pos_encoding_old_values = source_tensor[:, i+1:, 1:]\n",
    "\n",
    "    # from target tensor, get the positional encodings for t168 to t168+i (that is i+1 values)\n",
    "    pos_encoding_new_val = prediction_tensor[:, :i+1, 1:]\n",
    "\n",
    "    # build new positional encodings with 168 values\n",
    "    pos_encodings = tf.concat([pos_encoding_old_values, pos_encoding_new_val], axis=1)\n",
    "    pos_encodings = tf.cast(pos_encodings, dtype=tf.float32)\n",
    "\n",
    "    # build the values feature for the next input to the model\n",
    "    # pop i+1 values at the beginning of the previous input\n",
    "    value_feature_old_values = tf.expand_dims(source_tensor[:, i+1:, 0], axis=-1)\n",
    "    value_feature_old_values = tf.cast(value_feature_old_values, dtype=tf.float32)\n",
    "\n",
    "    # current predictions_list to NumPy array\n",
    "    value_feature_new_values = np.array(predictions_list[:i+1])\n",
    "\n",
    "    # current prediction array to tensor\n",
    "    value_feature_new_values = tf.convert_to_tensor(value_feature_new_values)\n",
    "\n",
    "    # expand dimensions of current prediction tensor to single-value feature\n",
    "    value_feature_new_values = tf.expand_dims(value_feature_new_values, axis=-1)\n",
    "\n",
    "    # expand dimensions of current prediction tensor to single-value batch\n",
    "    value_feature_new_values = tf.expand_dims(value_feature_new_values, axis=0)\n",
    "    \n",
    "    # build the value feature tensor\n",
    "    next_input_model = tf.concat([value_feature_old_values, value_feature_new_values], axis=1)\n",
    "    \n",
    "    # build the next input tensor for the model\n",
    "    next_input_model = tf.concat([next_input_model, pos_encodings], axis=2)\n",
    "\n",
    "    \n",
    "# iterative predictions over the forecast window reside in predictions_list\n",
    "predicted_values = np.array(predictions_list)\n",
    "\n",
    "# and the true values remain in the prediction tensor, pass them to a NumPy array\n",
    "true_values = prediction_tensor[0, :, 0].numpy()\n",
    "\n",
    "resulting_smape = smape(true_values, predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage plots\n",
    "plots = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"7053badb-cd55-4be7-bc4f-ee967ef8e147\" data-root-id=\"2752\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"251423f7-6262-450b-90e7-e0b36df91bc0\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"2763\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"2767\",\"type\":\"Grid\"},{\"id\":\"2772\",\"type\":\"Grid\"},{\"id\":\"2797\",\"type\":\"Legend\"}],\"left\":[{\"id\":\"2768\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":960,\"renderers\":[{\"id\":\"2789\",\"type\":\"GlyphRenderer\"},{\"id\":\"2802\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"2753\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"2779\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"2755\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"2759\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"2757\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"2761\",\"type\":\"LinearScale\"}},\"id\":\"2752\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data_source\":{\"id\":\"2786\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"2787\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2788\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"2790\",\"type\":\"CDSView\"}},\"id\":\"2789\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":\"Multi-step prediction for model ARTRFDC_TPU_001, execution 2, on test dataset, SMAPE = 0.24193155765533447\"},\"id\":\"2753\",\"type\":\"Title\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2787\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],\"y\":{\"__ndarray__\":\"vaEVP6pd2T5mUp4+qiN6PsncXD6crjg+wBxEPogtgz5cmJ8+jA7APuOt9T6YtRQ/Xr8pP6ijND+GHDw/9XhDP0ztSz8dY1Q/3TFUPyA3Sz/OBD4/its6P2NjTj+sOj8/\",\"dtype\":\"float32\",\"shape\":[24]}},\"selected\":{\"id\":\"2811\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"2810\",\"type\":\"UnionRenderers\"}},\"id\":\"2786\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"2786\",\"type\":\"ColumnDataSource\"}},\"id\":\"2790\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2774\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"2811\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null},\"id\":\"2755\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"2799\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"2800\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2801\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"2803\",\"type\":\"CDSView\"}},\"id\":\"2802\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2801\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"red\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2800\",\"type\":\"Line\"},{\"attributes\":{\"overlay\":{\"id\":\"2796\",\"type\":\"BoxAnnotation\"}},\"id\":\"2775\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"2761\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"2776\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"2759\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null},\"id\":\"2757\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"2777\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"2778\",\"type\":\"HelpTool\"},{\"attributes\":{\"axis_label\":\"Timestep\",\"formatter\":{\"id\":\"2793\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"2764\",\"type\":\"BasicTicker\"}},\"id\":\"2763\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2810\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"2769\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],\"y\":{\"__ndarray__\":\"VWITP2GS4T6qX7A+AKGJPhbbWD7uC0M+tAU6PkjSdj7WxoI+5ACEPjRhnT5aXNY+pDIHP/CuGT9wbyE/3eUeP1mKDj8bn/Y+v8DwPrHa9j7FkfQ+ySsBP2M1HD8hsw8/\",\"dtype\":\"float32\",\"shape\":[24]}},\"selected\":{\"id\":\"2922\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"2921\",\"type\":\"UnionRenderers\"}},\"id\":\"2799\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"grid_line_alpha\":0.5,\"ticker\":{\"id\":\"2764\",\"type\":\"BasicTicker\"}},\"id\":\"2767\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"2796\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"items\":[{\"id\":\"2798\",\"type\":\"LegendItem\"},{\"id\":\"2812\",\"type\":\"LegendItem\"}]},\"id\":\"2797\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"2764\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"grid_line_alpha\":0.5,\"ticker\":{\"id\":\"2769\",\"type\":\"BasicTicker\"}},\"id\":\"2772\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"2793\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"2922\",\"type\":\"Selection\"},{\"attributes\":{\"axis_label\":\"Value\",\"formatter\":{\"id\":\"2795\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"2769\",\"type\":\"BasicTicker\"}},\"id\":\"2768\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2921\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2788\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2773\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"2799\",\"type\":\"ColumnDataSource\"}},\"id\":\"2803\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2795\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"label\":{\"value\":\"predicted\"},\"renderers\":[{\"id\":\"2802\",\"type\":\"GlyphRenderer\"}]},\"id\":\"2812\",\"type\":\"LegendItem\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"2773\",\"type\":\"PanTool\"},{\"id\":\"2774\",\"type\":\"WheelZoomTool\"},{\"id\":\"2775\",\"type\":\"BoxZoomTool\"},{\"id\":\"2776\",\"type\":\"SaveTool\"},{\"id\":\"2777\",\"type\":\"ResetTool\"},{\"id\":\"2778\",\"type\":\"HelpTool\"}]},\"id\":\"2779\",\"type\":\"Toolbar\"},{\"attributes\":{\"label\":{\"value\":\"real\"},\"renderers\":[{\"id\":\"2789\",\"type\":\"GlyphRenderer\"}]},\"id\":\"2798\",\"type\":\"LegendItem\"}],\"root_ids\":[\"2752\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
       "  var render_items = [{\"docid\":\"251423f7-6262-450b-90e7-e0b36df91bc0\",\"roots\":{\"2752\":\"7053badb-cd55-4be7-bc4f-ee967ef8e147\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "2752"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots['identifier'] = figure(\n",
    "    # x_axis_type='datetime',\n",
    "    plot_width=960,\n",
    "    plot_height=400,\n",
    "    title='Multi-step prediction for model {}, execution {}, on {} dataset, SMAPE = {}'.format(\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset,\n",
    "        resulting_smape))\n",
    "\n",
    "plots['identifier'].grid.grid_line_alpha=0.5\n",
    "\n",
    "plots['identifier'].xaxis.axis_label = 'Timestep'\n",
    "plots['identifier'].yaxis.axis_label = 'Value'\n",
    "\n",
    "plots['identifier'].line(\n",
    "    np.arange(true_values.shape[0]),\n",
    "    true_values,\n",
    "    color='green',\n",
    "    legend_label='real')\n",
    "\n",
    "plots['identifier'].line(\n",
    "    np.arange(predicted_values.shape[0]),\n",
    "    predicted_values,\n",
    "    color='red',\n",
    "    legend_label='predicted')\n",
    "\n",
    "# uncomment the following two lines to save plot\n",
    "# output_file('/home/developer/gcp/cbidmltsf/datasets/cfe/{}_H_kw.html'.format(device))\n",
    "# save(fig_kw)\n",
    "\n",
    "# uncomment the following line to display plot\n",
    "show(plots['identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
