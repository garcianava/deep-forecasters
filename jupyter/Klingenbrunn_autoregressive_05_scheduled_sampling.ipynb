{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer-decoder with no Seq2Seq component (autoregressive)\n",
    "\n",
    "# no value embedding\n",
    "# sine-cosine positional encoding on the hour, day, and month of timestamp\n",
    "# modified transformer-encoder layer for masked self-attention\n",
    "\n",
    "# so far the Auto Regressive Transformer Decoder architecture (ARTRFDC)\n",
    "# based on Klingenbrunn is working in TensorFlow, TPU-based,\n",
    "# with teacher-forcing training (only target true values are passed)\n",
    "\n",
    "# it is intended now to include scheduled sampling functionality\n",
    "# to confirm if better models can be generated passing target true values and predictions\n",
    "\n",
    "# the basic problem with scheduled sampling on Keras is:\n",
    "# the length of the input sequence to the multi-head attention component\n",
    "# must be inferred outside the decoder layer,\n",
    "# then this length is used to build the look-backwards mask to be passed to all decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for TFA MultiHeadAttention\n",
    "import typing\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA class from TensorFlow AddOns source\n",
    "# it is compatible with TF 1.15 for CloudTPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    r\"\"\"MultiHead Attention layer.\n",
    "    Defines the MultiHead Attention operation as described in\n",
    "    [Attention Is All You Need](https://arxiv.org/abs/1706.03762) which takes\n",
    "    in the tensors `query`, `key`, and `value`, and returns the dot-product attention\n",
    "    between them:\n",
    "    >>> mha = MultiHeadAttention(head_size=128, num_heads=12)\n",
    "    >>> query = np.random.rand(3, 5, 4) # (batch_size, query_elements, query_depth)\n",
    "    >>> key = np.random.rand(3, 6, 5) # (batch_size, key_elements, key_depth)\n",
    "    >>> value = np.random.rand(3, 6, 6) # (batch_size, key_elements, value_depth)\n",
    "    >>> attention = mha([query, key, value]) # (batch_size, query_elements, value_depth)\n",
    "    >>> attention.shape\n",
    "    TensorShape([3, 5, 6])\n",
    "    If `value` is not given then internally `value = key` will be used:\n",
    "    >>> mha = MultiHeadAttention(head_size=128, num_heads=12)\n",
    "    >>> query = np.random.rand(3, 5, 5) # (batch_size, query_elements, query_depth)\n",
    "    >>> key = np.random.rand(3, 6, 10) # (batch_size, key_elements, key_depth)\n",
    "    >>> attention = mha([query, key]) # (batch_size, query_elements, key_depth)\n",
    "    >>> attention.shape\n",
    "    TensorShape([3, 5, 10])\n",
    "    Args:\n",
    "        head_size: int, dimensionality of the `query`, `key` and `value` tensors\n",
    "            after the linear transformation.\n",
    "        num_heads: int, number of attention heads.\n",
    "        output_size: int, dimensionality of the output space, if `None` then the\n",
    "            input dimension of `value` or `key` will be used,\n",
    "            default `None`.\n",
    "        dropout: float, `rate` parameter for the dropout layer that is\n",
    "            applied to attention after softmax,\n",
    "        default `0`.\n",
    "        use_projection_bias: bool, whether to use a bias term after the linear\n",
    "            output projection.\n",
    "        return_attn_coef: bool, if `True`, return the attention coefficients as\n",
    "            an additional output argument.\n",
    "        kernel_initializer: initializer, initializer for the kernel weights.\n",
    "        kernel_regularizer: regularizer, regularizer for the kernel weights.\n",
    "        kernel_constraint: constraint, constraint for the kernel weights.\n",
    "        bias_initializer: initializer, initializer for the bias weights.\n",
    "        bias_regularizer: regularizer, regularizer for the bias weights.\n",
    "        bias_constraint: constraint, constraint for the bias weights.\n",
    "    Call Args:\n",
    "        inputs:  List of `[query, key, value]` where\n",
    "            * `query`: Tensor of shape `(..., query_elements, query_depth)`\n",
    "            * `key`: `Tensor of shape '(..., key_elements, key_depth)`\n",
    "            * `value`: Tensor of shape `(..., key_elements, value_depth)`, optional, if not given `key` will be used.\n",
    "        mask: a binary Tensor of shape `[batch_size?, num_heads?, query_elements, key_elements]`\n",
    "        which specifies which query elements can attend to which key elements,\n",
    "        `1` indicates attention and `0` indicates no attention.\n",
    "    Output shape:\n",
    "        * `(..., query_elements, output_size)` if `output_size` is given, else\n",
    "        * `(..., query_elements, value_depth)` if `value` is given, else\n",
    "        * `(..., query_elements, key_depth)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        num_heads: int,\n",
    "        output_size: int = None,\n",
    "        dropout: float = 0.0,\n",
    "        use_projection_bias: bool = True,\n",
    "        return_attn_coef: bool = False,\n",
    "        kernel_initializer: typing.Union[str, typing.Callable] = \"glorot_uniform\",\n",
    "        kernel_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        kernel_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        bias_initializer: typing.Union[str, typing.Callable] = \"zeros\",\n",
    "        bias_regularizer: typing.Union[str, typing.Callable] = None,\n",
    "        bias_constraint: typing.Union[str, typing.Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        warnings.warn(\n",
    "            \"`MultiHeadAttention` will be deprecated in Addons 0.13. \"\n",
    "            \"Please use `tf.keras.layers.MultiHeadAttention` instead.\",\n",
    "            DeprecationWarning,\n",
    "        )\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if output_size is not None and output_size < 1:\n",
    "            raise ValueError(\"output_size must be a positive number\")\n",
    "\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.output_size = output_size\n",
    "        self.use_projection_bias = use_projection_bias\n",
    "        self.return_attn_coef = return_attn_coef\n",
    "\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self._dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        num_query_features = input_shape[0][-1]\n",
    "        num_key_features = input_shape[1][-1]\n",
    "        num_value_features = (\n",
    "            input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n",
    "        )\n",
    "        output_size = (\n",
    "            self.output_size if self.output_size is not None else num_value_features\n",
    "        )\n",
    "\n",
    "        self.query_kernel = self.add_weight(\n",
    "            name=\"query_kernel\",\n",
    "            shape=[self.num_heads, num_query_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.key_kernel = self.add_weight(\n",
    "            name=\"key_kernel\",\n",
    "            shape=[self.num_heads, num_key_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.value_kernel = self.add_weight(\n",
    "            name=\"value_kernel\",\n",
    "            shape=[self.num_heads, num_value_features, self.head_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.projection_kernel = self.add_weight(\n",
    "            name=\"projection_kernel\",\n",
    "            shape=[self.num_heads, self.head_size, output_size],\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "\n",
    "        if self.use_projection_bias:\n",
    "            self.projection_bias = self.add_weight(\n",
    "                name=\"projection_bias\",\n",
    "                shape=[output_size],\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.projection_bias = None\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "\n",
    "        # einsum nomenclature\n",
    "        # ------------------------\n",
    "        # N = query elements\n",
    "        # M = key/value elements\n",
    "        # H = heads\n",
    "        # I = input features\n",
    "        # O = output features\n",
    "\n",
    "        query = inputs[0]\n",
    "        key = inputs[1]\n",
    "        value = inputs[2] if len(inputs) > 2 else key\n",
    "\n",
    "        # verify shapes\n",
    "        if key.shape[-2] != value.shape[-2]:\n",
    "            raise ValueError(\n",
    "                \"the number of elements in 'key' must be equal to the same as the number of elements in 'value'\"\n",
    "            )\n",
    "\n",
    "        if mask is not None:\n",
    "            if len(mask.shape) < 2:\n",
    "                raise ValueError(\"'mask' must have atleast 2 dimensions\")\n",
    "            if query.shape[-2] != mask.shape[-2]:\n",
    "                raise ValueError(\n",
    "                    \"mask's second to last dimension must be equal to the number of elements in 'query'\"\n",
    "                )\n",
    "            if key.shape[-2] != mask.shape[-1]:\n",
    "                raise ValueError(\n",
    "                    \"mask's last dimension must be equal to the number of elements in 'key'\"\n",
    "                )\n",
    "\n",
    "        # Linear transformations\n",
    "        query = tf.einsum(\"...NI , HIO -> ...NHO\", query, self.query_kernel)\n",
    "        key = tf.einsum(\"...MI , HIO -> ...MHO\", key, self.key_kernel)\n",
    "        value = tf.einsum(\"...MI , HIO -> ...MHO\", value, self.value_kernel)\n",
    "\n",
    "        # Scale dot-product, doing the division to either query or key\n",
    "        # instead of their product saves some computation\n",
    "        depth = tf.constant(self.head_size, dtype=query.dtype)\n",
    "        query /= tf.sqrt(depth)\n",
    "\n",
    "        # Calculate dot product attention\n",
    "        logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n",
    "\n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "            # possibly expand on the head dimension so broadcasting works\n",
    "            if len(mask.shape) != len(logits.shape):\n",
    "                mask = tf.expand_dims(mask, -3)\n",
    "\n",
    "            logits += -10e9 * (1.0 - mask)\n",
    "\n",
    "        attn_coef = tf.nn.softmax(logits)\n",
    "\n",
    "        # attention dropout\n",
    "        attn_coef_dropout = self.dropout(attn_coef, training=training)\n",
    "\n",
    "        # attention * value\n",
    "        multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef_dropout, value)\n",
    "\n",
    "        # Run the outputs through another linear projection layer. Recombining heads\n",
    "        # is automatically done.\n",
    "        output = tf.einsum(\n",
    "            \"...NHI,HIO->...NO\", multihead_output, self.projection_kernel\n",
    "        )\n",
    "\n",
    "        if self.projection_bias is not None:\n",
    "            output += self.projection_bias\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            return output, attn_coef\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        num_value_features = (\n",
    "            input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n",
    "        )\n",
    "        output_size = (\n",
    "            self.output_size if self.output_size is not None else num_value_features\n",
    "        )\n",
    "\n",
    "        output_shape = input_shape[0][:-1] + (output_size,)\n",
    "\n",
    "        if self.return_attn_coef:\n",
    "            num_query_elements = input_shape[0][-2]\n",
    "            num_key_elements = input_shape[1][-2]\n",
    "            attn_coef_shape = input_shape[0][:-2] + (\n",
    "                self.num_heads,\n",
    "                num_query_elements,\n",
    "                num_key_elements,\n",
    "            )\n",
    "\n",
    "            return output_shape, attn_coef_shape\n",
    "        else:\n",
    "            return output_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            head_size=self.head_size,\n",
    "            num_heads=self.num_heads,\n",
    "            output_size=self.output_size,\n",
    "            dropout=self._dropout_rate,\n",
    "            use_projection_bias=self.use_projection_bias,\n",
    "            return_attn_coef=self.return_attn_coef,\n",
    "            kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n",
    "            kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n",
    "            bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the autoregressive version of the transformer-decoder does not use the Seq2Seq intermediate layer\n",
    "# as there is no transformer-encoder component sending hidden states, therefore\n",
    "# having only a self-attention layer and a position-wise feed-forward layer,\n",
    "# the autoregressive transformer-decoder is, in fact, a transformer-encoder\n",
    "\n",
    "# the only important modification is the masked self-attention layer\n",
    "\n",
    "# masked self-attention layer seems to be already implemented in\n",
    "# MHA module from TensorFlow AddOns, then will be added to the EncoderLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a mask for self-attention on the autoregressive transformer decoder\n",
    "def get_decoder_mask(self_attention_inputs):\n",
    "    # self_attention_input shape is (?, n_timesteps, n_features)\n",
    "    # get the dimension value of n_timesteps and build the mask\n",
    "    n_timesteps = self_attention_inputs.shape[1]\n",
    "    mask = tf.convert_to_tensor(np.tril(np.ones([n_timesteps, n_timesteps]), 0),\n",
    "                                dtype=tf.float32)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(ARDecoderLayer, self).__init__()\n",
    "        # multi-head attention initialization\n",
    "        self.attention_layer = MultiHeadAttention(head_size=embed_dim, num_heads=num_heads)\n",
    "        self.ff_layer = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.add_norm_layer_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add_norm_layer_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, mask):\n",
    "        # mask for self-attention is passed to MHA on call\n",
    "        attention_output = self.attention_layer([inputs, inputs], mask=mask)\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        input_to_ffn = self.add_norm_layer_1(inputs + attention_output)\n",
    "        ffn_output = self.ff_layer(input_to_ffn)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        return self.add_norm_layer_2(input_to_ffn + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_pickle(\"/home/developer/gcp/cbidmltsf/timeseries/CPE04115_H_kw_20201021084001/ts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.274317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.217363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0.168545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>0.122996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>0.080440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     kw_scaled\n",
       "timestamp                     \n",
       "2016-01-01 00:00:00   0.274317\n",
       "2016-01-01 01:00:00   0.217363\n",
       "2016-01-01 02:00:00   0.168545\n",
       "2016-01-01 03:00:00   0.122996\n",
       "2016-01-01 04:00:00   0.080440"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22629"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of time series\n",
    "total_lectures = ts['kw_scaled'].count()\n",
    "total_lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a training dataset\n",
    "# features: m consecutive lectures with their timestamps\n",
    "# target: m consecutive lectures (lectures in features, shifted by 1 to the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the embedding dimension, the lenght of tranining, evaluation, and test datasets\n",
    "# use the most of the 22K+ lectures in the original time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of input sequence, hours in a week\n",
    "m = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sine-cosine positional encoding for the input sequence\n",
    "hours_in_day = 24\n",
    "days_in_month = 30\n",
    "months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15840.3,  4525.8,  2262.9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_lectures*np.array([0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the previous dataset split, use the following indexes for building datasets\n",
    "\n",
    "# 15000 rows in training dataset\n",
    "train_start = 0\n",
    "train_end = 15000\n",
    "\n",
    "# 4000 rows in evaluation dataset\n",
    "eval_start = 16000\n",
    "eval_end = 20000\n",
    "\n",
    "# 1000 rows in test dataset\n",
    "test_start = 21000\n",
    "test_end = 22000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training NumPy arrays\n",
    "with open('data/x_train.npy', 'rb') as filename:\n",
    "    x_train = np.load(filename)\n",
    "\n",
    "with open('data/y_train.npy', 'rb') as filename:\n",
    "    y_train = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 168, 7), (15000, 168, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evaluation NumPy arrays\n",
    "with open('data/x_eval.npy', 'rb') as filename:\n",
    "    x_eval = np.load(filename)\n",
    "\n",
    "with open('data/y_eval.npy', 'rb') as filename:\n",
    "    y_eval = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 168, 7), (4000, 168, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test NumPy arrays\n",
    "with open('data/x_test.npy', 'rb') as filename:\n",
    "    x_test = np.load(filename)\n",
    "\n",
    "with open('data/y_test.npy', 'rb') as filename:\n",
    "    y_test = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 168, 7), (1000, 168, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture details according to the Klingenbrunn experiment\n",
    "# (including notes to further modifications on the basic autoregressive model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of timesteps is the length of the input sequence,\n",
    "# is the embedding dimension from SLDB\n",
    "num_timesteps = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features is the active load value (main feature)\n",
    "# plus the six components of the sine-cosine positional encoding on hour, day, month\n",
    "\n",
    "# important: there is no value embedding, therefore d_model is very low\n",
    "d_model = 7\n",
    "\n",
    "# ToDo: use value embedding to a high-dimensional space and compare results\n",
    "# ToDo: use a different positional encoding system and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as long as there is no value embedding, neither convolutional nor dense layers are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 168, 7) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input layer for Keras functional\n",
    "input_layer = tf.keras.layers.Input(shape=(num_timesteps, d_model))\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 168, 7) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_transformer_block = input_layer\n",
    "input_to_transformer_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "ff_dim = 1024\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(168, 168), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same mask for all the ARDecoderLayers in the ARDecoder\n",
    "mask = get_decoder_mask(input_to_transformer_block)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:74: DeprecationWarning: `MultiHeadAttention` will be deprecated in Addons 0.13. Please use `tf.keras.layers.MultiHeadAttention` instead.\n"
     ]
    }
   ],
   "source": [
    "ar_decoder_layer = ARDecoderLayer(embed_dim=d_model,\n",
    "                                  num_heads=num_heads,\n",
    "                                  ff_dim=ff_dim,\n",
    "                                  dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 168, 7) dtype=float32 (created by layer 'ar_decoder_layer')>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_from_ar_decoder = ar_decoder_layer(input_to_transformer_block, mask=mask)\n",
    "output_from_ar_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klingenbrunn uses a linear layer to decode the output_from_encoder\n",
    "# from (?, num_timesteps, num_features) to (?, num_timesteps, 1)\n",
    "\n",
    "# the equivalent operation in TensorFlow is a TimeDistributed Dense layer to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_in_first_dense = 1\n",
    "first_dense = tf.keras.layers.Dense(units_in_first_dense, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 168, 1) dtype=float32 (created by layer 'time_distributed')>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_first_dense = tf.keras.layers.TimeDistributed(first_dense)(output_from_ar_decoder)\n",
    "distributed_first_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=input_layer, outputs=distributed_first_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 35s 72ms/step - loss: 0.0238 - root_mean_squared_error: 0.1512 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0576\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0066 - root_mean_squared_error: 0.0811 - val_loss: 0.0024 - val_root_mean_squared_error: 0.0490\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_loss: 0.0023 - val_root_mean_squared_error: 0.0479\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0045 - root_mean_squared_error: 0.0672 - val_loss: 0.0022 - val_root_mean_squared_error: 0.0471\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0041 - root_mean_squared_error: 0.0644 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0432\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0039 - root_mean_squared_error: 0.0623 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0436\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0037 - root_mean_squared_error: 0.0606 - val_loss: 0.0018 - val_root_mean_squared_error: 0.0428\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0035 - root_mean_squared_error: 0.0593 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0438\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0034 - root_mean_squared_error: 0.0587 - val_loss: 0.0018 - val_root_mean_squared_error: 0.0419\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0033 - root_mean_squared_error: 0.0575 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0435\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=10, validation_data=(x_eval, y_eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the get_decoder_mask function is working,\n",
    "# now use it as a base to code the scheduled sampling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_from_probability(p):\n",
    "    return True if random() < p else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY IMPORTANT NOTE\n",
    "# THERE IS NO SIMPLE WAY TO CODE THE SCHEDULED SAMPLING PROCESS FOR TPUESTIMATOR\n",
    "# (AS IT CAN BE DONE IN KERAS OR IN LOW-LEVEL TENSORFLOW)\n",
    "# BECAUSE THE INPUT TO THE MODEL IS DEFINED IN THE TRAINING INPUT FUNCTION\n",
    "# AND THE TEMPORARY OUTPUT OF THE MODEL IS DEFINED IN THE MODEL FUNCTION,\n",
    "# AND IT IS NOT CLEAR IF THESE SCOPES CAN BE COMBINED (MAYBE A HOOK?)\n",
    "\n",
    "# AS A RESULT, THIS AUTOREGRESSIVE TRANSFORMER FOR TPUESTIMATOR WILL ALWAYS USE TRUE VALUES,\n",
    "# AND THE TPU COMPUTING POWER WILL BE USED TO EXTEND BOTH THE SOURCE SEQUENCE AND THE FORECAST WINDOW\n",
    "# TO ENSURE THE PREDICTIVE PERFORMANCE IS COMPETITIVE WITH THE STATE-OF-THE-ART\n",
    "\n",
    "# LOOK FOR ANOTHER INTERESTING VARIATIONS TO SUPPORT THE THESIS\n",
    "# FOR INSTANCE, MODIFY THE MULTI-HEAD-ATTENTION COMPONENT\n",
    "# SUCH AS IN THE INFORMER, OR IN THE BOTTLENECK TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# therefore, pause here coding for the autoregressive transformer\n",
    "# and start a new notebook for making inferences with the TPU-based saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
