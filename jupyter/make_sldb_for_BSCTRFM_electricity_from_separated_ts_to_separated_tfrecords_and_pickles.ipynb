{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365\n",
    "# weeks_of_year and month_of_year become redundant when using days_of_year, do not evaluate them\n",
    "# weeks_in_year = 52\n",
    "# months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 15:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "# 243 weeks for seen data, 1 week for unseen data\n",
    "no_lectures_seen_data = 243*24 # 5832\n",
    "\n",
    "# seen data is divided as 90% for training and 10% for evaluation\n",
    "train_eval_limit = 0.9\n",
    "\n",
    "train_interval_end = int(no_lectures_seen_data*train_eval_limit) # 5248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_SEPARATED_MT_320-MT_330',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1,\n",
    "    'preprocessed': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_SEPARATED_MT_320-MT_330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'preprocessed': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168 already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a directory for the complete SLDB\n",
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the training TFRecord files\n",
    "try:\n",
    "    os.mkdir('{}/train'.format(sldb_dir))\n",
    "    print('Directory {}/train was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {}/train already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the evaluation TFRecord files\n",
    "try:\n",
    "    os.mkdir('{}/eval'.format(sldb_dir))\n",
    "    print('Directory {}/eval was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {}/eval already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/test was created.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory to persist time series pickles used for inference\n",
    "try:\n",
    "    os.mkdir('{}/test'.format(sldb_dir))\n",
    "    print('Directory {}/test was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {}/test already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a path to the scalers sub-directory\n",
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a sub-directory for the scalers\n",
    "try:\n",
    "    os.mkdir(scalers_dir)\n",
    "    print('Directory {} was created.'.format(scalers_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(scalers_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage a MinMaxScaler per individual customer_id\n",
    "min_max = dict()\n",
    "\n",
    "# a dictionary to manage a StandardScaler per individual customer_id\n",
    "standard = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of cores available for training in Cloud TPU\n",
    "num_cores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 320, 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [token_id for token_id in np.arange(start, end + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are we training over raw data or preprocessed data?\n",
    "state = 'preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer_id in customer_ids:\n",
    "    customer_data_path = '{}/separated_{}/{}.pkl'.format(data_folder, state, customer_id)\n",
    "    data[customer_id] = pd.read_pickle(customer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for SLDB generation, run this unified code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing for MT_320\n",
      "Test dataset persisted as a time series pickle for MT_320\n",
      "Started processing for MT_321\n",
      "Test dataset persisted as a time series pickle for MT_321\n",
      "Started processing for MT_322\n",
      "Test dataset persisted as a time series pickle for MT_322\n",
      "Started processing for MT_323\n",
      "Test dataset persisted as a time series pickle for MT_323\n",
      "Started processing for MT_324\n",
      "Test dataset persisted as a time series pickle for MT_324\n",
      "Started processing for MT_325\n",
      "Test dataset persisted as a time series pickle for MT_325\n",
      "Started processing for MT_326\n",
      "Test dataset persisted as a time series pickle for MT_326\n",
      "Started processing for MT_327\n",
      "Test dataset persisted as a time series pickle for MT_327\n",
      "Started processing for MT_328\n",
      "Test dataset persisted as a time series pickle for MT_328\n",
      "Started processing for MT_329\n",
      "Test dataset persisted as a time series pickle for MT_329\n",
      "Started processing for MT_330\n",
      "Test dataset persisted as a time series pickle for MT_330\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    \n",
    "    # initialize the examples dictionary for each customer\n",
    "    examples = {\n",
    "        'train': [],\n",
    "        'eval': [],\n",
    "        # test dataset is not passed to SLDB\n",
    "        # 'test': []\n",
    "    }\n",
    "\n",
    "    # get the customer identifier\n",
    "    customer_id = 'MT_{:03d}'.format(token_id)\n",
    "    customer_id\n",
    "    print('Started processing for {}'.format(customer_id))\n",
    "\n",
    "    # a temporary dataframe with data per customer_id to build the sub-series/examples\n",
    "    # data_df = filtered_output[filtered_output['token_id'] == token_id].copy()\n",
    "    \n",
    "    # use now a reference to the dataframe in the data dictionary \n",
    "    data_df = data[customer_id]\n",
    "\n",
    "    # expand with positional encodings\n",
    "    data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['sin_day_month'] = np.sin(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['cos_day_month'] = np.cos(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['sin_day_year'] = np.sin(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "    data_df['cos_day_year'] = np.cos(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "\n",
    "    # get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "    lectures_train_data = data_df['power_usage'][:train_interval_end]\n",
    "\n",
    "    # fit a scaler only on train data\n",
    "    # it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "    lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)\n",
    "\n",
    "    # get MinMaxScaler on train data, store it in a dictionary\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max = min_max_scaler.fit(lectures_train_data_array)\n",
    "    print('MinMax scaler generated on training data for {}'.format(customer_id))\n",
    "\n",
    "    # persist the scaler\n",
    "    scaler_filename = '{}/{}_min_max.save'.format(scalers_dir, customer_id)\n",
    "    joblib.dump(min_max, scaler_filename)\n",
    "    print('MinMax scaler persisted for {}'.format(customer_id))\n",
    "    \n",
    "    # get an array from the variable time series (seen and unseen)\n",
    "    all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)\n",
    "\n",
    "    # apply the scaler over all data (seen and unseen)\n",
    "    # rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "    data_df['kw_scaled'] = np.squeeze(min_max.transform(all_data_variable_array))\n",
    "    \n",
    "    # at this moment, the individual time series is ready to be window-rolled to produce\n",
    "    # sub-series/examples to serialize\n",
    "    \n",
    "    # BSCTRFM inference process is not direct, but iterative, therefoer\n",
    "    # no TFRecord SLDB is required for test dataset,\n",
    "    \n",
    "    # persist only the time series corresponding to the inference interval as test dataset\n",
    "    data_to_persist = data_df[sldb_columns][no_lectures_seen_data - (m + t) + 1:]\n",
    "    \n",
    "    # path to persist the time series dataframe corresponding to test dataset\n",
    "    path = '{}/test/{}.pkl'.format(sldb_dir, customer_id)\n",
    "    \n",
    "    data_to_persist.to_pickle(path)\n",
    "    print('Test dataset persisted as a time series pickle for {}'.format(customer_id))\n",
    "    \n",
    "    \n",
    "    # get an iterable with all the possible sub-series for training examples\n",
    "    for starting_point in np.arange(train_interval_end - (m + t) + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + (m + t)]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['train'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'train', len(examples['train'])))\n",
    "\n",
    "\n",
    "    # ToDo: remove evaluation step from Cloud TPU training and use all seen data for training stage\n",
    "    build_eval_set = True\n",
    "\n",
    "    if build_eval_set:\n",
    "\n",
    "        # get an iterable with all the possible sub-series for evaluation examples\n",
    "        for starting_point in np.arange(train_interval_end, no_lectures_seen_data - (m + t) + 1):\n",
    "\n",
    "            sub_series_df = data_df[sldb_columns][starting_point:starting_point + (m + t)]\n",
    "\n",
    "            encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "            decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "            target_df = sub_series_df[target_columns][m:m+t]\n",
    "            id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "            encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "            id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "            examples['eval'].append(\n",
    "                {\n",
    "                    'encoder_input': encoder_input_list,\n",
    "                    'decoder_input': decoder_input_list,\n",
    "                    'target': target_list,\n",
    "                    'id': id_list,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "              format(customer_id, 'eval', len(examples['eval'])))\n",
    "\n",
    "\n",
    "    # DO NOT PRODUCE A TEST DATASET FOR SLDB, AS INFERENCE PROCESS IS NOT DIRECT\n",
    "    # (IT IS ITERATIVE OVER UNSEEN DATA TIME SERIES)\n",
    "\n",
    "    # on each customer dataset, adjust the number of examples to the number of training cores\n",
    "    for stage in ['train', 'eval']:\n",
    "        # how many examples/rows must be removed from examples[stage] to comply with the number of cores\n",
    "        examples_to_remove = len(examples[stage])%num_cores\n",
    "\n",
    "        # remove the last 'examples_to_remove' examples from the dataset\n",
    "        for _ in np.arange(examples_to_remove):\n",
    "            examples[stage].pop(-1)\n",
    "\n",
    "        print('For {} cores in Cloud TPU, the number of {} examples for {} was adjusted to {}'.\\\n",
    "             format(num_cores, stage, customer_id, len(examples[stage])))\n",
    "\n",
    "\n",
    "    # serialize the rows in examples['train'] and, if present, examples['eval']\n",
    "    # to avoid excesive memory consumption\n",
    "\n",
    "    # write a TFRecord file for each consumer_id/stage\n",
    "    for stage in ['train', 'eval']:\n",
    "        # N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "        N_ROWS = len(examples[stage])\n",
    "        filename = '{}/{}/{}.tfrecord'.format(sldb_dir, stage, customer_id)\n",
    "\n",
    "        with tf.io.TFRecordWriter(filename) as writer:\n",
    "            for row in np.arange(N_ROWS):\n",
    "\n",
    "                example = tf.train.Example(\n",
    "                    # features within the example\n",
    "                    features=tf.train.Features(\n",
    "                        # individual feature definition\n",
    "                        feature={'encoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['encoder_input']),\n",
    "                                 'decoder_input':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['decoder_input']),\n",
    "                                 'target':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['target']),\n",
    "                                 'id':\n",
    "                                 _float_feature_from_list_of_values(\n",
    "                                     examples[stage][row]['id'])\n",
    "                                 }\n",
    "                    )\n",
    "                )\n",
    "                serialized_example = example.SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "\n",
    "            # report TFRecord file as completed\n",
    "            print('Persisted {} TFRecord file for {}'.format(stage, customer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the sldb dictionary with final statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'n_rows': 54032,\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_rows': 2728\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_SEPARATED_MT_320-MT_330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'stats': {'train': {'n_rows': 54032}, 'eval': {'n_rows': 2728}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the sldb from the examples dictionary (keys are stages, values are lists of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo: read TFRecord file into a Dataset and confirm the values in the source dataframe!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_320.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_321.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_322.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_323.tfrecord [Content-Type=application/octet-stream]...\n",
      "\\ [4 files][ 12.2 MiB/ 12.2 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rsync ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_324.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_325.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_326.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_327.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_328.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_329.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/eval/MT_330.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_320_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_321_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_322_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_323_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_324_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_325_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_326_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_327_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_328_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_329_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/scalers/MT_330_min_max.save [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_320.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_321.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_322.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_323.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_324.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_325.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_326.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_327.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_328.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_329.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_SEPARATED_MT_320-MT_330_BSCTRFM_168_168/train/MT_330.tfrecord [Content-Type=application/octet-stream]...\n",
      "- [34 files][697.0 MiB/697.0 MiB]    3.7 MiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rsync ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 34 objects/697.0 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!\n",
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
