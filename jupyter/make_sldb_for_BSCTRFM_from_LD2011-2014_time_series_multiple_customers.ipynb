{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365\n",
    "# weeks_of_year and month_of_year become redundant when using days_of_year, do not evaluate them\n",
    "# weeks_in_year = 52\n",
    "# months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 15:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "# 243 weeks for seen data, 1 week for unseen data\n",
    "no_lectures_seen_data = 243*24 # 5832\n",
    "\n",
    "# seen data is divided as 90% for training and 10% for evaluation\n",
    "train_eval_limit = 0.9\n",
    "\n",
    "train_interval_end = int(no_lectures_seen_data*train_eval_limit) # 5248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168\n",
    "\n",
    "span = m + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store sub-series for training examples\n",
    "sub_series = {\n",
    "    'train': [],\n",
    "    'eval': [],\n",
    "    'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store sub-series for training examples\n",
    "examples = {\n",
    "    'train': [],\n",
    "    'eval': [],\n",
    "    'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_MT320-MT330',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_MT320-MT330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_MT320-MT330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168 already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(scalers_dir)\n",
    "    print('Directory {} was created.'.format(scalers_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(scalers_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_pickle('{}/hourly_electricity_complete.pkl'.format(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to match range used by other academic papers\n",
    "filtered_output = output[(output['days_from_start'] >= 1096) & (output['days_from_start'] < 1346)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage a MinMaxScaler per individual customer_id\n",
    "min_max = dict()\n",
    "# a dictionary to manage a StandardScaler per individual customer_id\n",
    "standard = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 320, 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [token_id for token_id in np.arange(start, end + 1)]\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MT_320',\n",
       " 'MT_321',\n",
       " 'MT_322',\n",
       " 'MT_323',\n",
       " 'MT_324',\n",
       " 'MT_325',\n",
       " 'MT_326',\n",
       " 'MT_327',\n",
       " 'MT_328',\n",
       " 'MT_329',\n",
       " 'MT_330']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]\n",
    "customer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for SLDB generation, run this unified code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_320 processed. The number of examples in train dataset is 4913\n",
      "MT_320 processed. The number of examples in eval dataset is 249\n",
      "MT_320 processed. The number of examples in test dataset is 168\n",
      "MT_321 processed. The number of examples in train dataset is 9826\n",
      "MT_321 processed. The number of examples in eval dataset is 498\n",
      "MT_321 processed. The number of examples in test dataset is 336\n",
      "MT_322 processed. The number of examples in train dataset is 14739\n",
      "MT_322 processed. The number of examples in eval dataset is 747\n",
      "MT_322 processed. The number of examples in test dataset is 504\n",
      "MT_323 processed. The number of examples in train dataset is 19652\n",
      "MT_323 processed. The number of examples in eval dataset is 996\n",
      "MT_323 processed. The number of examples in test dataset is 672\n",
      "MT_324 processed. The number of examples in train dataset is 24565\n",
      "MT_324 processed. The number of examples in eval dataset is 1245\n",
      "MT_324 processed. The number of examples in test dataset is 840\n",
      "MT_325 processed. The number of examples in train dataset is 29478\n",
      "MT_325 processed. The number of examples in eval dataset is 1494\n",
      "MT_325 processed. The number of examples in test dataset is 1008\n",
      "MT_326 processed. The number of examples in train dataset is 34391\n",
      "MT_326 processed. The number of examples in eval dataset is 1743\n",
      "MT_326 processed. The number of examples in test dataset is 1176\n",
      "MT_327 processed. The number of examples in train dataset is 39304\n",
      "MT_327 processed. The number of examples in eval dataset is 1992\n",
      "MT_327 processed. The number of examples in test dataset is 1344\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "\n",
    "    # get the customer identifier\n",
    "    customer_id = 'MT_{:03d}'.format(token_id)\n",
    "\n",
    "    # a temporary dataframe with data per customer_id to build the sub-series/examples\n",
    "    data_df = filtered_output[filtered_output['token_id'] == token_id].copy()\n",
    "\n",
    "    # expand with positional encodings\n",
    "    data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['sin_day_month'] = np.sin(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['cos_day_month'] = np.cos(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['sin_day_year'] = np.sin(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "    data_df['cos_day_year'] = np.cos(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "\n",
    "    # get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "    lectures_train_data = data_df['power_usage'][:train_interval_end]\n",
    "\n",
    "    # fit a scaler only on train data\n",
    "    # it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "    lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)\n",
    "\n",
    "    # get MinMaxScaler on train data, store it in a dictionary\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max = min_max_scaler.fit(lectures_train_data_array)\n",
    "\n",
    "    # persist the scaler\n",
    "    scaler_filename = '{}/{}_min_max.save'.format(scalers_dir, customer_id)\n",
    "    joblib.dump(min_max, scaler_filename)\n",
    "    \n",
    "    # get an array from the variable time series (seen and unseen)\n",
    "    all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)\n",
    "\n",
    "    # apply the scaler over all data (seen and unseen)\n",
    "    # rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "    data_df['kw_scaled'] = np.squeeze(min_max.transform(all_data_variable_array))\n",
    "\n",
    "    # get an iterable with all the possible sub-series for training examples\n",
    "    for starting_point in np.arange(train_interval_end - span + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['train'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'train', len(examples['train'])))\n",
    "    \n",
    "\n",
    "    # get an iterable with all the possible sub-series for evaluation examples\n",
    "    for starting_point in np.arange(train_interval_end, no_lectures_seen_data - span + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['eval'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'eval', len(examples['eval'])))\n",
    "    \n",
    "\n",
    "    # remember that conditional range of test dataset overlaps with evaluation dataset\n",
    "    # for this experiment design# get an iterable with all the possible sub-series for test examples\n",
    "    for starting_point in no_lectures_seen_data - span + 1 + np.arange(168):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['test'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'test', len(examples['test'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the sldb dictionary with final statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'n_rows': len(examples['train'])\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_rows': len(examples['eval'])\n",
    "    },\n",
    "    'test': {\n",
    "        'n_rows': len(examples['test'])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_MT320-MT330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'stats': {'train': {'n_rows': 14739},\n",
       "  'eval': {'n_rows': 747},\n",
       "  'test': {'n_rows': 504}}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the sldb from the examples dictionary (keys are stages, values are lists of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in ['train', 'eval', 'test']:\n",
    "    N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for row in np.arange(N_ROWS):\n",
    "            \n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    feature={'encoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['encoder_input']),\n",
    "                             'decoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['decoder_input']),\n",
    "                             'target':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['target']),\n",
    "                             'id':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['id'])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read TFRecord file into a Dataset and confirm the values in given rows against the source dataframe!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "- [4 files][130.9 MiB/130.9 MiB]    4.2 MiB/s                                   \n",
      "Operation completed over 4 objects/130.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!\n",
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: random sample the serialization of examples to TFRecord SLDB!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
