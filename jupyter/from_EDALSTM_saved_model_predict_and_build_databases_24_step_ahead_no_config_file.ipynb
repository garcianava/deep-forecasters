{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for prediction and evaluation of multi-step forecasting EDALSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# uncomment the following line for compatibility with TensorFlow 1.15 (on GCP)\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# uncomment the following line for TensorFlow 2.X (local execution)\n",
    "import tensorflow as tf\n",
    "\n",
    "# forecast model was saved in TensorFlow 1.15\n",
    "# but, in order to make predictions locally, has to be loaded with TensorFlow 2\n",
    "from tensorflow.saved_model import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical mean absolute percentage error\n",
    "def smape(targets, predictions):\n",
    "    '''\n",
    "    predictions: a list with the predicted values\n",
    "    targets: a list with the actual values\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # lists to NumPy arrays\n",
    "    targets, predictions = np.array(targets), np.array(predictions)\n",
    "    # verify predictions and targets have the same shape\n",
    "    if predictions.shape == targets.shape:\n",
    "            return(np.sum(2*np.abs(predictions - targets) /\n",
    "                          (np.abs(targets) + np.abs(predictions)))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/developer/gcp/cbidmltsf'\n",
    "\n",
    "# during batch prediction, the SLDB identifier is obtained via Abseil Flags\n",
    "sldb_id = 'CPE04115_H_kw_20201021084001_256001_024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block will be imported as:\n",
    "# from edalstm.data import _parse_dataset_function\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_dataset_function(example_proto, objective_shapes, parse_timestamp):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    # do not parse the timestamp to TPUEstimator, as it does not support string types!\n",
    "    # ToDo: code timestamps into features, as numbers\n",
    "    #  so they can be parsed to training\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "    # the parsed dataset must have the shape {features}, target!!!\n",
    "    # so:\n",
    "    feature_dict = {\n",
    "        'hourly': hourly,\n",
    "        'oh_wd': oh_wd,\n",
    "        'oh_dh': oh_dh,\n",
    "    }\n",
    "    # Do not parse the timestamp for training!!! Strings are not supported in TPUs!!!,\n",
    "    # or parse it as a number\n",
    "    if parse_timestamp:\n",
    "        feature_dict['timestamp'] = timestamp\n",
    "\n",
    "    # _parse_dataset_function returns:\n",
    "    # features as a dictionary, and\n",
    "    # target as a float scalar\n",
    "    # ToDo: review notebook for making sldbs, it must persist target as a vector\n",
    "    # return feature_dict, target[0]\n",
    "    return feature_dict, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all the code to a single notebook cell, then to a function, later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_00/export/exporter/1610773894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_00 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_00 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_01/export/exporter/1610774005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_01 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_01 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_02/export/exporter/1610774126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_02 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_02 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_03/export/exporter/1610774237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_03 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_03 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_04/export/exporter/1610774348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_04 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_04 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_05/export/exporter/1610774459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_05 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_05 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_06/export/exporter/1610774571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_06 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_06 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_07/export/exporter/1610774690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_07 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_07 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_08/export/exporter/1610774800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_08 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_08 on test.tfrecord\n",
      "Exported model path is /home/developer/gcp/cbidmltsf/models/EDALSTM_TPU_002_09/export/exporter/1610774907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded for time series CPE04115_H_kw_20201021084001\n",
      "Number of rows in the test dataset is 1984.\n",
      "Loaded all string timestamps: True\n",
      "Loaded all targets: True\n",
      "Persisted Pandas dataframe for predictions detail of EDALSTM_TPU_002_09 on test.tfrecord\n",
      "Persisted Pandas dataframe for predictions summary of EDALSTM_TPU_002_09 on test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# during batch prediction, the model identifier is obtained via Abseil Flags\n",
    "model_id = 'EDALSTM_TPU_002'\n",
    "\n",
    "# during batch prediction, the dataset name is obtained via Abseil Flags\n",
    "dataset = 'test'\n",
    "\n",
    "# during batch prediction, the execution identifier is obtained via Abseil Flags\n",
    "# execution = 0\n",
    "for execution in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "\n",
    "    # use model identifier and execution number to build the model directory string\n",
    "    model_dir = '{}_{:02d}'.format(model_id, execution)\n",
    "\n",
    "    # get the path to the saved model main directory\n",
    "    saved_model_path = '{}/{}/{}/export/exporter'.format(PROJECT_ROOT,\n",
    "                                                         'models',\n",
    "                                                         model_dir)\n",
    "\n",
    "    # get all the files in the saved model path, to find the most recent one\n",
    "    all_files = os.listdir(saved_model_path)\n",
    "    # get the path to the most recent saved model\n",
    "    latest_saved_model_id = sorted(all_files)[-1]\n",
    "\n",
    "    # build the full path for the latest saved model dir\n",
    "    export_dir = '{}/{}'.format(saved_model_path, latest_saved_model_id)\n",
    "    print ('Exported model path is {}'.format(export_dir))\n",
    "\n",
    "    # load the saved model and the prediction function\n",
    "    imported = load(export_dir=export_dir, tags='serve')\n",
    "    predict_fn = imported.signatures[\"serving_default\"]\n",
    "\n",
    "    data_dir = '{}/{}/{}'.format(PROJECT_ROOT, 'sldbs', sldb_id)\n",
    "\n",
    "    # then get the ts_identifier from the json file in the sldb directory\n",
    "    sldb_json_file = '{}/sldb.json'.format(data_dir)\n",
    "\n",
    "    # open the json file\n",
    "    with open(sldb_json_file, 'r') as inputfile:\n",
    "        sldb_dict = json.load(inputfile)\n",
    "\n",
    "    # and get the time series identifier\n",
    "    ts_identifier = sldb_dict['ts']\n",
    "\n",
    "    # use the time series identifier to obtain the SK-Learn scaler used on it\n",
    "    scaler = joblib.load('{}/{}/{}/scaler.save'.format(PROJECT_ROOT,\n",
    "                                                        'timeseries',\n",
    "                                                        ts_identifier))\n",
    "\n",
    "    print('Scaler loaded for time series {}'.format(ts_identifier))\n",
    "\n",
    "    # build a path to the dataset for prediction\n",
    "    dataset_path = '{}/{}.tfrecord'.format(data_dir, dataset)\n",
    "\n",
    "    # load the dataset\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(dataset_path)\n",
    "\n",
    "    # a list to store prediction values\n",
    "    predictions_list = list()\n",
    "\n",
    "    # TensorFlow 2 eager execution allows to iterate over a dataset\n",
    "    for element in tfrecord_dataset:\n",
    "        predictions_list.append(predict_fn(element))\n",
    "\n",
    "    # get prediction values from predictions list\n",
    "    predictions = [p['forecast'][0] for p in predictions_list]\n",
    "\n",
    "    # pass predictions to a NumPy array\n",
    "    predictions_array = np.asarray(predictions)\n",
    "\n",
    "    # array from shape (rows, timesteps, 1) to (rows, timesteps)\n",
    "    predictions_array = np.squeeze(predictions_array)\n",
    "\n",
    "    # inverse-scale predictions\n",
    "    rescaled_predictions = scaler.inverse_transform(predictions_array)\n",
    "\n",
    "    # temporarily skip JSON serialization of predictions and targets for multistep forecasting\n",
    "\n",
    "    # get the SLDB parameters for the forecasting model\n",
    "    parameters_json_file = '{}/{}/{}/sldb_parameters.json'.format(PROJECT_ROOT,\n",
    "                                                                  'parameters',\n",
    "                                                                  model_id)\n",
    "\n",
    "    # recover the sldb dictionary from the json file in parameters/\n",
    "    with open(parameters_json_file, 'r') as inputfile:\n",
    "        sldb_parameters = json.load(inputfile)\n",
    "\n",
    "    # store the objective shapes for reshaping tensors in a dictionary\n",
    "    _EXTRACTING_OBJECTIVE_SHAPES = {\n",
    "        'hourly': [sldb_parameters['embedding']['hourly'], 1],\n",
    "        'target': [sldb_parameters['no_targets'], 1],\n",
    "        'oh_wd': [7, 1],  # Monday to Sunday\n",
    "        'oh_dh': [24, 1],  # midnight to 23:00\n",
    "        # ToDo: verify only the initial timestamp is passed to the model\n",
    "        'timestamp': [sldb_parameters['no_targets'], 1]\n",
    "    }\n",
    "\n",
    "    # test_dataset was previously acquired from tfrecord file\n",
    "    # use it again to build arrays for targets and timestamps\n",
    "    parsed_dataset = tfrecord_dataset.map(\n",
    "        lambda row: _parse_dataset_function(\n",
    "            example_proto=row,\n",
    "            objective_shapes=_EXTRACTING_OBJECTIVE_SHAPES,\n",
    "            parse_timestamp=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # a list to store the string_timestamps\n",
    "    string_timestamps_list = list()\n",
    "\n",
    "    # a list to store the targets\n",
    "    targets_list = list()\n",
    "\n",
    "    # get string_timestamps and targets associated to the predictions previously served\n",
    "    for parsed_example in parsed_dataset:\n",
    "        string_timestamps = np.squeeze(np.asarray(parsed_example[0]['timestamp']).astype(str))\n",
    "        string_timestamps_list.append(string_timestamps)\n",
    "        targets = np.squeeze(parsed_example[1])\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # get the number of rows in the dataset for prediction\n",
    "    length = sldb_parameters['total_{}_rows'.format(dataset)]\n",
    "    print('Number of rows in the {} dataset is {}.'.format(dataset, length))\n",
    "\n",
    "    # confirm all string_timestamps were loaded\n",
    "    print('Loaded all string timestamps: {}'.format(\n",
    "        len(string_timestamps_list) == length)\n",
    "    )\n",
    "\n",
    "    # confirm all targets were loaded\n",
    "    print('Loaded all targets: {}'.format(\n",
    "        len(targets_list) == length)\n",
    "    )\n",
    "\n",
    "    # targets to array\n",
    "    targets_array = np.asarray(targets_list)\n",
    "\n",
    "    # rescale the targets\n",
    "    rescaled_targets = scaler.inverse_transform(targets_array)\n",
    "\n",
    "    # a columns list for the predictions dataframe\n",
    "    pred_df_columns = ['model_id', 'execution', 'dataset', 'string_timestamps', 'predictions', 'targets']\n",
    "\n",
    "    # a list with model_id repeated length times, to populate the predictions detail dataframe\n",
    "    model_id_repeat_list = [model_id]*length\n",
    "    # same for execution\n",
    "    execution_repeat_list = [execution]*length\n",
    "    # same for dataset\n",
    "    dataset_repeat_list = [dataset]*length\n",
    "\n",
    "    # predictions dataframe\n",
    "    predictions_detail_df = pd.DataFrame(list(zip(model_id_repeat_list,\n",
    "                                           execution_repeat_list,\n",
    "                                           dataset_repeat_list,\n",
    "                                           # from 2D NumPy array to list of 1D arrays\n",
    "                                           string_timestamps_list,\n",
    "                                           # from 2D NumPy array to list of 1D arrays\n",
    "                                           rescaled_predictions.tolist(),\n",
    "                                           rescaled_targets.tolist())), columns=pred_df_columns)\n",
    "\n",
    "    # complement the detailed predictions dataframe with mae, rmse, smape\n",
    "    # row by row, will be averaged at model-execution level, later...\n",
    "\n",
    "    # a list with MAE, evaluated row by row\n",
    "    predictions_detail_df['mae'] = [mean_absolute_error(row.targets, row.predictions) \\\n",
    "                                    for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # a list with RMSE, evaluated row by row\n",
    "    predictions_detail_df['rmse'] = [sqrt(mean_squared_error(row.targets, row.predictions)) \\\n",
    "                                     for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # a list with SMAPE, evaluated row by row\n",
    "    predictions_detail_df['smape'] = [smape(row.targets, row.predictions) \\\n",
    "                                      for _, row in predictions_detail_df.iterrows()]\n",
    "\n",
    "    # build a predictions summary dataframe, reset index to avoid making a multi-column index when grouping by\n",
    "    predictions_summary_df = predictions_detail_df.groupby(['model_id', 'execution', 'dataset']).mean().reset_index()\n",
    "\n",
    "    # a range to iterate on prediction timesteps\n",
    "    targets_range = np.arange(sldb_parameters['no_targets'])\n",
    "\n",
    "    # vector metric (vector component to vector component)\n",
    "    # an array no_targets-d: metric for 1, 2,..., no_targets step-ahead (target versus prediction for rows in dataset)\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    mae_vector = [\n",
    "        mean_absolute_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['mae_vector'] = [mae_vector]\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    rmse_vector = [\n",
    "        sqrt(mean_squared_error(\n",
    "            # a list with the n-rows target values for the n-th step ahead\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            # a list with the n-rows prediction values for the n-th step ahead\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        )) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['rmse_vector'] = [rmse_vector]\n",
    "\n",
    "    # for index, row in dataframe.iterrows()\n",
    "    smape_vector = [\n",
    "        smape(\n",
    "            [row.targets[n] for _, row in predictions_detail_df.iterrows()],\n",
    "            [row.predictions[n] for _, row in predictions_detail_df.iterrows()]\n",
    "        ) for n in targets_range\n",
    "    ]\n",
    "    predictions_summary_df['smape_vector'] = [smape_vector]\n",
    "\n",
    "    # insert count of rows as a column value\n",
    "    predictions_summary_df.insert(3, 'count', length)\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_detail/\n",
    "    detail_pickle_path = '{}/{}/{}/{}_{:02d}_on_{}_tfrecord.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_detail',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset)\n",
    "\n",
    "    # persist the Pandas dataframe to database/predictions_detail/\n",
    "    predictions_detail_df.to_pickle(detail_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions detail of {}_{:02d} on {}.tfrecord'.format(model_id,\n",
    "                                                                                          execution,\n",
    "                                                                                          dataset))\n",
    "\n",
    "    # build a path to persist the dataframe to database/predictions_summary/\n",
    "    summary_pickle_path = '{}/{}/{}/{}_{:02d}_on_{}_tfrecord.pkl'.format(\n",
    "        PROJECT_ROOT,\n",
    "        'database',\n",
    "        'predictions_summary',\n",
    "        model_id,\n",
    "        execution,\n",
    "        dataset)\n",
    "\n",
    "    # persist the Pandas dataframe to database/predictions_summary/\n",
    "    predictions_summary_df.to_pickle(summary_pickle_path)\n",
    "    print('Persisted Pandas dataframe for predictions summary of {}_{:02d} on {}.tfrecord'.format(model_id,\n",
    "                                                                                          execution,\n",
    "                                                                                          dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
