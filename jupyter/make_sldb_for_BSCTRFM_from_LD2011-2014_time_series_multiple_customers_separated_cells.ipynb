{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to encode float values for serialized examples\n",
    "def _float_feature_from_list_of_values(list_of_values):\n",
    "    \"\"\"Returns a float_list from a list of floats / doubles.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/developer/gcp/cbidmltsf/datasets/electricity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values for positional encodings\n",
    "hours_in_day = 24\n",
    "days_in_week = 7\n",
    "days_in_month = 30\n",
    "days_in_year = 365\n",
    "# weeks_of_year and month_of_year become redundant when using days_of_year, do not evaluate them\n",
    "# weeks_in_year = 52\n",
    "# months_in_year = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the time series in seen (train, eval) and unseen (test) data\n",
    "# according to academic papers:\n",
    "\n",
    "# 243 days on seen data, 7 days on unseen data \n",
    "\n",
    "# seen data:      '2014-01-01 00:00:00' to '2014-08-31 23:00:00', 243*24 = 5832 lectures\n",
    "\n",
    "# train/eval split is 0.9/0.1, then\n",
    "\n",
    "# train data:     '2014-01-01 00:00:00' to '2014-08-07 15:00:00', 5248 lectures\n",
    "# eval data:      '2014-08-07 15:00:00' to '2014-08-31 23:00:00', 584 lectures\n",
    "\n",
    "# unseen data:    '2014-09-01 00:00:00' to '2014-09-07 23:00:00', 7*24 = 168 lectures\n",
    "\n",
    "# 243 weeks for seen data, 1 week for unseen data\n",
    "no_lectures_seen_data = 243*24 # 5832\n",
    "\n",
    "# seen data is divided as 90% for training and 10% for evaluation\n",
    "train_eval_limit = 0.9\n",
    "\n",
    "train_interval_end = int(no_lectures_seen_data*train_eval_limit) # 5248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sub-series to be persisted as serialized training examples\n",
    "\n",
    "# dimensionality of the encoder input\n",
    "m = 168\n",
    "\n",
    "# dimensionality of the decoder output \n",
    "t = 168\n",
    "\n",
    "span = m + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be included in the SLDB\n",
    "sldb_columns = [\n",
    "    'date',\n",
    "    'token_id',\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store sub-series for training examples\n",
    "sub_series = {\n",
    "    'train': [],\n",
    "    'eval': [],\n",
    "    'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to store sub-series for training examples\n",
    "examples = {\n",
    "    'train': [],\n",
    "    'eval': [],\n",
    "    'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb = {\n",
    "    'ts': 'LD2011-2014_MT320-MT330',\n",
    "    'embedding': {\n",
    "        'hourly': 168\n",
    "    },\n",
    "    'tau': {\n",
    "        'hourly': 1\n",
    "    },\n",
    "    'no_targets': 168,\n",
    "    'BSCTRFM': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_MT320-MT330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BSCTRFM_168_168'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a string with the basic specifications of the SLDB, as part of the SLDB identifier\n",
    "sldb_specs = 'BSCTRFM_{:03d}_{:03d}'.format(sldb['embedding']['hourly'], sldb['no_targets'])\n",
    "sldb_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD2011-2014_MT320-MT330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time-based identifier for the SLDB\n",
    "sldb_identifier = '{}_{}'.format(sldb['ts'], sldb_specs)\n",
    "sldb_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb_dir = '/home/developer/gcp/cbidmltsf/sldbs/{}'.format(sldb_identifier)\n",
    "sldb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168 already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(sldb_dir)\n",
    "    print('Directory {} was created.'.format(sldb_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(sldb_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalers_dir = '{}/scalers'.format(sldb_dir)\n",
    "scalers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: directory /home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(scalers_dir)\n",
    "    print('Directory {} was created.'.format(scalers_dir))\n",
    "except FileExistsError:\n",
    "    print('Error: directory {} already exists.'.format(scalers_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_columns = [\n",
    "    'kw_scaled',\n",
    "    'sin_hour_day',\n",
    "    'cos_hour_day',\n",
    "    'sin_day_week',\n",
    "    'cos_day_week',\n",
    "    'sin_day_month',\n",
    "    'cos_day_month',\n",
    "    'sin_day_year',\n",
    "    'cos_day_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the encoder input and the decoder input use the same columns from the source sub_series dataframe\n",
    "decoder_input_columns = encoder_input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['kw_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_pickle('{}/hourly_electricity_complete.pkl'.format(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to match range used by other academic papers\n",
    "filtered_output = output[(output['days_from_start'] >= 1096) & (output['days_from_start'] < 1346)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage data per individual customer_id\n",
    "data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to manage a MinMaxScaler per individual customer_id\n",
    "min_max = dict()\n",
    "# a dictionary to manage a StandardScaler per individual customer_id\n",
    "standard = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 320, 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [token_id for token_id in np.arange(start, end + 1)]\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MT_320',\n",
       " 'MT_321',\n",
       " 'MT_322',\n",
       " 'MT_323',\n",
       " 'MT_324',\n",
       " 'MT_325',\n",
       " 'MT_326',\n",
       " 'MT_327',\n",
       " 'MT_328',\n",
       " 'MT_329',\n",
       " 'MT_330']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_ids = ['MT_{:03d}'.format(token_id) for token_id in token_ids]\n",
    "customer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code elements for the main iterative cycle: do not run the following cells but the unified one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MT_322'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the customer identifier\n",
    "customer_id = 'MT_{:03d}'.format(token_id)\n",
    "customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a temporary dataframe with data per customer_id to build the sub-series/examples\n",
    "data_df = filtered_output[filtered_output['token_id'] == token_id].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>token_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9146673</th>\n",
       "      <td>36.461278</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146674</th>\n",
       "      <td>37.733183</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146675</th>\n",
       "      <td>37.591860</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146676</th>\n",
       "      <td>38.863765</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146677</th>\n",
       "      <td>39.005088</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-01-01 04:00:00</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152668</th>\n",
       "      <td>149.802148</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-09-07 19:00:00</td>\n",
       "      <td>32299.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152669</th>\n",
       "      <td>154.465800</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-09-07 20:00:00</td>\n",
       "      <td>32300.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152670</th>\n",
       "      <td>144.290560</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-09-07 21:00:00</td>\n",
       "      <td>32301.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152671</th>\n",
       "      <td>108.959864</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-09-07 22:00:00</td>\n",
       "      <td>32302.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152672</th>\n",
       "      <td>53.278689</td>\n",
       "      <td>322</td>\n",
       "      <td>2014-09-07 23:00:00</td>\n",
       "      <td>32303.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         power_usage  token_id                date  hours_from_start  \\\n",
       "9146673    36.461278       322 2014-01-01 00:00:00           26304.0   \n",
       "9146674    37.733183       322 2014-01-01 01:00:00           26305.0   \n",
       "9146675    37.591860       322 2014-01-01 02:00:00           26306.0   \n",
       "9146676    38.863765       322 2014-01-01 03:00:00           26307.0   \n",
       "9146677    39.005088       322 2014-01-01 04:00:00           26308.0   \n",
       "...              ...       ...                 ...               ...   \n",
       "9152668   149.802148       322 2014-09-07 19:00:00           32299.0   \n",
       "9152669   154.465800       322 2014-09-07 20:00:00           32300.0   \n",
       "9152670   144.290560       322 2014-09-07 21:00:00           32301.0   \n",
       "9152671   108.959864       322 2014-09-07 22:00:00           32302.0   \n",
       "9152672    53.278689       322 2014-09-07 23:00:00           32303.0   \n",
       "\n",
       "         days_from_start  hour_of_day  day_of_week  day_of_month  day_of_year  \\\n",
       "9146673             1096            0            2             1            1   \n",
       "9146674             1096            1            2             1            1   \n",
       "9146675             1096            2            2             1            1   \n",
       "9146676             1096            3            2             1            1   \n",
       "9146677             1096            4            2             1            1   \n",
       "...                  ...          ...          ...           ...          ...   \n",
       "9152668             1345           19            6             7          250   \n",
       "9152669             1345           20            6             7          250   \n",
       "9152670             1345           21            6             7          250   \n",
       "9152671             1345           22            6             7          250   \n",
       "9152672             1345           23            6             7          250   \n",
       "\n",
       "         week_of_year  month_of_year  \n",
       "9146673             1              1  \n",
       "9146674             1              1  \n",
       "9146675             1              1  \n",
       "9146676             1              1  \n",
       "9146677             1              1  \n",
       "...               ...            ...  \n",
       "9152668            36              9  \n",
       "9152669            36              9  \n",
       "9152670            36              9  \n",
       "9152671            36              9  \n",
       "9152672            36              9  \n",
       "\n",
       "[6000 rows x 11 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand with positional encodings\n",
    "data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "data_df['sin_day_month'] = np.sin(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "data_df['cos_day_month'] = np.cos(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "data_df['sin_day_year'] = np.sin(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "data_df['cos_day_year'] = np.cos(2*np.pi*data_df.day_of_year/days_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "lectures_train_data = data_df['power_usage'][:train_interval_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a scaler only on train data\n",
    "# it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MinMaxScaler on train data, store it in a dictionary\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max = min_max_scaler.fit(lectures_train_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers/MT_322_min_max.save'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist the scaler\n",
    "scaler_filename = '{}/{}_min_max.save'.format(scalers_dir, customer_id)\n",
    "scaler_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT330_BSCTRFM_168_168/scalers/MT_322_min_max.save']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(min_max, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array from the variable time series (seen and unseen)\n",
    "all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scaler over all data (seen and unseen)\n",
    "# rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "data_df['kw_scaled'] = np.squeeze(min_max.transform(all_data_variable_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use Pandas dataframes as sub-series list contents because it overrides memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an iterable with all the possible sub-series for training examples\n",
    "for starting_point in np.arange(train_interval_end - span + 1):\n",
    "    \n",
    "    sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "    \n",
    "    encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "    decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "    target_df = sub_series_df[target_columns][m:m+t]\n",
    "    id_df = sub_series_df[id_columns][:1]\n",
    "    \n",
    "    encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "    examples['train'].append(\n",
    "        {\n",
    "            'encoder_input': encoder_input_list,\n",
    "            'decoder_input': decoder_input_list,\n",
    "            'target': target_list,\n",
    "            'id': id_list,\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sub-series/examples in train dataset is 14739\n"
     ]
    }
   ],
   "source": [
    "print('The number of sub-series/examples in {} dataset is {}'.\\\n",
    "      format('train', len(examples['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an iterable with all the possible sub-series for evaluation examples\n",
    "for starting_point in np.arange(train_interval_end, no_lectures_seen_data - span + 1):\n",
    "    \n",
    "    sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "    \n",
    "    encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "    decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "    target_df = sub_series_df[target_columns][m:m+t]\n",
    "    id_df = sub_series_df[id_columns][:1]\n",
    "    \n",
    "    encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "    examples['eval'].append(\n",
    "        {\n",
    "            'encoder_input': encoder_input_list,\n",
    "            'decoder_input': decoder_input_list,\n",
    "            'target': target_list,\n",
    "            'id': id_list,\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sub-series/examples in eval dataset is 747\n"
     ]
    }
   ],
   "source": [
    "print('The number of sub-series/examples in {} dataset is {}'.\\\n",
    "      format('eval', len(examples['eval'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that conditional range of test dataset overlaps with evaluation dataset\n",
    "# for this experiment design# get an iterable with all the possible sub-series for test examples\n",
    "for starting_point in no_lectures_seen_data - span + 1 + np.arange(168):\n",
    "    \n",
    "    sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "    \n",
    "    encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "    decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "    target_df = sub_series_df[target_columns][m:m+t]\n",
    "    id_df = sub_series_df[id_columns][:1]\n",
    "    \n",
    "    encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "    id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "    examples['test'].append(\n",
    "        {\n",
    "            'encoder_input': encoder_input_list,\n",
    "            'decoder_input': decoder_input_list,\n",
    "            'target': target_list,\n",
    "            'id': id_list,\n",
    "        }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sub-series/examples in test dataset is 504\n"
     ]
    }
   ],
   "source": [
    "print('The number of sub-series/examples in {} dataset is {}'.\\\n",
    "      format('test', len(examples['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for SLDB generation, run this unified code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_320 processed. The number of examples in train dataset is 19652\n",
      "MT_320 processed. The number of examples in eval dataset is 996\n",
      "MT_320 processed. The number of examples in test dataset is 672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-fd3e59a75af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstarting_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_interval_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msub_series_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msldb_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstarting_point\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstarting_point\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mencoder_input_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_series_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2810\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3407\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \"\"\"\n\u001b[0;32m-> 3409\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3395\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3396\u001b[0m         )\n\u001b[1;32m   3397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1394\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m         )\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             new_blocks = [\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                             \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                             \u001b[0mfill_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m                         )\n\u001b[1;32m   1356\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m         )\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[0;31m# and the fill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mout_shape_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0mout_shape_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[0mout_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "\n",
    "    # get the customer identifier\n",
    "    customer_id = 'MT_{:03d}'.format(token_id)\n",
    "    customer_id\n",
    "\n",
    "    # a temporary dataframe with data per customer_id to build the sub-series/examples\n",
    "    data_df = filtered_output[filtered_output['token_id'] == token_id].copy()\n",
    "\n",
    "    # expand with positional encodings\n",
    "    data_df['sin_hour_day'] = np.sin(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['cos_hour_day'] = np.cos(2*np.pi*data_df.hour_of_day/hours_in_day)\n",
    "    data_df['sin_day_week'] = np.sin(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['cos_day_week'] = np.cos(2*np.pi*data_df.day_of_week/days_in_week)\n",
    "    data_df['sin_day_month'] = np.sin(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['cos_day_month'] = np.cos(2*np.pi*data_df.day_of_month/days_in_month)\n",
    "    data_df['sin_day_year'] = np.sin(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "    data_df['cos_day_year'] = np.cos(2*np.pi*data_df.day_of_year/days_in_year)\n",
    "\n",
    "    # get a series for the power usage variable on the training dataset, to fit the scaler\n",
    "    lectures_train_data = data_df['power_usage'][:train_interval_end]\n",
    "\n",
    "    # fit a scaler only on train data\n",
    "    # it is required to pass the power usage time series to a (?, 1) NumPy array\n",
    "    lectures_train_data_array = np.array(lectures_train_data).reshape(-1, 1)\n",
    "\n",
    "    # get MinMaxScaler on train data, store it in a dictionary\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max = min_max_scaler.fit(lectures_train_data_array)\n",
    "\n",
    "    # persist the scaler\n",
    "    scaler_filename = '{}/{}_min_max.save'.format(scalers_dir, customer_id)\n",
    "    joblib.dump(min_max, scaler_filename)\n",
    "    \n",
    "    # get an array from the variable time series (seen and unseen)\n",
    "    all_data_variable_array = np.array(data_df.power_usage).reshape(-1, 1)\n",
    "\n",
    "    # apply the scaler over all data (seen and unseen)\n",
    "    # rescale, and squeeze to drop the extra dimension, then assign to the new column kw_scaled\n",
    "    data_df['kw_scaled'] = np.squeeze(min_max.transform(all_data_variable_array))\n",
    "\n",
    "    # get an iterable with all the possible sub-series for training examples\n",
    "    for starting_point in np.arange(train_interval_end - span + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['train'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'train', len(examples['train'])))\n",
    "    \n",
    "\n",
    "    # get an iterable with all the possible sub-series for evaluation examples\n",
    "    for starting_point in np.arange(train_interval_end, no_lectures_seen_data - span + 1):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['eval'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'eval', len(examples['eval'])))\n",
    "    \n",
    "\n",
    "    # remember that conditional range of test dataset overlaps with evaluation dataset\n",
    "    # for this experiment design# get an iterable with all the possible sub-series for test examples\n",
    "    for starting_point in no_lectures_seen_data - span + 1 + np.arange(168):\n",
    "\n",
    "        sub_series_df = data_df[sldb_columns][starting_point:starting_point + span]\n",
    "\n",
    "        encoder_input_df = sub_series_df[encoder_input_columns][:m]\n",
    "        decoder_input_df = sub_series_df[decoder_input_columns][m-1:m-1+t]\n",
    "        target_df = sub_series_df[target_columns][m:m+t]\n",
    "        id_df = sub_series_df[id_columns][:1]\n",
    "\n",
    "        encoder_input_list = encoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        decoder_input_list = decoder_input_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        target_list = target_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "        id_list = id_df.reset_index(drop=True).to_numpy().flatten().tolist()\n",
    "\n",
    "        examples['test'].append(\n",
    "            {\n",
    "                'encoder_input': encoder_input_list,\n",
    "                'decoder_input': decoder_input_list,\n",
    "                'target': target_list,\n",
    "                'id': id_list,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print('{} processed. The number of examples in {} dataset is {}'.\\\n",
    "          format(customer_id, 'test', len(examples['test'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the sldb dictionary with final statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldb['stats'] = {\n",
    "    'train': {\n",
    "        'n_rows': len(examples['train'])\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_rows': len(examples['eval'])\n",
    "    },\n",
    "    'test': {\n",
    "        'n_rows': len(examples['test'])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ts': 'LD2011-2014_MT320-MT330',\n",
       " 'embedding': {'hourly': 168},\n",
       " 'tau': {'hourly': 1},\n",
       " 'no_targets': 168,\n",
       " 'BSCTRFM': 1,\n",
       " 'stats': {'train': {'n_rows': 14739},\n",
       "  'eval': {'n_rows': 747},\n",
       "  'test': {'n_rows': 504}}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the sldb from the examples dictionary (keys are stages, values are lists of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in ['train', 'eval', 'test']:\n",
    "    N_ROWS = sldb['stats'][stage]['n_rows']\n",
    "    filename = '{}/{}.tfrecord'.format(sldb_dir, stage)\n",
    "\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for row in np.arange(N_ROWS):\n",
    "            \n",
    "            example = tf.train.Example(\n",
    "                # features within the example\n",
    "                features=tf.train.Features(\n",
    "                    # individual feature definition\n",
    "                    feature={'encoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['encoder_input']),\n",
    "                             'decoder_input':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['decoder_input']),\n",
    "                             'target':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['target']),\n",
    "                             'id':\n",
    "                             _float_feature_from_list_of_values(\n",
    "                                 examples[stage][row]['id'])\n",
    "                             }\n",
    "                )\n",
    "            )\n",
    "            serialized_example = example.SerializeToString()\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = '{}/sldb.json'.format(sldb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, 'w') as filename:\n",
    "    json.dump(sldb, filename, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read TFRecord file into a Dataset and confirm the values in given rows against the source dataframe!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/eval.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/sldb.json [Content-Type=application/json]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/test.tfrecord [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/developer/gcp/cbidmltsf/sldbs/LD2011-2014_MT320-MT321_BSCTRFM_168_168/train.tfrecord [Content-Type=application/octet-stream]...\n",
      "- [4 files][130.9 MiB/130.9 MiB]    4.2 MiB/s                                   \n",
      "Operation completed over 4 objects/130.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# do not forget to sync sldbs/ from local to GS after the previous operations!\n",
    "!gsutil rsync -d -r /home/developer/gcp/cbidmltsf/sldbs gs://cbidmltsf/sldbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: random sample the serialization of examples to TFRecord SLDB!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
