{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer implementation in Keras and TensorFlow 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code for transformer block implements multi-head attention from Keras, only on TF 2X\n",
    "# question: is it possible to change it to use multi-head attention from TF AddOns, on TF 1.15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!\n",
    "# there is no support for TensorFlow addons on TF 1.15\n",
    "# code must be used from Python source\n",
    "# then, complete the Transformer model using Keras MHA layer, on TF 2.4\n",
    "\n",
    "# or just try using tf.compat.v1.keras.layers.MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first experiment:\n",
    "# TensorFlow 2.4\n",
    "# multi-head attention layer from Keras\n",
    "# Transformer-encoder only (autoencoder option)\n",
    "# value embedding\n",
    "# positional embedding\n",
    "# encoder layer with MHA\n",
    "# encoder output to linear to multi-step target (vector output and TimeDistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block as a layer, from\n",
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets for selected substation, load them as NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/256_to_24_train_hourly.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('data/256_to_24_train_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17824, 256), (17824, 24))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = np.load('data/256_to_24_eval_hourly.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval = np.load('data/256_to_24_eval_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1984, 256), (1984, 24))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use number of timesteps in the input sequence as maximal length for positional encoding\n",
    "num_timesteps = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_2')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input layer for Keras functional\n",
    "# use embedding dimension from SLDB as the input dimensionality\n",
    "input_layer = layers.Input(shape=(num_timesteps,))\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256, 1) dtype=float32 (created by layer 'tf.expand_dims_1')>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a layer to expand dimensions of input tensor (required prior to using a convolutional layer)\n",
    "expanded_input_layer = tf.expand_dims(input_layer, axis=2)\n",
    "expanded_input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256, 32) dtype=float32 (created by layer 'conv1d_1')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple (Dense) layer to project time series data (scalar) to d_model\n",
    "value_embedding_layer = layers.Conv1D(filters=32,\n",
    "                                      kernel_size=3,\n",
    "                                      activation=\"relu\",\n",
    "                                      padding=\"same\")(expanded_input_layer)\n",
    "value_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a simple position encoding\n",
    "# for instance, the one in Keras Transformer-encoder block for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_to_encode = tf.range(start=0, limit=num_timesteps, delta=1)\n",
    "positions_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32), dtype=float32, numpy=\n",
       "array([[-0.00018823,  0.04932977,  0.03218402, ..., -0.034956  ,\n",
       "        -0.03760637, -0.01789354],\n",
       "       [ 0.01210878,  0.02822688,  0.03157374, ..., -0.00940269,\n",
       "        -0.00340117,  0.01310406],\n",
       "       [ 0.0178569 , -0.03671504,  0.04829026, ...,  0.01711998,\n",
       "        -0.00582873,  0.00629357],\n",
       "       ...,\n",
       "       [-0.03976671,  0.03521476,  0.0437463 , ...,  0.00367503,\n",
       "        -0.02330271,  0.00906388],\n",
       "       [-0.00106432, -0.01607291,  0.00646626, ..., -0.03976947,\n",
       "         0.00817459, -0.03963844],\n",
       "       [ 0.04193385, -0.01860239, -0.0338046 , ..., -0.04548728,\n",
       "        -0.03488844, -0.00129445]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding_layer = layers.Embedding(input_dim=num_timesteps,\n",
    "                                            output_dim=embed_dim) (positions_to_encode)\n",
    "position_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256, 32) dtype=float32 (created by layer 'tf.__operators__.add_2')>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_transformer_block = value_embedding_layer + position_embedding_layer\n",
    "input_to_transformer_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality of Q, K, V\n",
    "embed_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of attention heads\n",
    "num_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer size in feed forward network inside transformer\n",
    "ff_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout rate inside the transformer block\n",
    "rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block = TransformerBlock(embed_dim=embed_dim,\n",
    "                                     num_heads=num_heads,\n",
    "                                     ff_dim=ff_dim,\n",
    "                                     rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256, 32) dtype=float32 (created by layer 'transformer_block')>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_from_transformer_block = transformer_block(input_to_transformer_block)\n",
    "output_from_transformer_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the output from transformer block towards the target\n",
    "# case 1: based on TransformerBlock example at\n",
    "# https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 32) dtype=float32 (created by layer 'global_average_pooling1d_1')>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_from_pooling = layers.GlobalAveragePooling1D()(output_from_transformer_block)\n",
    "output_from_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 32) dtype=float32 (created by layer 'repeat_vector')>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated = layers.RepeatVector(num_targets)(output_from_pooling)\n",
    "repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dropout = layers.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 32) dtype=float32 (created by layer 'time_distributed')>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_first_dropout = layers.TimeDistributed(first_dropout)(repeated)\n",
    "distributed_first_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_in_first_dense = 16\n",
    "first_dense = layers.Dense(units_in_first_dense, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 16) dtype=float32 (created by layer 'time_distributed_1')>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_first_dense = layers.TimeDistributed(first_dense)(distributed_first_dropout)\n",
    "distributed_first_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_dropout = layers.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 16) dtype=float32 (created by layer 'time_distributed_2')>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_second_dropout = layers.TimeDistributed(second_dropout)(distributed_first_dense)\n",
    "distributed_second_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_in_second_dense = 1\n",
    "second_dense = layers.Dense(units_in_second_dense, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 1) dtype=float32 (created by layer 'time_distributed_3')>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_second_dense = layers.TimeDistributed(second_dense)(distributed_second_dropout)\n",
    "distributed_second_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input_layer, outputs=distributed_second_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "557/557 [==============================] - 60s 108ms/step - loss: 0.0407 - root_mean_squared_error: 0.2018 - val_loss: 0.0432 - val_root_mean_squared_error: 0.2078\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 66s 118ms/step - loss: 0.0406 - root_mean_squared_error: 0.2016 - val_loss: 0.0434 - val_root_mean_squared_error: 0.2084\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 70s 126ms/step - loss: 0.0406 - root_mean_squared_error: 0.2016 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2077\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 82s 148ms/step - loss: 0.0406 - root_mean_squared_error: 0.2014 - val_loss: 0.0435 - val_root_mean_squared_error: 0.2085\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 100s 179ms/step - loss: 0.0406 - root_mean_squared_error: 0.2014 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2077\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 66s 118ms/step - loss: 0.0405 - root_mean_squared_error: 0.2013 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2076\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 74s 132ms/step - loss: 0.0405 - root_mean_squared_error: 0.2013 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2076\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0405 - root_mean_squared_error: 0.2012 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2076\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0405 - root_mean_squared_error: 0.2011 - val_loss: 0.0430 - val_root_mean_squared_error: 0.2075\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0404 - root_mean_squared_error: 0.2010 - val_loss: 0.0430 - val_root_mean_squared_error: 0.2073\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0404 - root_mean_squared_error: 0.2009 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2072\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0401 - root_mean_squared_error: 0.2002 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2062\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0400 - root_mean_squared_error: 0.2000 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2060\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0399 - root_mean_squared_error: 0.1998 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2063\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0399 - root_mean_squared_error: 0.1996 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2064\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0398 - root_mean_squared_error: 0.1995 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0398 - root_mean_squared_error: 0.1995 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2058\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0398 - root_mean_squared_error: 0.1994 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2070\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0397 - root_mean_squared_error: 0.1993 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0397 - root_mean_squared_error: 0.1993 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2060\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0397 - root_mean_squared_error: 0.1992 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0397 - root_mean_squared_error: 0.1991 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0396 - root_mean_squared_error: 0.1991 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0396 - root_mean_squared_error: 0.1991 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0396 - root_mean_squared_error: 0.1990 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2064\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0396 - root_mean_squared_error: 0.1990 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 76s 136ms/step - loss: 0.0396 - root_mean_squared_error: 0.1989 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2055\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 80s 144ms/step - loss: 0.0395 - root_mean_squared_error: 0.1989 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 104s 186ms/step - loss: 0.0395 - root_mean_squared_error: 0.1988 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0395 - root_mean_squared_error: 0.1988 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 71s 127ms/step - loss: 0.0395 - root_mean_squared_error: 0.1988 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0395 - root_mean_squared_error: 0.1988 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2058\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 76s 136ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0395 - root_mean_squared_error: 0.1988 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 77s 137ms/step - loss: 0.0395 - root_mean_squared_error: 0.1986 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0395 - root_mean_squared_error: 0.1986 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0395 - root_mean_squared_error: 0.1986 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2059\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0395 - root_mean_squared_error: 0.1986 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2055\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 73s 132ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 75s 134ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 76s 136ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 74s 132ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 74s 133ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2064\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 74s 132ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 74s 133ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 74s 133ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 73s 132ms/step - loss: 0.0394 - root_mean_squared_error: 0.1986 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 57/100\n",
      "557/557 [==============================] - 110s 198ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 95s 171ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2064\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 73s 130ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 78s 141ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 79s 141ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 78s 141ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 77s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 79s 141ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0394 - root_mean_squared_error: 0.1985 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 75s 135ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 77s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 78s 141ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 77s 139ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 78s 139ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 78s 140ms/step - loss: 0.0393 - root_mean_squared_error: 0.1984 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 78s 141ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/557 [==============================] - 74s 134ms/step - loss: 0.0394 - root_mean_squared_error: 0.1984 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 76s 136ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 77s 139ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 76s 136ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 77s 138ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2048\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 76s 137ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=100, validation_data=(x_eval, y_eval)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
