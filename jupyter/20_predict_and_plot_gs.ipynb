{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import to_datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running local, in the tpuestimator-tensorflow-1-15-vm, on Jupyter Lab\n",
    "_LOCAL_ROOT_DIR = '{0}/gcp/cbidmltsf'.format(os.getenv(\"HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/gcp/cbidmltsf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LOCAL_ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SLDB_ID = 'CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D008024001_W004168001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BUCKET_NAME = 'cbidmltsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_features is declared, later imported from data\n",
    "read_features = {\n",
    "    'hourly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'daily': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'weekly': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'target': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_wd': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'oh_dh': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    'timestamp': tf.io.VarLenFeature(dtype=tf.string)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.get_bucket(_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a path to the scaler file in GCP\n",
    "scaler_path = 'sldbs/{}/scaler.save'.format(_SLDB_ID)\n",
    "blob = bucket.blob(scaler_path)\n",
    "# temporarily download the scaler to VM's block storage, use the original name\n",
    "blob.download_to_filename('scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build a path to the sldb.json file in GCP\n",
    "sldb_path = 'sldbs/{}/sldb.json'.format(_SLDB_ID)\n",
    "blob = bucket.blob(sldb_path)\n",
    "# temporarily download the json file to VM's block storage, use the original name\n",
    "blob.download_to_filename('sldb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the sldb dictionary from vm's block storage\n",
    "with open('sldb.json', 'r') as filename:\n",
    "    sldb = json.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EQUIPMENT = sldb['description']['equipment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPE04015'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_EQUIPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no main function, build it later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2f776201abf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "# module bokeh is not accessible from Jupyter Lab, but it is available from command line\n",
    "# on /home/developer, so let's continue\n",
    "from bokeh.plotting import figure, output_file, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '086401_083201_043201_TPU_17_00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/086401_083201_043201_TPU_17_00/export/exporter'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = 'models/{0}/export/exporter'.format(model_dir)\n",
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(bucket_folder):\n",
    "    \"\"\"List all files in GCP bucket.\"\"\"\n",
    "    # ToDo: restructure the main function as this method uses the global variable 'bucket'\n",
    "    files = bucket.list_blobs(prefix=bucket_folder)\n",
    "    files_list = [file.name for file in files if '.' in file.name]\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list_files(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/086401_083201_043201_TPU_17_00/export/exporter/1583418813/saved_model.pb',\n",
       " 'models/086401_083201_043201_TPU_17_00/export/exporter/1583418813/variables/variables.data-00000-of-00001',\n",
       " 'models/086401_083201_043201_TPU_17_00/export/exporter/1583418813/variables/variables.index']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the names of the subdirectories in export/exporter (one for each training process)\n",
    "prefix_length = len(saved_model_path)\n",
    "# get the string that identifies the last saved model directory\n",
    "latest_saved_model_id = sorted(list(set([file[prefix_length+1:prefix_length+11] for file in all_files])))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1583418813'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_saved_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LATEST_SAVED_MODEL_DIR = 'gs://{0}/{1}/{2}'.format(_BUCKET_NAME,\n",
    "                                                    saved_model_path,\n",
    "                                                    latest_saved_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/models/086401_083201_043201_TPU_17_00/export/exporter/1583418813'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LATEST_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from gs://cbidmltsf/models/086401_083201_043201_TPU_17_00/export/exporter/1583418813/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# build a prediction function\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(_LATEST_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SavedModelPredictor with feed tensors {'example_bytes': <tf.Tensor 'Placeholder:0' shape=() dtype=string>} and fetch_tensors {'forecast': <tf.Tensor 'output_4_1/Sigmoid:0' shape=(1, 1) dtype=float32>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load the scaler from the vm's block storage\n",
    "scaler = joblib.load('scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_filename = 'gs://cbidmltsf/sldbs/{0}/test.tfrecord'.format(_SLDB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://cbidmltsf/sldbs/CPE04015_desbI_H_2017-04-01_00:00:00_2018-02-28_23:00:00_H008001001_D008024001_W004168001/test.tfrecord'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_dataset = tf.data.TFRecordDataset(test_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV1 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-410299e7502c>:4: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "dataset = test_dataset.map(lambda row: row)\n",
    "# ToDo: find out if there is a faster way to build the predictions list\n",
    "# (with no iterator on the dataset)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            example = sess.run(next_element)\n",
    "            predictions_list.append(predict_fn({'example_bytes': example}))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'forecast': array([[0.6250299]], dtype=float32)},\n",
       " {'forecast': array([[0.65739983]], dtype=float32)},\n",
       " {'forecast': array([[0.601554]], dtype=float32)},\n",
       " {'forecast': array([[0.23903506]], dtype=float32)},\n",
       " {'forecast': array([[0.3743351]], dtype=float32)},\n",
       " {'forecast': array([[0.46934643]], dtype=float32)},\n",
       " {'forecast': array([[0.49625847]], dtype=float32)},\n",
       " {'forecast': array([[0.60185355]], dtype=float32)},\n",
       " {'forecast': array([[0.37686834]], dtype=float32)},\n",
       " {'forecast': array([[0.41060016]], dtype=float32)}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scalar values from predictions list\n",
    "predictions = [p['forecast'][0][0] for p in predictions_list]\n",
    "# pass predictions to an array\n",
    "predictions = np.asarray(predictions)\n",
    "# inverse-scale predictions\n",
    "predictions = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "# remove all dimensions equal to 1 in the predictions array\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.043574 , 10.563725 ,  9.666341 ,  3.8410423,  6.015172 ,\n",
       "        7.5419044,  7.9743524,  9.671155 ,  6.0558786,  6.5979133],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_shapes = {\n",
    "    'hourly': [sldb['components']['hourly']['m'], 1],\n",
    "    'daily': [sldb['components']['daily']['m'], 1],\n",
    "    'weekly': [sldb['components']['weekly']['m'], 1],\n",
    "    'target': [sldb['components']['hourly']['no_targets'], 1],\n",
    "    'oh_wd': [7, 1],  # Monday to Sunday\n",
    "    'oh_dh': [24, 1],  # midnight to 23:00\n",
    "    'timestamp': [sldb['components']['hourly']['no_targets'], 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not pass objective shapes as a parameter, get them from an outer scope variable instead\n",
    "def _parse_dataset_function(example_proto):\n",
    "    # parse the input tf.Example proto using the dictionary above\n",
    "    row = tf.io.parse_single_example(example_proto, read_features)\n",
    "    # pass objective shape as a list of lists [hourly_shape, daily_shape, weekly_shape]\n",
    "    hourly = tf.reshape(row['hourly'].values, objective_shapes['hourly'])\n",
    "    daily = tf.reshape(row['daily'].values, objective_shapes['daily'])\n",
    "    weekly = tf.reshape(row['weekly'].values, objective_shapes['weekly'])\n",
    "    target = tf.reshape(row['target'].values, objective_shapes['target'])\n",
    "    oh_wd = tf.reshape(row['oh_wd'].values, objective_shapes['oh_wd'])\n",
    "    oh_dh = tf.reshape(row['oh_dh'].values, objective_shapes['oh_dh'])\n",
    "    timestamp = tf.reshape(row['timestamp'].values, objective_shapes['timestamp'])\n",
    "\n",
    "    # important: this is a different parse function from the one in training\n",
    "    # it returns target as a feature to use target values and timestamps for plotting\n",
    "    return {'hourly': hourly,\n",
    "            'daily': daily,\n",
    "            'weekly': weekly,\n",
    "            'target': target,\n",
    "            'oh_wd': oh_wd,\n",
    "            'oh_dh': oh_dh,\n",
    "            'timestamp': timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset was previously acquired from tfrecord file\n",
    "# use it again to build arrays for targets and timestamps\n",
    "dataset = test_dataset.map(lambda row: _parse_dataset_function(row))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: merge the operations in this iterator in the previous use of the same iterator???\n",
    "timestamps_list = []\n",
    "targets_list = []\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            item = sess.run(next_element)\n",
    "            # get the timestamp, then the scalar value, then the string value, then convert it to datetime\n",
    "            timestamps_list.append(to_datetime(item['timestamp'][0][0].decode()))\n",
    "            # get the target, then the scalar value\n",
    "            targets_list.append(item['target'][0][0])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.asarray(timestamps_list)\n",
    "targets = np.asarray(targets_list)\n",
    "targets = scaler.inverse_transform(targets.reshape(-1, 1))\n",
    "targets = np.squeeze(targets)\n",
    "# so far, predictions and targets are two NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then evaluate metrics here\n",
    "mse = mean_squared_error(targets, predictions)\n",
    "mae = mean_absolute_error(targets, predictions)\n",
    "# ToDo: persist metrics to a dictionary or database for reporting the complete experiment over a single circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.542434, 1.18655)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything is ready now for plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
